{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/","title":"The Recce Blog","text":"<p>Here you can find articles about dbt best practices and how to get the most out of Recce. For more articles, don't forget to also check out our Medium publication.</p>"},{"location":"blog/tags/","title":"Tags","text":"<p>Following is a list of tags for Recce Blog content:</p>"},{"location":"blog/tags/#tag:best-practices","title":"Best Practices","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:compliance","title":"Compliance","text":"<ul> <li>            Recce Is Now SOC 2 Type 1 Compliant          </li> </ul>"},{"location":"blog/tags/#tag:data-integrity","title":"Data Integrity","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:data-quality","title":"Data Quality","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:data-review","title":"Data Review","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:data-validaton","title":"Data Validaton","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:dataops","title":"DataOps","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:security","title":"Security","text":"<ul> <li>            Recce Is Now SOC 2 Type 1 Compliant          </li> </ul>"},{"location":"blog/tags/#tag:self-serve-analytics","title":"Self-Serve Analytics","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:self-serve-data","title":"Self-Serve Data","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:best-practices","title":"best practices","text":"<ul> <li>            Recce - Your data change management toolkit          </li> </ul>"},{"location":"blog/tags/#tag:code-review","title":"code review","text":"<ul> <li>            Recce - Your data change management toolkit          </li> </ul>"},{"location":"blog/tags/#tag:data-exploration","title":"data exploration","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> </ul>"},{"location":"blog/tags/#tag:data-impact","title":"data impact","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> </ul>"},{"location":"blog/tags/#tag:data-validaton","title":"data validaton","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> <li>            Recce - Your data change management toolkit          </li> </ul>"},{"location":"blog/tags/#tag:dbt","title":"dbt","text":"<ul> <li>            Recce - Your data change management toolkit          </li> </ul>"},{"location":"blog/tags/#tag:dbt-ci","title":"dbt CI","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:dbt-pr-review","title":"dbt PR Review","text":"<ul> <li>            Support Self-Serve Data with Comprehensive PR Review          </li> <li>            The Ultimate PR Comment Template Boilerplate for dbt data projects          </li> </ul>"},{"location":"blog/tags/#tag:dbt-data-model-validation","title":"dbt data model validation","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> </ul>"},{"location":"blog/tags/#tag:dbt-models","title":"dbt models","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> </ul>"},{"location":"blog/tags/#tag:impact-assessment","title":"impact assessment","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> <li>            Recce - Your data change management toolkit          </li> </ul>"},{"location":"blog/tags/#tag:root-cause-analysis","title":"root cause analysis","text":"<ul> <li>            Explore data impact and focus on tracking data validations with Recce's new interface          </li> </ul>"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/","title":"Next-Level Data Validation Toolkit for dbt Data Projects \u2014 Introducing Recce","text":"Recce: Data Validation Toolkit for dbt <p>Validating data modeling changes and reviewing pull requests for dbt projects can be a challenging task. The difficulty of performing a proper \u2018code review\u2019 for data projects, due to both the code and data needing review, means the data validation stage is often omitted, poorly implemented, or drastically slows down time-to-merge for your time sensitive data updates.</p> <p>How can you maintain data best practices, but speed up the validation and review process?</p>"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#recce-the-data-validation-toolkit-for-dbt-projects","title":"Recce \u2014 The data validation toolkit for dbt projects","text":"<p>Recce (short for reconnaissance) is a data modeling validation toolkit with a focus on environment diffing. Take two dbt environments, such as dev and prod, and compare them using the suite of diff tools in Recce.</p>"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#your-diffing-toolkit","title":"Your Diffing Toolkit","text":"<p>With Recce you\u2019re able to validate your data modeling changes against a known-good baseline. The only real way to verify modeling changes is to check it against historical/production data.</p>"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#lineage-dag-diff","title":"Lineage DAG Diff","text":"<p>Start from the zone of impact of your changes, and see which models have been modified, added, and removed. The dbt docs lineage DAG only shows you the current state of the DAG, Recce shows you how the DAG differs from before you made any changes.</p> Lineage DAG Diff in Recce"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#data-profile-diff-value-diff","title":"Data Profile Diff &amp; Value Diff","text":"<p>Perform holistic checks by diffing the data profile stats for your development branch, then check the percentage of values matching for each column in a model.</p> Data Profile in Recce"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#query-diff","title":"Query Diff","text":"<p>If something needs further investigation, drill down and query the data. One query will run on both environments, and you\u2019ll be able to see the difference on a row-by-row basis. Enable change-only view to see just what\u2019s changed.</p> SQL Query Diff in Recce"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#schema-and-row-count","title":"Schema and Row Count","text":"<p>In addition to the above diffs, you can also check the schema and row count, just to be sure you didn\u2019t lose any data, or an important column.</p> Schema and Row Count in Recce"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#create-your-checklist","title":"Create your checklist","text":"<p>As you create validations, add them to your checklist with notes about what you found, and re-re-run checks if the data changes. When you\u2019re ready, export the checks to your PR comment.</p> Data Project PR Checklist in Recce"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#create-your-all-signal-no-noise-pr-comment","title":"Create your all signal, no noise PR comment","text":"<p>When you\u2019re ready to share your validations as proof-of-correctness for your work, you can export checks into your PR comment template. You can copy your notes, and export a screenshot of the check as it appears in Recce. By curating the validations for your PR comment, you can create an \u2018all-signal, no noise\u2019 comment with the validations that are relevant to the context of your changes.</p> Data Modeling Validations in PR Comment <p>The reviewer will be able to see the query and results of your data spot-checks and have the comprehensive information required to request further investigation, or sign-off on your changes.</p>"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#why-recce","title":"Why Recce?","text":"<p>As mentioned above, whether you\u2019re the pull request author, or reviewer, you\u2019ve got the difficult task of understanding what is going on and trying to verify if the intentions of the PR were realized without screwing up production data. Here\u2019s some common issues we\u2019ve heard about working on large, or business critical, dbt data projects:</p> <ul> <li>QA for pull requests takes too long \u2014 stakeholders want to merge new data features faster.</li> <li>dbt build worked, but the data was actually wrong.</li> <li>I\u2019m sick of downtime from silent errors making it into prod.</li> <li>CI takes too long and current data quality tools are costly to run.</li> <li>I just want to see a summary of what changed for modified models.</li> </ul> <p>If any of these pain points ring true, Recce can help with the code review on your data project. Open-source and available now</p> <p>Recce OSS is available on GitHub now. Follow the instructions in our Getting Started guide to start using Recce to validate your data modeling changes.</p> <ul> <li>GitHub: DataRecce/Recce</li> <li>Docs: DataRecce.io/docs</li> <li>Discord: Recce Community</li> </ul>"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#try-recce-online","title":"Try Recce Online","text":"<p>If you want to try Recce out without having to install, check out the demo instance below.</p>"},{"location":"blog/data-validaton-toolkit-for-dbt-data-projects/#demo","title":"Demo","text":"<p>The demo PR makes a simple change to the dbt\u2019s Jaffle Shop project and changes how customer_lifetime_value (CLV) is calculated (fixes it to only calculated completed orders).</p> Can you validate this code change using Recce? <p>The expectation from this change is that CLV will be reduced overall, and that this will also impact the customer segments downstream model, With that in mind, see if you can determine if the if the PR has any issues by checking the data in Recce:</p> <ul> <li>The PR: https://github.com/DataRecce/jaffle_shop_duckdb/pull/1</li> <li>Recce Demo instance: https://pr1.cloud.datarecce.io/</li> </ul> <p>Hint: Run a Profile Diff, then a Query Diff, on the customers model, then check for downstream impact.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/","title":"Hands-On Data Impact Analysis for dbt Data Projects with Recce","text":"<p>dbt data projects aren\u2019t getting any smaller and, with the increasing complexity of DAGs, properly validating your data modeling changes has become a difficult task. The adoption of best practices such as data project pull request templates, and other \u2018pull request guard rails\u2019 has increased merge times and prolonged the QA process for pull requests.</p> Validate data modeling changes in dbt projects by comparing two environments with Recce <p>The difficulty comes from your responsibility to check not only the model SQL code, but also the data, which is a product of your code. Even when code looks right, silent errors and hard to notice bugs can make their way into the data. A proper pull request review is not complete with data validation.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#what-is-data-validation-and-why-you-need-to-do-it","title":"What is data validation and why you need to do it","text":"<p>Validation is the process of checking that your work is correct and achieves your intention. Common forms of validation are data change impact analysis and regression testing. </p> <p>With impact analysis checks you want to verify that any impact observed is desirable, such as a change in business logic for which you would expect to see data change. Regression testing, on the other hand, is about confirming that data change did not take place. This is useful for cases in which any change to modeling code should not impact the data.</p> <p>For both of these cases comparing the resultant data from your dev branch with a known-good baseline is the key - Show me what\u2019s different, or help me validate that nothing changed. This is where Recce can help.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#data-validation-toolkit-recce","title":"Data Validation Toolkit: Recce","text":"<p>Recce is a toolkit especially designed to validate data modeling changes in dbt projects by comparing two dbt environments, such as your dev branch and production data. Recce can help you validate your changes during development, and then use those validations in your PR comment for better PR review - the ultimate purpose of modeling validation.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#use-case-data-impact-analysis-in-dbts-jaffle-shop","title":"Use Case: Data Impact Analysis in dbt\u2019s Jaffle Shop","text":"<p>Jaffle Shop is dbt\u2019s standard intro project for dbt, so hopefully everyone is familiar with it. For this use-case demo, I\u2019ll show how to go from a modeling code change in Jaffle Shop and validate that change using the check suite diffing of tools in Recce.  </p> <p>Follow along with the drill down steps detailed below by using the online Recce demo, and the actual PR:</p> <ul> <li>Online Recce Demo: https://pr1.cloud.datarecce.io/</li> <li>PR: Fix: customer lifetime value calculation in customers</li> </ul>"},{"location":"blog/hands-on-data-impact-analysis-recce/#the-code-change","title":"The code change","text":"<p>The code change for this example is simple. It\u2019s a one-liner in the Customers model that modifies the calculation used for <code>customer_lifetime_value</code> to only include completed orders.</p> Business logic change to the customers model"},{"location":"blog/hands-on-data-impact-analysis-recce/#the-expected-impact","title":"The expected impact","text":"<p>This is a change to business logic (maybe more a bug fix), so we would expect to see data impact. As the engineer working on the PR, you\u2019ll already have an expectation of the type of impact you should see. </p> <p>Previously, the calculation for <code>custom_lifetime_value</code> included all orders, regardless of status, meaning <code>returned</code>, and <code>return_pending</code> were erroneously included. After adjusting the calculation we would expect to see:</p> <ul> <li>An overall reduction in the <code>customer_lifetime_value</code> figures in this table.</li> <li>Impact to downstream tables that use this figure</li> </ul> <p>We don\u2019t know the scale of the impact, yet. Let\u2019s first validate our expectations and then check the impact.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#data-drill-down-process","title":"Data drill-down process","text":"<p>For a small change such as this, you might go straight to some smaller data spot-checks, but in larger projects, or with more complex updates, it\u2019s better to follow a process of \u2018drilling down\u2019 into the data from high level checks, and then narrowing the focus to spot-checks. This will give you the full overview of impact to model and then give you the clues about what to check next.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#impact-overview","title":"Impact Overview","text":"<p>Lineage DAG Diff shows how the DAG has changed based on your branch code and shows added, removed, and modified models.</p> Lineage Diff in Recce with Row Count and Schema Diff <p>In the Lineage Diff for this example, you can see that only the <code>customers</code> model has been modified. </p> <p>Clicking the model pulls up the schema and you can check the row count. If there were schema changes they would be shown here (added, removed, and renamed columns). We don\u2019t see any indication of change to the schema, the row count shows no change, and the only modified model is the <code>customers</code> model, so you have validated that there is no undesired change to the high level structure of the project.</p> <p>Note: Lineage Diff differs (ha!) from the regular dbt lineage DAG because it shows the state of the DAG compared to another state. Whereas the dbt docs lineage DAG shows only the current state.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#high-level-data-impact-assessment","title":"High-level data impact assessment","text":"<p>The next stage of validation is high-level assessment. There are a number of checks that you can employ for this, currently Profile Diff, Value-Diff, and Top-K Diff. Each providing a different statistical comparison to your baseline data.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#profile-diff","title":"Profile Diff","text":"<p>With the <code>Customers</code> model still selected, click the <code>Advanced Diffs</code> button and click <code>Profile Diff</code>.</p> Data Profile Diff in Recce <p>The Profile Diff shows lots of stats for each column but, for this modeling change, check the min, max, and avg. You can see that the average value for the <code>customer_lifetime_value</code> column has dropped, which confirms our expectation, because a lot of orders have been excluded from the calculation now.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#value-diff","title":"Value Diff","text":"<p>A Value Diff is also a good way to get a high level overview of change to a model. With the <code>Customers</code> model selected in the Lineage, click <code>Value Diff</code> from the Advanced Diffs. Select <code>customer_id</code> as the primary key, and check the <code>All Columns</code> checkbox, then click the Execute button.</p> Choose the primary key and which columns to diff <p>The Value Diff shows the percentage match for each column:</p> Value Diff results in Recce <p>All columns are 100% match, except the <code>customer_lifetime_value</code> column, which now shows only 1.19% match (2 rows) - That\u2019s quite a lot of impact, you better spot-check some of the data! ;-)</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#fine-grained-data-spot-checks","title":"Fine-grained data spot-checks","text":"<p>Query Diff enables you to run a query against both environments, and diff the difference. It\u2019s the best way to spot-check data and see what\u2019s different. You can use any macros that are installed in your dbt project, and in a lot of ways this is like creating an ephemeral model especially for the purpose of data validation. In the simplest form, we can just select a subset of rows and see where the change is.</p> <p>With the <code>Customers</code> model still selected in the Lineage Diff, click the Query button at the bottom right and then adjust the query to select a subset of data:</p> <pre><code>select * from {{ ref(\"customers\") }} where customer_id &lt; 100;\n</code></pre> <p>Note: Avoid using LIMIT in Query Diff because LIMIT does not return the same rows each time, which is required form a useful query diff. </p> <p>Click <code>Run Diff</code> to query both dbt environments.</p> <p>When the results are returned click on the key icon in the customer_id column to confirm the primary key.  Any columns with a mismatched value will be displayed in red.</p> Query Diff results in Recce <p>Spot-checking the <code>customer_lifetime_value</code> column you can see that the current value (the dev branch) are all decreased. This is correct as the calculation we adjusted should only result in a decrease in total order value.</p> <p>Also, from the Value Diff we ran earlier, we know that only the <code>customer_lifetime_value</code> column should have changed, and that\u2019s confirmed from this Query Diff. </p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#checking-downstream-impact","title":"Checking downstream impact","text":"<p>This code change updates a key model, and the <code>customer_lifetime_value</code> column is used downstream in the <code>customer_segments</code> table. With the impact being so great (~99% of customer_lifetime_value has changed) we should check how this affects downstream models.</p> The customer_segments model uses customer_lifetime_value <p>Running a Top-K Diff on the <code>value_segment</code> column of <code>customer_segments</code> shows how the distribution has changed with a notable reduction in customers categorized as \u2018High value\u2019.</p> Top-K Diff on customer_segments.value_segment <p>Given the change in distribution, if this was a real world situation, you might choose to notify downstream stakeholders. </p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#the-checklist","title":"The Checklist","text":"<p>The process above demonstrates how you would validate your work during development but, as mentioned, the ultimate purpose of data validation is to provide proof-of-correctness of your work for PR review. </p> <p>At each stage in the validation process Recce allows you to add a validation check to the checklist, along with your notes. Just click the <code>Add to Checklist</code> or <code>Add Check</code> button when you see it, and the current validation check will be added to your checklist.</p> Validation checklist in Recce <p>In the checklist you can add a note for your validation to explain your findings and provide context. Then, when you\u2019re ready to create a PR, you can copy the checklist items into your PR to help the reviewer understand and sign-off on your work. </p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#conclusion","title":"Conclusion","text":"<p>In the above steps, you performed a data impact analysis and successfully validated the code changes by inspecting the data at varying grains of detail.</p> <p>Check back for more articles in the future that look at other use cases such as validating a refactoring job, and also how to make the most of Recce validations in your PR comment.</p>"},{"location":"blog/hands-on-data-impact-analysis-recce/#get-started-with-recce","title":"Get started with Recce","text":"<p>Recce OSS is available on GitHub now. Follow the instructions in our Getting Started guide to start using Recce to validate your data modeling changes in your project.</p> <ul> <li>GitHub: DataRecce/Recce</li> <li>Docs: DataRecce.io/docs</li> <li>Discord: Recce Community</li> </ul>"},{"location":"blog/histogram-overlay-top-k-dbt/","title":"Use Histogram Overlay and Top-K Charts to Understand Data Change in dbt","text":"<p>Data profiling stats are a really efficient way to get an understanding of the distribution of data in a dbt model. You can immediately see skewed data and spot data outliers, something which is difficult to do when checking data at the row level. Here's how Recce can help you make the most of these high-level data stats:</p>"},{"location":"blog/histogram-overlay-top-k-dbt/#visualize-data-change-with-histogram-and-top-k-charts","title":"Visualize data change with histogram and top-k charts","text":"<p>Profiling stats become even more useful when applied to data change validation. Let\u2019s say you\u2019ve updated a data model in dbt and changed the calculation logic for a column \u2014 how can you get an overview of how the data was changed or impacted? This is where checking the top-k values, or the histogram, of before-and-after you made the changes, comes in handy \u2014 But there\u2019s one major issue...</p> The best way to visualize data change in a histogram chart"},{"location":"blog/histogram-overlay-top-k-dbt/#somethings-not-right","title":"Something\u2019s not right","text":"<p>If you generate a histogram graph from prod data, then do the same for your dev branch, you\u2019ve got two distinct graphs. The axes don\u2019t match, and it\u2019s difficult to compare:</p> Comparing to distinct histogram charts is impossible <p>You might be able to spot some differences, such as at the top end of the graph, but the overall impact is mostly hidden.</p> <p>The same is true for top-k. Cross-referencing categories might be doable when there are only a handful, but it\u2019s still not a meaningful way to visualize the differences:</p> Cross-referencing Top-K will become difficult with more categories <p>There\u2019s a real possibility that you\u2019ll miss some edge cases when you can\u2019t compare precisely and accurately. That means silent errors, or even pipeline-breaking errors, will make it into prod. Errors that you won\u2019t find out about until the client or stakeholder calls to asks what\u2019s up with the data.</p>"},{"location":"blog/histogram-overlay-top-k-dbt/#how-to-meaningfully-compare-histograms-and-top-k","title":"How to meaningfully compare histograms and top-k","text":"<p>The best way to diff profile stats like histogram and top-k stats, is to plot them on a single chart, overlaid on top of each other using shared axes.</p> <p>Here\u2019s the same histogram charts as the image above, but with the histograms plotted on a single chart:</p> An overlaid histogram makes the charts actually useful! <p>In this overlay histogram chart, you can quickly see the distribution change in a meaningful way. At each value range the scale of the data change is clear. The same is true for top-k. When the values are compared directly next to each other the impact is immediately understandable.</p> Side-by-side Top-K values for direct comparison <p>Doing this manually isn\u2019t straight forward. You\u2019d need to code it in a Jupyter notebook with python and would take a lot of configuration, especially given that for dbt data projects you\u2019ll be pulling data from two different schemas if you\u2019re diffing dev and prod. Checking this kind of data profile diff for each PR in your project would require an unreasonable amount of work.</p>"},{"location":"blog/histogram-overlay-top-k-dbt/#easily-diff-histogram-top-k-and-other-profiling-stats-in-recce","title":"Easily diff histogram, top-k and other profiling stats in Recce","text":"<p>You can get histogram and top-k diffs, especially designed for dbt data projects, as part of the suite of data modeling validation tools in Recce.</p> <p>Recce compares your development and production datasets (or any two dbt envs) and enables a visual representation of data change through multiple diffing tools and query comparisons.</p> Get an overlaid histogram diff for your dbt PR Review with Recce"},{"location":"blog/histogram-overlay-top-k-dbt/#improved-visibility-of-data-change-for-dbt","title":"Improved visibility of data change for dbt","text":"<p>As a data or analytics engineer, the improved visibility of data change makes validating your work quicker, easier, and more accurate.</p> <p>As a PR reviewer you can do your job more efficiently and confidently sign off on data changes knowing that an edge case won\u2019t come back to bite you.</p> <p>Recce is open-source and available now, so you can start properly validating your dev branch right away.</p>"},{"location":"blog/histogram-overlay-top-k-dbt/#get-recce","title":"Get Recce","text":"<ul> <li>GitHub</li> <li>Recce Docs</li> <li>dbt Slack Channel #tools-Recce</li> </ul>"},{"location":"blog/check-critical-models/","title":"Identify and Automate Data Checks on Critical dbt Models","text":"<p>Do you know which are the critical models in your data project?</p> <p>I\u2019m sure the answer is yes. Even if you don\u2019t rank models, you can definitely point to which models you should tread carefully around.</p> <p>Do you check these critical models for data impact with every pull request?</p> <p>Maybe some, but it\u2019s probably on a more ad-hoc basis. If they really are critical models, you need to be aware of unintended impact. The last thing you want to do is mistakenly change historical metrics, or lose data.</p> Impacted Lineage DAG from Recce showing modified and impacted models on the California Integrated Travel Project dbt project"},{"location":"blog/check-critical-models/#identifying-critical-models","title":"Identifying critical models","text":"<p>Knowing the critical models in your project comes from your domain knowledge. You know these models have:</p> <ul> <li>a particular significance to business,</li> <li>a ton of downstream models,</li> <li>or, from experience, you\u2019ll get a call about that data if something goes wrong</li> </ul> <p>If you check these models in an ad-hoc way, it might be time to apply a more formal ranking system to models, which will also help the triage process if something does go wrong.</p>"},{"location":"blog/check-critical-models/#when-to-check-critical-models","title":"When to check critical models","text":"<p>Checking that these critical models didn\u2019t change is really important, and you need to do this before merging your updated models into prod. You might have a data observability tool in place to monitor prod data but, if you merge a breaking change, it\u2019s already too late. The downstream damage is already done.</p> <p>You can check the data in two ways before merging:</p> <ul> <li>During development: This is when you\u2019re making SQL changes, and editing models and metrics, checking the data and proactively and looking for data impact.</li> <li>Automatically in CI: These checks run automatically in continuous integration (CI). You still need to review the checks, but they are run automatically for each pull request (PR) on your data project.</li> </ul> <p>This mix of manual and automated checks gives you a good chance at comprehensive coverage, but you still need the domain knowledge to identify those critical models.</p>"},{"location":"blog/check-critical-models/#a-plan-for-checking-critical-models","title":"A plan for checking critical models","text":"<p>Checking the data in critical models involves comparing the data generated from your development branch with production (or staging data). The process looks like:</p> <ul> <li>Identify critical models \u2014 Models which are bottlenecks or are important to business</li> <li>Decide which checks to run on these models \u2014 Data should not change in these models so you should run structural and data profiling checks</li> <li>Automate the checks \u2014 Run the checks in CI with each PR</li> <li>Review check results \u2014 As part of PR review check if there is impact, and if it\u2019s is intended or not</li> </ul>"},{"location":"blog/check-critical-models/#automating-critical-model-checks","title":"Automating Critical Model Checks","text":"<p>Critical models checks should run with each PR, here\u2019s how you can do that with the preset-checks feature in Recce, by committing a checklist to your data project repo.</p> <p>Let\u2019s say you want to have the following structural checks run with each PR for your customers model:</p> <ul> <li>Row count \u2014 You shouldn\u2019t lose any data.</li> <li>Schema \u2014 The schema should not change unexpectedly.</li> </ul> <p>These are fundamental checks that all critical models should have, regardless of the type of PR.</p> Schema and row count checks performed in Data Recce <p>You can run these checks in the Recce UI, and any of these checks can also be automated in CI. Here\u2019s how:</p>"},{"location":"blog/check-critical-models/#1-generate-your-check-file-recceyml-and-commit-to-your-project","title":"1. Generate your check file (recce.yml) and commit to your project","text":"<p>To add the row count and schema checks to your dbt CI job you just need to commit a Recce checklist (<code>recce.yml</code>) to your dbt project. From the Recce UI you would:</p> <ol> <li>Add the check to your checklist.</li> <li>Copy the preset check template to your <code>recce.yml</code></li> </ol> Recce.yml in the dbt project root <p>Then commit the <code>recce.yml</code> in the root of your dbt data project. Each branch will get a copy of these checks, and Recce will run the checks each time a PR is opened.</p>"},{"location":"blog/check-critical-models/#2-run-recce-in-ci","title":"2. Run Recce in CI","text":"<p>When Recce runs in CI, the checks in <code>recce.yml</code> will be automatically run. Recce also provides a command to generate a PR summary with your results, which can also be automatically added to the PR comments for reviewers to check</p> Run Recce in CI"},{"location":"blog/check-critical-models/#3-review-the-results","title":"3. Review the results","text":"<p>The Recce summary, posted to your PR comments, shows you the following things:</p> <ul> <li>A diagram of the impacted lineage \u2014 this is just the part of the lineage that has modifications, or is downstream of modified models</li> <li>A list of checks that detected a data mismatch</li> </ul> Review Recce CI Summary <p>If there was a difference in the row count, or the schema changed, on the customers model, then those checks will be listed here.</p> <p>The idea behind the Recce Summary is \u2018all signal, no noise\u2019, which means you only want actionable or useful information \u2014 You only want to know when there\u2019s a difference, if everything is the same there\u2019s no point telling you that, it\u2019s just noise that would crowd out the signal.</p>"},{"location":"blog/check-critical-models/#4-optional-data-impact-exploration","title":"4. (optional) Data impact exploration","text":"<p>If one of the checks indicated a data-mismatch, or you\u2019ve otherwise seen something that you want to inspect, you can download the Recce state file and run Recce in review mode to see the results of the checks.</p> Download the Recce state file and execute Recce in Review Mode <p>To perform live checks on the data, you need only add a <code>dbt_project.yml</code> and <code>profiles.yml</code>, there\u2019s no need to checkout the whole dbt project. Then you can perform live checks to compare dev with prod/staging.</p>"},{"location":"blog/check-critical-models/#conclusion","title":"Conclusion","text":"<p>Every data project will have those critical models. It\u2019s fine to check them ad-hoc while you are working on the data project and validating the data generated from your changes. But it\u2019s best practice to identify and automate checks on these critical models.</p> <ul> <li>Identify your critical models</li> <li>Curate a checklist for things you know should not change in these models (project specific)</li> <li>Run this checklist in your PRs with Recce</li> <li>Review impact if necessary</li> </ul>"},{"location":"blog/check-critical-models/#get-recce","title":"Get Recce","text":"<ul> <li>GitHub</li> <li>Recce Docs</li> <li>dbt Slack Channel #tools-Recce</li> </ul>"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/","title":"From DevOps to DataOps: A Fireside Chat on Practical Strategies for Effective Data Productivity","text":"<p>Top priorities for data-driven organizations are data productivity, cost reduction, and error prevention. The four strategies to improve DataOps are: </p> <ol> <li>start with small, manageable improvements, </li> <li>follow a clear blueprint,</li> <li>conduct regular data reviews, and</li> <li>gradually introduce best practices across the team.</li> </ol> <p>In a recent fireside chat, CL Kao, founder of Recce, and Noel Gomez, co-founder of Datacoves, shared their combined experience of over two decades in the data and software industry. They discussed practical strategies to tackle these challenges, the evolution from DevOps to DataOps, and the need for companies to focus on data quality to avoid costly mistakes.</p> Data Productivity - Beyonig DevOps &amp; dbt <p>Noel Gomez began his journey at Amgen, where he worked extensively on data governance and digital transformation. He learned that data quality is essential to preventing errors. With his software development background, he realized that many software practices weren\u2019t applied to analytics\u2014and saw the opportunity to merge these worlds.</p> <p>CL Kao worked on version control systems predating Git. He was involved more heavily in data when he helped start Taiwan\u2019s civic tech community, focusing on making public data understandable.</p> <p>Despite their different paths, both CL and Noel agree that many software practices can be adapted to data management, but there are critical differences. What are the similarities, and what sets DataOps apart?</p>"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/#similarities-between-devops-and-dataops","title":"Similarities Between DevOps and DataOps","text":"<p>DevOps and DataOps stem from a shared philosophy:</p> <ol> <li>DevOps and DataOps both emphasize agility, adapting quickly to changing requirements while maintaining efficiency, rooted in lean manufacturing and agile software development.</li> <li>Cross-functional collaboration is essential in both practices. By reducing handoffs between teams, they ensure workflows are smooth. This helps keep pipelines running efficiently and deliver faster insights.</li> <li>A focus on quality is key in both DevOps and DataOps. DevOps ensures clean, bug-free code and DataOps ensures accurate, reliable data for trustworthy decision-making insights.</li> <li>Infrastructure as code is a foundational principle in both practices. It enables teams to manage systems through version-controlled code to ensure consistency across environments and reduce risks from manual configuration.</li> <li>Version control in DevOps and DataOps allows teams to track changes, review updates, and roll back to previous versions to prevent errors in production.</li> <li>Testing and automation are crucial in both practices. They help identify issues early and ensure reliability, with DevOps focusing on code quality and DataOps on data accuracy throughout the pipeline.</li> </ol> <p>DevOps and DataOps share these principles, but applying them to data introduces unique challenges. While DevOps follows the well-established \"preview, decide, deploy\" cycle, DataOps often lacks this maturity. As CL and Noel pointed out, in many data environments, teams \"deploy first, then decide\"\u2014leading to reactive fixes instead of proactive prevention.</p> <p>Why is DataOps so different from DevOps, despite having the same underlying principles?</p>"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/#key-differences-between-devops-and-dataops","title":"Key Differences Between DevOps and DataOps","text":"<p>DevOps focuses on smooth software deployment without downtime, while DataOps ensures the data driving business decisions is accurate and trustworthy, while revisiting how we extract insights.</p> <p>In DataOps, it\u2019s not just about system uptime; the real challenge is preventing incorrect or incomplete data from leading to flawed decisions or costly errors. This introduces a new layer of complexity, as ensuring data accuracy can be more difficult than ensuring software stability.</p> <p>A key distinction between DevOps and DataOps is the confidence teams have before releasing changes. In DevOps, you can deploy code changes confidently because everything has been tested in a controlled environment before production. In DataOps, even with the right tools, validating data is trickier because real-world data constantly changes and behaves unpredictably. It\u2019s not just about running tests; it\u2019s about verifying that the insights or automation from the data still make sense after transformations and processes.</p>"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/#1-different-goals","title":"1. Different Goals","text":"<ul> <li> <p>DevOps: Focus on Reliable Software Delivery  The primary goal of DevOps is to ensure software functions properly and consistently for users. Teams strive to deploy small, frequent changes confidently, without disruptions or compromising user experience. A significant part of this is validating the application performs as expected\u2014whether it's a website or backend system\u2014without crashes or performance degradation. Even if DevOps achieves perfect functionality delivery, the data generated or consumed by the software may not be accurate. Noel pointed out, \u201cTheir website isn\u2019t going down... but it\u2019s still not delivering the data for analytics and decision-making.\u201d This highlights a key limitation of DevOps: ensuring software stability doesn\u2019t guarantee data correctness.</p> </li> <li> <p>DataOps: Focus on Data Accuracy and Validity DataOps focuses on validating data. The goal is to ensure systems are running and to confirm the data being used or processed is accurate and reliable. This means checking if the data makes sense\u2014whether the joins, transformations, and calculations are correct and result in actionable insights. If data is incorrect or missing, even the best-maintained software won\u2019t provide value, and businesses could make poor decisions based on faulty insights.</p> </li> </ul>"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/#2-the-validation-process","title":"2. The Validation Process","text":"<ul> <li> <p>DevOps: Validating Applications In DevOps, validation focuses on application functionality. Automated tests ensure the application runs correctly, features behave as expected, and the infrastructure remains stable. The primary concern is application stability while introducing changes\u2014ensuring services are consistently available for users without unexpected crashes or interruptions.</p> </li> <li> <p>DataOps: Validating Data DataOps validation focuses on data correctness. Teams must ensure data transformations, aggregations, and insights are accurate before use in decision-making or customer-facing products. This involves automated tests and in-depth data checks to ensure changes in one part of the pipeline don\u2019t affect other areas. For example, an innocent change could lead to missing or duplicated data, affecting downstream dashboards and reports. This makes DataOps more complex and crucial, as faulty data can lead to poor business decisions or financial losses, worsening with more automated systems.</p> </li> </ul>"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/#why-dataops-is-more-complex","title":"Why DataOps is More Complex","text":"<p>Ensuring data correctness introduces more complexity than ensuring software stability due to unique data management challenges, making DataOps a more intricate process.</p> <ol> <li> <p>High Costs of Bad Data Poor data quality can lead to significant financial losses, inefficiencies, and missed opportunities. Minor data errors can propagate through decision-making processes, leading to flawed insights and costly consequences. This makes the stakes in DataOps much higher than traditional DevOps.</p> </li> <li> <p>Data-Centric Software Stacks As software systems become more data-driven, especially with AI and automated decision-making, managing data quality becomes complex. Without a coherent strategy, errors in data pipelines or transformations can lead to inaccurate decisions and a greater need for robust data management processes.</p> </li> <li> <p>Disconnected Teams and Legacy Processes A common issue in DataOps is the disconnect between teams responsible for producing data and those using it, which leads to misunderstandings and inconsistencies. Many organizations still use outdated, manual processes with little awareness of modern data management practices. This absence of governance, code review, and version control makes it challenging to implement best practices and increases the risk of errors.</p> </li> <li> <p>Underrated Data Pipelines Data pipelines are the backbone of decision-making processes, yet they often don't get the respect they deserve. While they may not be as flashy as AI models or analytics tools, everything depends on the quality and reliability of the data flowing through them. Inconsistent or poorly managed pipelines can lead to data issues, undermining the decision-making framework.</p> </li> </ol> <p>As the software stack becomes more data-centric and data plays a larger role in decision-making, especially in AI trends, this complexity increases. Without a structured approach to managing data, companies risk relying on flawed or inaccurate data, leading to costly mistakes.</p> bad data cost 15%~25% of Revenue"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/#practical-dataops-strategies","title":"Practical DataOps Strategies","text":"<p>As the discussion dove into the root causes of DataOps challenges, CL and Noel shared four practical strategies from their work with various companies. Attendees were eager to ask: How do you encourage teams using legacy systems and entrenched processes to adopt best practices? How do you recommend data professionals secure buy-in for implementing modern solutions?</p> <p>The four strategies to improve DataOps are: start with small, manageable improvements, follow a clear blueprint, conduct regular data reviews, and gradually introduce best practices across the team. Each step helps build a strong foundation for effective DataOps implementation.</p> <p>If you're interested in their strategies and answers to these questions, as well as detailed advice on implementing DataOps in your organization,</p> <p>Sign up using the link below to access the full video recording: https://datarecce.io/firesidechat/</p>"},{"location":"blog/from-devops-to-dataops-effective-data-productivity/#closing-thoughts-a-brief-look-at-datacoves-and-recce","title":"Closing Thoughts: A Brief Look at Datacoves and Recce","text":"<p>The fireside chat ended with something that stood out\u2014the passion of these founders. After understanding the challenges teams face with data, CL and Noel focused on different key points, leading them to build two distinct but complementary products.</p> <p>Datacoves, co-founded by Noel Gomez, focuses on accelerating DataOps workflows by integrating open-source tools like dbt. It's designed to be flexible, scalable, and provides teams a clear path to improve data quality without rigid systems. As Noel put it, \"It's really about explaining to people and teaching them what's possible out there because we're very passionate about this kind of stuff.\"</p> <p>Recce, founded by CL Kao, ensures data correctness across the pipeline. It simplifies validating data changes by curating proof of correctness, helping teams maintain confidence in their data as things get more complex. CL's vision is clear: \"We are very passionate about the future of data-driven software and its development workflow.\"</p> <p>What data problems resonate with you? Let's connect and chat!</p>"},{"location":"blog/self-serve-analytics-pr-review/","title":"The Guide to Supporting Self-Serve Data and Analytics with Comprehensive PR Review","text":"<p>dbt, the platform that popularized ELT, has revolutionized the way data teams create and maintain data pipelines. The key is in the \u2018T\u2019, of ELT. Rather than transforming data before it hits the data warehouse, as in traditional ETL, dbt flips this and promotes loading raw data into your data warehouse and transforming it there, thus ELT.</p> <p>This, along with bringing analytics inside the data project, makes dbt an interesting solution for data teams looking to maintain a single-source-of-truth(SSoT) for their data, ensuring data quality and integrity.</p> Move Fast and DON'T Break Prod <p>dbt transformations are stored as a series of SQL models that transform the data through raw, staging, intermediate, and then mart stages, in which the data is ready for production use.  Keeping all of these transformations in the same place, as the SSoT for how your data is generated, ensures that data validation and data quality checks can be systematically applied at every stage. This also facilitates PR review to maintain data integrity.</p> <ul> <li>Version control - the dbt project can be managed in a version control system such as git, essential for dbt CI workflows and PR review.</li> <li>Modular SQL - dbt encourages a modular design to data pipelines. This better facilitates reviewing data models and transformed data, and enabling the re-use and referencing of data models.</li> <li>Reproducible pipelines - the dbt project contains everything required to run the pipeline, which means anyone can checkout the project from version control and build the data. This is a fundamental part of data best practices and dataops.</li> </ul> <p>If you\u2019ve come from a software engineering (SE) background, then these benefits may be familiar to you. Version control, modularity, and reproducibility are the tenets of DevOps, and have benefited software development for years. Through dbt, you\u2019re able to adapt them as part of a new practice of DataOps, and the pull request (PR) is at the center of the process.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#data-incidents-still-happen","title":"Data Incidents still happen","text":"<p>Given the benefits that dbt brings, you\u2019d be forgiven for thinking that the data problem was solved - That data was always accurate, and bad data was never merged into production. Unfortunately, due to the relative infancy of DataOps, that\u2019s still not the case. Data incidents still happen.</p> <p>As CL Kao, founder of Data Recce put it in a recent fireside chat with Noel Gomez of Datacoves:</p> <p>\u201cThe construct that turned devops from a best practice into a viable, productionized, practice, is the pull request\u2026 preview, decide, deploy. (However) In data, we deploy, and then decide if we want to fix it later\u201d- CL Kao, Data Productivity - Beyond DevOps and dbt </p> <p>The \u201cdeploy and then fix it later\u201d quote probably rings true for a lot of data teams that are working under tight deadlines, with many simultaneous PRs, and find it difficult to perform due diligence on every PR before merging.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#look-ma-no-guardrails","title":"Look ma, no guardrails!","text":"<p>The version-controlled nature of dbt makes code review straightforward, but data review remains a challenge. It\u2019s vital to build guardrails around the PR review process to ensure that data quality is upheld and the data impact of changes is thoroughly assessed.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#how-much-does-a-data-incident-cost","title":"How much does a data incident cost?","text":"<p>Merging bad data, whether that be from accidentally changing historic data, to introducing silent errors into production data, can be very costly. Data impact assessment prevents such incidents through rigorous data validation, data quality checks, and structured PR review - Avoiding substantial financial losses caused by bad data.</p> The Cost of Data Incidents <p>Mikkel Dengsoe of Sync recently calculated that if the cost of a data incident ranges from $1,000 to $100,000 per-incident, that for a large data team that could result in costs of \u201cbetween $500,000 and $4,000,000 per year\u201d.</p> <p>As Mikkel puts it, the value of data for business critical applications is high:</p> <p>\u201cA few hours of incorrect data for an eCommerce ad bidding machine learning model can easily cost $100,000.\u201d  Mikkel Dengsoe, The cost of data incidents </p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#why-dbt-data-prs-are-hard-to-review","title":"Why dbt data PRs are hard to reviewDownload the Deck","text":"<p>Firstly, data change is inherently hard to review. The pull request review process from software practices is not suited to reviewing data. PR review is all about reviewing code, not data.  When you open a pull request on a data project, you can easily see how the SQL code and transformation logic has changed, but the data is still a black box, and makes data impact assessment and data validation a unique challenge.</p> New Challenges to Data Integrity <p>Secondly, dbt has introduced \u2018self-serve data\u2019., where more users within an organization can access and manipulate data. This shift means that dataops practices much evolve to ensure that data quality and data integrity is maintained as the number of data users grows.</p> * indicates required Enter your email address and we'll send you the presentation deck: *","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#what-is-self-serve-data","title":"What is self-serve data?","text":"<p>The core principle of self-serve data is to enable wider access to data without the need for specialized knowledge, or the help of data specialists. As atlan says, users should be able to \u201caccess, analyze, and manipulate the data\u201d by themselves:</p> <p>Self-serve data refers to tools\u2026 that enable \u2026users to access, analyze, and manipulate data without requiring the direct intervention of data specialists \u2014 atlan </p> <p>dbt takes it a step further, in that users should not require specialized knowledge to create new data products:</p> <p>A self-serve data platform\u2026 supports creating new data products without the need for custom tooling or specialized knowledge. \u2014 dbt </p> <p>All you need to know is SQL, and you can start using dbt and access and manipulate the data to your needs.</p> <p>The actual implementation of self-serve data will differ from organization to organization, but at the very least you would expect that analysts will be shifting some analytics out of BI and directly into dbt. In other cases, non-technical (in engineering terms) teams, such as finance or marketing, may also be directly accessing the data project.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#data-meet-integrity","title":"Data, meet integrity","text":"<p>An initial worry from data engineering teams is that self-serve data may introduce data integrity issues, and undermine the work of data engineers who are traditionally tasked with maintaining the stability and integrity of data and data pipelines. Indeed, to successfully adopt self-serve analytics, a shift in internal culture is required.</p> Self-serve data doesn\u2019t have to mean self-serve chaos <p>Rather than gatekeeping access to data, data engineering teams should actively support the self-serve nature of data. The pull request process is central to maintaining data integrity. It\u2019s the stage in which data quality is upheld, and the intention of the PR author defined and validated.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#how-to-support-a-self-serve-data-culture","title":"How to support a self-serve data culture","text":"<p>With the PR review being central to ensuring data integrity, the team must validate both the code and the data changes to understand the data impact. Following dbt best practices for data validation and data review is critical to building trust in self-serve data environments.</p> <p>If the author of the PR records the steps that they took to validate the data generated from the models in the PR, this enables:</p> <ol> <li>The PR author is able to review their own work, ensuring data quality.</li> <li>The data team is able to review the work of others (peer review), enhancing DataOps processes.</li> <li>The data validation checks can be shared with others for approval ensuring data integrity by enabling business-context review.</li> </ol> <p>To enable this PR review workflow for data, the PR process for data projects needs to be augmented with suitable tools, enabling 'self-serve review' for self-serve data.</p> Data impact assessment is more important than ever","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#communication-overhead","title":"Communication overhead","text":"<p>As more people modify the data project, more PR review is required, bringing with it communication overhead. The team needs to communicate their adjustments and modifications to the data pipeline that require review, and the data team needs to be able to support this review.</p> <p>To enable better comunication between data teams and stakeholders, we must first understand what the review process for data actually looks like, and understand how to review data.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#how-to-perform-data-impact-assessment-for-pr-review","title":"How to perform data impact assessment for PR review","text":"<p>Data impact is difficult to assess. You\u2019re faced with the problem of having to interpret if the numbers you\u2019re looking at are correct. How can you know that with confidence, ensuring data quality?</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#1-show-me-the-change","title":"1. Show me the change","text":"<p>The best way to perform data impact assessment is to see exactly how the data has changed.</p> <p>Benn Stancil, of Mode Analytics, takes a humorous look at this in a recent blog post. Benn provides two methods. The first, a complicated, convoluted process following the data through contract alerts, and log files. The second, simply:</p> <p>\u201cCheck if historical values of the metric are the same today as they were yesterday.\u201d   - Benn Stancil, All I want to know is what\u2019s different </p> <p>It\u2019s that simple. You want to be able to easily compare data from before-and-after making data model changes. Doing this will, and confirming that metrics or key figures didn\u2019t change will give you the confidence to merge a PR knowing that unintended impact did not occur, therefore maintaining data integrity.</p> Just show me what's different","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#2-help-me-validate-change","title":"2. Help me validate change","text":"<p>Confirming that data didn\u2019t change is good for refactoring work and detecting unintended impact, but there are also instances when you would expect to see data change - You want to confirm that the data is correct, and that the SQL (data model logic change) has the  intended data impact.</p> <p>Comparing data, and the structure of the data pipeline, before-and-after making changes will enable you to see data impact clearly. You want to be able to easily see newly added, removed, and modified data models, and perform data validation checks to ensure accuracy. By doing this, you can assess data quality and look into the models to see how that data has changed, ensuring data integrity throughout the PR review process.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#whats-your-intention-and-expectation","title":"What\u2019s your intention and expectation","text":"<p>To understand if data impact is correct, you need to already have an expectation of what to look for. This expectation is based on your intention, or, what you are trying to achieve. This is why the pull request template is such a fundamental tool, it helps the PR author clarify these things.</p> <ul> <li>Intention - What you trying to do, such as fix a bug, add a new column, refactor a group of models etc.</li> <li>Expectation - Given you intention, what is the expected data impact once the work is completed? For example, no rows should change on models X and Y, or the average value of Z column should remain constant.</li> </ul> <p>Once these are defined, the process of validating data impact becomes a lot easier. The main problem is then the mechanism to perform the data validation checks to prove your expectation, and the groundwork that is needed in your data project to enable that.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#clarity-through-comparison","title":"Clarity through comparison","text":"<p>Verifying your work before pushing to production is the only way to be absolutely confident that there are no breaking changes. In order to do this, you need to have a duplication of your production environment, or a method to reproduce, or replicate, a subset of production, so that you have a known-correct baseline to check your work against.</p> <p>Noel Gomez, of Datacoves, describes the importance of having reproducible environments:</p> <p>\u201cWe want to make sure that we have reproducible environments\u2026 this allows you to experiment. \u2026you\u2019re working in a different environment, you\u2019re not going to break something. (then) there should be a review before (it goes to production). The goal is \u2018Does this data make sense?\u2019\u201d  - Noel Gomez, Data Productivity - Beyond DevOps and dbt </p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#reproducible-environments-in-dbt","title":"Reproducible environments in dbt","text":"<p>Following dbt best practices for how to replicate data environments include:</p> <ul> <li>Per-developer schemas - this allows the person modifying the pipeline to check their work by building models into their own schema. Enabling the developer (data engineer, analyst etc) to perform data quality checks before opening a PR.</li> <li>Per-PR schemas - Each time a PR is opened, the new project code should build the project into a PR-specific schema. This enables automated, dbt CI-time (continuous integration) checks to run (more on that below), helping with data validation and ensuring the PR review process is efficient.</li> <li>Staging schema (subset of prod) - As part of CD (continuous delivery) build a subset of production data into a separate schema. This acts as the known-good baseline and can be used to check developer and PR schemas against. The benefit is that you don\u2019t need to replicate the whole of production, which can be time consuming and expensive.</li> </ul> <p>As Noel mentions in the quote above, having the ability to replicate environments empowers developers\u2014those modifying the data pipeline and validating changes\u2014to confidently assess data quality and data impact without compromising production data. A strategy that streamlines the development process and also adheres to data integrity principles within a self-serve data culture.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#data-validation-with-recce","title":"Data validation with Recce","text":"<p>Once you have a suitable system in place to manage your database environments during development and deployment, you\u2019re then able to perform comparisons between these environments. This is where Recce can help enhance your data validation process and improve your dbt PR review.</p> <p>Recce provides a suite of tools that are specially designed to assess data impact by comparing the data in two dbt environments.</p> <ul> <li>You\u2019re clear what your intention is</li> <li>You\u2019re clear what your expectation is</li> <li>Now, prove it</li> </ul> <p>Recce helps you to prove that your data model changes have the intended impact through its data impact check framework. Recce data checks, and enable you to:</p> <ul> <li>compare your data</li> <li>record your findings</li> <li>share those findings</li> </ul>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#recce-checks","title":"Recce Checks","text":"<p>Recce checks are individual data validations that show the results of a specific type of comparison between your data environments. These checks help to prove that your intention was successful, and your expectation has been realized in the data. You use these check results for:</p> <ul> <li>validating your work, as you work</li> <li>sharing as part of discussion between colleagues or stakeholders</li> <li>sharing as proof-of-correctness of your work</li> <li>allowing others to approve the results for data validation</li> </ul> <p>There are various check types that range from high level profiling, right down to row level data inspection. Each type of check provides an insight into how data differs, or not, compared to the previous state. By recording the results of your checks, you can demonstrate that proper impact assessment has been carried out and share the results with others for data QA. </p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#structural-checks","title":"Structural Checks","text":"<p>Structural checks provide an overview of structural impact to the data project.</p> <ul> <li>Lineage Diff - Shows how the lineage has changed in the PR and the dependencies between models.</li> <li>Row Count Diff - Shows the row count change for models that gives a high level indication that the amount of data is the same.</li> <li>Schema Diff - Shows if any models have added, removed, or renamed columns, providing insight into data structure modifications.</li> </ul>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#statistical-checks","title":"Statistical Checks","text":"<p>Statistical checks provide a more detailed look at the data which helps to determine the type of data impact that may have occurred.</p> <ul> <li>Profile Diff - Shows the comparison of statistical profiles, such as mean, median, standard deviation, and distinct count, between the current and previous data.</li> <li>Value Diff - Shows the percentage matched rate for each column between the current and previous data.</li> <li>Top-K Diff - Shows the most frequent values in categorical fields between current and previous data.</li> <li>Histogram Diff (distribution) - Shows an overlay comparison of data distribution between current and previous data.</li> </ul> Detect statistical anomalies with Data Profile Diff","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#row-level-checks","title":"Row-Level Checks","text":"<p>Row-level checks enable you to do data spot-checks on specific rows.</p> <ul> <li>Query Diff - Run any query and see how the results of that query differ between current and previous data.</li> </ul> <p>Checks can be added to the Recce Checklist as part of your validation work, or they can be added automatically (see below).</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#recce-checklist","title":"Recce checklist","text":"<p>Organized into a checklist, Recce checks help to collectively validate a PR, ensuring data quality and data integrity throughout the development process. Each check contains:</p> <ul> <li>Check results - The results of a specific check that helps to validate the success of the PR.</li> <li>Author\u2019s annotation - A description that explains what is being shown and why the check results are vital for understanding the data quality and data integrity of the PR.</li> </ul> <p>The checklist helps to guide the reviewer through the process that was used to validate the PR. When accompanied by a detailed PR comment, will enable the reviewer to understand the author's intention and expectation, allowing for informed sign-off on the PR or requests for further data checks.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#automate-checks-for-critical-models-preset-checks","title":"Automate checks for critical models (preset checks)","text":"<p>In addition to checks that fall within the scope of the PR, there should also be global checks that run regardless of the work being done for the PR. These global checks are called Preset Checks in Recce, are designed to ensure complete data quality coverage of your project, and target models that have:</p> <ul> <li>Significance importance to the business,</li> <li>Numerous downstream dependencies,</li> <li>Historical issues that may prompt inquiries about data integrity if problems arise (in other words, you'll get a call about that data if something goes wrong!)</li> </ul> <p>Read more about the importance of identifying and checking critical dbt models as part of data validation strategry for dbt and maintaining data quality in dbt PR reviews.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#share-validation-check-results","title":"Share validation check results","text":"<p>Once you\u2019ve curated a checklist of data validation checks, it\u2019s time to share those checks in your PR comment. With Recce you can do this in two ways:</p> <ol> <li>Share individual checks for focused data impact discussions</li> <li>Share the Recce File, which contains the complete checklist and allows other team members to continue review or help validate impact.</li> <li>Use Recce Cloud for to sync checks across Recce instances and block PR merging until all data impact has been assessed</li> </ol>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#sharing-individual-checks","title":"Sharing individual checks","text":"<p>Sharing individual checks can be useful for smaller PRs, or when you want to have discussion around the results. For more complex PRs, with many checks, the Recce File is the more powerful choice for collaborating on data validation.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#sharing-the-recce-file","title":"Sharing the Recce File","text":"<p>Exporting the Recce File offers two key advantages:</p> <ol> <li>The full checklist is made available for review, enhancing transparency in your data validation processes.</li> <li>Reviewers can interact with data checks and perform and save additional checks if necessary. </li> </ol> <p>The PR author can export the Recce File, then attach it to the PR, or save it in your organization\u2019s shared storage platform. Another colleague can then run Recce in \u2018review mode\u2019 to start the PR review, facilitating thorough data validation and promoting teamwork in reviewing data PRs. </p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#recce-cloud","title":"Recce Cloud","text":"<p>For those using Recce Cloud, the process is a lot smoother, with checks synced between Recce Instances, and the ability to launch a Recce Instance and review a PR within minutes from the Recce Cloud interface.</p> <p>With Recce Cloud, there\u2019s no need to export and share the Recce File, or deal with manually exporting checks.</p> <p>Recce Cloud integrates with your GitHub PR and can block merging until all checks have been reviewed. </p> Download the Deck * indicates required Enter your email address and we'll send you the presentation deck: *","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/self-serve-analytics-pr-review/#conclusion","title":"Conclusion","text":"<p>dbt brings a lot of benefits to data projects by incorporating essential software engineering practices into the data analytics workflow. By embracing and adopting organizations can support a self-serve data culture that empowers teams to access and leverage data effectively, while also maintaining data integrity. </p> <p>For discussion on how to properly adopt devops best practices for datatops, check out the Data Productivity - Beyond DevOps and dbt Fireside Chat between the founders of Recce and Datacoves.</p>","tags":["Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","dbt PR Review","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/meet-recce-at-coalesce-2024-and-the-data-renegate-happy-hour/","title":"Meet Recce at Coalesce 2024 and The Data Renegade Happy Hour","text":"<p>The Recce team will be joining Coalesce 2024 in Las Vegas! Meet our founder, CL Kao, and product manager, Karen Hsieh, who has also been hosting the Taipei dbt meetups. As a company focused on helping data teams prevent bad merges and improve data quality, we believe Coalesce is the perfect venue to connect with fellow data professionals, share insights, and gain fresh perspectives.</p> We are attending Coalesce 2024 <p>At Recce, our mission is to transform the data PR review process, ensuring that data pipelines not only run smoothly but also deliver accurate, validated results. We believe that data should be correct, collaborative, and continuously improved. Coalesce 2024 offers an ideal platform for these crucial conversations, gathering experts across the field to discuss the future of data management. Whether it\u2019s gaining new insights into best practices or forging valuable partnerships, Coalesce is where we aim to make an impact.</p>"},{"location":"blog/meet-recce-at-coalesce-2024-and-the-data-renegate-happy-hour/#coalesce-provides-invaluable-insights","title":"Coalesce provides invaluable insights","text":"<p>This journey didn\u2019t start overnight. The idea for Recce was born from deep conversations with data practitioners at Coalesce 2023 in San Diego. At the time, CL Kao was working on PipeRider, a data impact assessment tool for dbt data projects.</p> <p>It became clear from the conversations that the need for a more comprehensive solution\u2014something beyond data comparisons and observability\u2014was critical. From those learnings, CL pivoted PipeRider into what is now Recce, focused on improving the entire data PR review process. Coalesce 2023 provided invaluable insights, and this year, we\u2019re returning with an even stronger vision.</p> <p>The Analytics Development Lifecycle (ADLC) white paper sets the stage for the discussions at Coalesce, and we\u2019re eager to dive in. Trends like DataOps, automation, collaboration and data quality resonates deeply with Recce\u2019s goals. While modern data practices draw significant inspiration from software engineering, data has its own unique challenges. One such challenge is that simply applying software methods directly to data isn\u2019t always effective. Data review, for example, requires more than just SQL code reviews; it demands a thorough examination of the actual data.</p>"},{"location":"blog/meet-recce-at-coalesce-2024-and-the-data-renegate-happy-hour/#the-data-renegade-happy-hour-where-data-power-meets-happy-hour","title":"The Data Renegade Happy Hour: Where Data Power Meets Happy Hour","text":"<p>It\u2019s not just all work! Coalesce also gives us the chance to let loose and connect in a more relaxed setting. That\u2019s why we\u2019re teaming up with some of the brightest minds in data to co-host a special event:</p> <p>Join the team from Recce and Tobiko, Cube, Paradime.io, Datacoves, and Steep on October 8th at the Data Renegade Happy Hour.</p> Data Renegade Happy Hour <p>It\u2019s set to be a fun-filled evening where you can connect with industry pros over witty banter, enjoy data-themed cocktails, and be amazed by world-class magic from international champion John George. Don\u2019t miss the perfect blend of insights and entertainment.  RSVP Soon, space is limited!</p> <p>RSVP Today!</p>"},{"location":"blog/meet-recce-at-coalesce-2024-and-the-data-renegate-happy-hour/#chat-with-us","title":"Chat with us","text":"<p>We are looking forward to engaging with the data community on some of the most pressing challenges facing teams today, including how to prevent bad merges, validate results, and perform robust impact analysis. Every data practitioner who\u2019s ever created or reviewed a PR knows the frustration of spot checks and incomplete data analysis.</p> <p>We\u2019re also curious to understand what\u2019s holding teams back from addressing these issues. Why do 74% of business stakeholders report revenue loss due to poor data quality, yet many data teams still struggle to prioritize improvements? Everyone talks about the need for high-quality data, but not everyone is taking action. We want to hear how different industries, company sizes, and data teams approach these challenges and discuss how Recce can play a role in solving them\u2014or even understand why Recce might not be the right fit in some cases.</p> <p>Stay tuned for more details about where to find us during the event! We can\u2019t wait to meet you all, exchange ideas, and explore how we can transform the way data is managed\u2014stickers and limited swags are waiting for you, too.</p>"},{"location":"blog/dbt-data-pr-comment-template/","title":"The Ultimate PR Comment Template Boilerplate for dbt data projects","text":"<p>If you\u2019re looking to level-up your dbt game and improve the PR review process for your team, then using a PR comment template is essential. </p> <p>A PR template is a markdown-formatted boilerplate comment that you copy and paste into the comment box when opening a PR. You then fill out the relevant sections and submit the comment with your PR.</p> Example of a PR comment with comprehensive data validation checks","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#the-benefits-of-using-a-pr-comment-template","title":"The benefits of using a PR comment template","text":"<p>The sections in the template help by providing a systematic approach to defining and checking your work. Having a template also helps to avoid ambiguous or superficial comments which make PRs difficult to review. The benefits of using a PR comment template for modern dbt dataops are:</p> <ol> <li>As the PR author, the structure of the template helps you to define and describe your work related to this PR, which helps to you to perform data validation and understand data impact.</li> <li>As the PR reviewer, the sections help you to review the PR by following the logical steps that the author took by seeing exactly what work as done, and how it was checked. Making data quality and data integrity checks integral to the PR review process. </li> <li>For future you, it provides a historical record of what was done and why, which is invaluable as part of data best practices. </li> </ol> <p>Check the end of the article for some examples from the data projects of Prefeitura do Rio de Janeiro and the California Integrated Travel Project (Cal-ITP), who both make deft use or PR comment templates to improve their pull request process.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#where-can-you-get-a-dbt-pr-comment-template-from","title":"Where can you get a dbt PR comment template from?","text":"<p>dbt provides an official pull request template that you can paste into your comment when you open a PR. It\u2019s a great template, and an essential foundation to dbt best practices but, due to the complicated nature of reviewing dbt data project pull requests, there's still room for improvement.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#the-official-dbt-pr-comment-template","title":"The official dbt PR comment template","text":"<p>The official dbt template includes the following sections:</p> <ul> <li>Description &amp; motivation</li> <li>To-do before merge</li> <li>Screenshots</li> <li>Validation of models</li> <li>Changes to existing models</li> <li>Checklist (of small things that are sometimes overlooked)</li> </ul> <p>At Recce, we\u2019re all about validating your work - providing the proof-of-correctness that shows why your work is complete and the PR can be merged. So, we feel this PR comment template can be improved to make it even better suited to this job.</p> <p>Let\u2019s add a few new sections, and update/re-define some of the existing sections to make them clearer.</p> What sections should the perfect PR comment contain?","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#making-a-better-pr-comment-template-for-dbt-data-projects","title":"Making a better PR comment template for dbt data projects","text":"<p>To improve the dbt PR comment template and make your data quality check processes even better, we can start with adding some new sections that will clarify the purpose of the PR and make it easier to review.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#type-of-change","title":"Type of change","text":"<p>Classifying the type of change helps to frame the work. It guides the PR author toward the type of validations they should run based on the category of work, and the reviewer will have an initial idea how to properly review the work </p> <p>A sensible set of change types to start with could be as follows:</p> <ul> <li>[ ]  New model</li> <li>[ ]  Bugfix</li> <li>[ ]  Refactoring</li> <li>[ ]  Breaking change</li> <li>[ ]  Documentation</li> <li>[ ]  (Other project-specific item)</li> </ul>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#related-issues","title":"Related Issues","text":"<p>Discussion surrounding a piece of work can happen anywhere, and likely starts with a ticket, chat, or Github issue etc. Provide links to any related discussion that can help clarify your work. This is essential to validating data quality with context. (dbt covers this in the \u2018description\u2019 section, but I prefer to have this in its own section.)</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#impact-considerations","title":"Impact considerations","text":"<p>In this section, include validation on how downstream models have/have not been impacted and what considerations are required. This is crucial for understanding data impact and helps ensure that critical models or exposures remain unaffected. As with <code>validation of models</code>, use screenshots and queries to illustrate the impact.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#updated-sections","title":"Updated Sections","text":"<p>Some of the sections could also be better defined, and potential use clarified:</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#screenshots-lineage-dagdiff","title":"Screenshots \u2192 Lineage DAG/Diff","text":"<p>You can use screenshots anywhere in the PR comment, so let\u2019s change this section to be \u2018Lineage DAG\u2019 and use it for specifically showing the parts of the DAG that have been impacted by this PR. This is where you\u2019d share the Lineage Diff screenshot from Recce that shows only the <code>modified+</code> part of the DAG after comparing it to before you made any changes.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#validation-of-models","title":"Validation of models","text":"<p>This is how you prove that your work is correct and complete. Arguably the most important section of the PR comment as it\u2019s how the reviewer will know that you\u2019ve conducted a thorough data validation.</p> <p>The dbt template mentions sharing dbt test results in this section, but depending on the size of the your project, the test results might crowd out important, PR-specific, data validations. So we\u2019ve moved dbt test results to their own section below.</p> <p>This section should consist of custom queries, profiling stats, data quality checks, data comparisons, and diffs, examples of data impact, anything that helps to validate your work and serve as proof-of-correctness. Use the Recce data validation toolkit to check your work, and include any of the relevant check results, such as</p> <ul> <li>Ad-hoc queries \u2014 Spot-check queries to confirm the results are as expected</li> <li>Profiling stats \u2014 Do you see the expected impact in the overall profile of the data</li> <li>Value diff stats \u2014 Compare the percentage of matched rows between dev and prod</li> <li>Profile diff \u2014 A statistical comparison of dev and prod</li> <li>Schema diff \u2014 If the schema has changed and if it\u2019s intended</li> </ul> <p>Take screenshots of the checks that you ran and post them as validation of models.</p> <p>Pro Tip</p> <p>Export the Recce State File and attach it to PR comment. The Recce State File contains your checks and the PR reviewer will be able to load the checks in Review Mode to view the results.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#dbt-test-results","title":"dbt test results","text":"<p>As mentioned above, depending on the size of your project, you may consider moving dbt test results to their own section. This enables your validation of models section to focus on the checks that help to validate your PR-specific work.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#the-updated-pr-comment-template-for-dbt-data-projects","title":"The updated PR comment template for dbt data projects","text":"<p>Here\u2019s the updated PR comment template that you can use to adapt for your needs. As mentioned, this is adapted directly from the official dbt example, with the changes that were detailed above applied. By aligning with dbt best practices, and augmenting with Recce data checks, this template enhances data integrity and supports a more streamlined PR review process.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#what-a-pr-template-looks-like-in-action","title":"What a PR Template looks like in-action","text":"<p>To see what this PR comment looks like in action check out this demo PR comment that shows how using this template together with Recce data validation checks, can create the ultimate PR comment. The data quality check results listed help ensure data impact is accurately assessed.</p> <p>If every PR comment had the data validation check results listed, you\u2019d be able to sign-off a PR and merge in no time.</p> A demo showing a PR comment template in use with Recce data validations","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#whos-using-pr-comment-templates-like-this","title":"Who\u2019s using PR comment templates like this?","text":"<p>For teams who collaborate on data changes and hold data integrity in high regard, the use of a PR comment templates can streamline the PR review process. As is demonstrated in these real-world examples.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#municipal-government-of-rio-de-janeiro","title":"Municipal government of Rio de Janeiro","text":"<p>A great example of a PR template in action is by data team for the municipal government of Rio de Janeiro (Prefeitura do Rio de Janeiro). The team there uses a PR template and Recce checks for their validation of models, ensuring data quality and data integrity throughout the data review process:</p> Prefeitura do Rio de Janeiro uses Recce in their pr comment","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#the-california-integrated-travel-project-cal-itp","title":"The California Integrated Travel Project (Cal-ITP)","text":"<p>Cal-ITP is a textbook example of dbt best practices with their extensively detailed PR comments, and supplemental data impact assessments.</p> <ul> <li>Previous and resolved issues are linked.</li> <li>The work is clearly described with reasoning.</li> <li>The reviewer is tagged with questions about changes.</li> <li>Post-merge actions are defined, adhering to dataops best practices.</li> </ul> Cal-ITP is a textbook example of data due diligence","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/dbt-data-pr-comment-template/#conclusion","title":"Conclusion","text":"<p>Using a PR comment template for dbt projects is an excellent way to streamline the pull request process, ensuring that both authors and reviewers are aligned on what changes have been made and how those changes have been validated. </p> <p>The updated PR comment template not only helps to define the work contained in the PR, but also helps the PR author to consider and anticipates the types of data impact that may occur. Using Recce\u2019s data validation checks, such as spot-check queries, profile diffs, and schema comparisons, the validation process is standardized, which helps speed up the PR review and time-to-merge.  </p> <p>Teams like Cal-ITP and the municipal government of Rio de Janeiro have adopted PR comment templates and use them as part of their PR process to improve reviewability and ensure data integrity.</p> <p>For your dbt projects, use this structured approach to PR comments to achieve faster, more thorough reviews, and ensure that any changes you introduce follow dbt best practices and are well-documented and correctly validated before merging.</p>","tags":["dbt PR Review","Data Validaton","Data Integrity","Self-Serve Data","Self-Serve Analytics","dbt CI","Data Review","Best Practices","Data Quality","DataOps"]},{"location":"blog/explore-data-impact-with-focus/","title":"Explore data impact and focus on tracking data validations with Recce's new interface","text":"<p>There\u2019s nothing worse than being distracted while you\u2019re in the middle of working on a complex task or debugging an issue.  All the things that you were juggling in your mind come crashing down, and you have to pick up the pieces and start again.</p> <p>Recce's updated interface lets you stay on track while assessing and exploring data impact in your dbt project when making dbt data model changes, and performing dbt PR review.</p>","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/explore-data-impact-with-focus/#track-and-record-data-impact-assessment-results","title":"Track and record data impact assessment results","text":"<p>Staying in context is especially important when you\u2019re running data validations to verify your dbt data models, exploring data change and impact, or finding the root cause of a data incident. You need to:</p> <ul> <li>Keep track of the data models that you are checking.</li> <li>Keep track of the type of data validations that you are running.</li> <li>Record the results of your data validations and impact checks.</li> <li>Record the meaning of the results.</li> </ul> <p>It\u2019s important to record and track these things as you validate your work during data modeling, and then also after opening a PR on your dbt data project.</p>","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/explore-data-impact-with-focus/#how-to-explore-data-impact-without-being-distracted","title":"How to explore data impact without being distracted","text":"<p>This is why, in the latest version of Recce, you can stay in context while adding data validation checks and continue from where you left off after adding a check. </p> <ol> <li>Explore changes in your dbt data project by running data validations.</li> <li>Add a data validation check to your checklist.</li> <li>Continue where you left off.</li> </ol> Explore data impact and focus on tracking data validations with Recce's updated interface","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/explore-data-impact-with-focus/#data-comparison-results-show-potential-impact","title":"Data comparison results show potential impact","text":"<p>When exploring data impact with Recce, the Lineage Diff is the main interface. The default view is <code>modified+</code> and so represents the potential impact radius of your changes. </p> <p>Lineage Diff is where you\u2019ll focus your data impact assessment efforts, so it\u2019s essential that it stays visible during your exploration. This means you can stay focused on your job of validating the correctness of your work, without having to leave your train-of-thought to view results on another page.</p>","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/explore-data-impact-with-focus/#perform-multiple-checks-without-losing-context","title":"Perform multiple checks without losing context","text":"<p>Now, when you run a diff, the results are shown in the the panel below. There\u2019s no need to leave the lineage diff interface; and you can see the lineage, schema, and diff results, all displayed together.</p> <p>If you do add the results of a diff to your checklist, when you return to the Lineage view, everything is exactly as you left it, so you can continue your work.</p>","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/explore-data-impact-with-focus/#model-and-row-level-diffs","title":"Model and row-level diffs","text":"<p>You can also run model-level diffs such as data profiling, or column-level diffs like histogram and top-k, right from the main interface. The results for each diff will be shown right below the lineage.</p> <p>The new interface means you can find and explore data impact without losing your flow state. Then, when you\u2019ve got the check results you need, add them to your checklist.</p>","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/explore-data-impact-with-focus/#save-and-share-data-checks-with-your-team-for-pr-review","title":"Save and share data checks with your team for PR review","text":"<p>To save a diff to your checklist, click the \u2018Add to Checklist\u2019 button and you\u2019ll be taken to the checklist to add your check annotation. Then go back to the Lineage tab to continue where you left off.</p> <p>When you\u2019ve finished your data validation work, you can export the checklist as a Recce File and share with your colleagues, or add checks to your PR comment. If you\u2019re a Recce Cloud user, your checks and approval status will be automatically synced.</p>","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/explore-data-impact-with-focus/#join-us-to-make-data-productive","title":"Join us to make data productive","text":"<p>For early access to Recce Cloud, and a hand in making Recce better meet your needs, book a meeting for a demo and chat. We're looking forward to hearing from you!</p>","tags":["data validaton","dbt data model validation","root cause analysis","data exploration","DataOps","dbt models","data impact","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/","title":"Recce: Your data change management toolkit","text":"<p>Whether you\u2019re the author of a pull request or the one reviewing it, you\u2019ve got a tough job: figuring out what changed, verifying that the PR does what it\u2019s supposed to, and making sure nothing breaks in production. In large or business-critical dbt projects, this can be a slow, frustrating process. That\u2019s why we built Recce - an open-source toolkit that\u2019s here to make your data modeling validation and pull request (PR) reviews a breeze.</p> Build the ultimate PR comment to validate your dbt data modeling changes","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#what-is-recce","title":"What is Recce?","text":"<p>Recce (pronounced \u201creh-kee\u201d, short for \u201creconnaissance\u201d) is a suite of change management tools designed to help you compare dbt environments, assess data impacts, and streamline your PR reviews. Recce gives you visibility into the effects of your data modeling changes before they hit production. With Recce, you can take two dbt environments, such as dev and prod, and compare them using the suite of diff tools.</p>","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#your-diffing-toolkit","title":"Your diffing toolkit","text":"<p>With Recce you\u2019re able to validate your data modeling changes against a known-good baseline, comparing datasets before and after your modifications, in a risk-free environment. And there\u2019s a diff for every occasion.</p>","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#lineage-dag-diff","title":"Lineage DAG diff","text":"<p>Start from the zone of impact of your changes, and see which models have been modified, added, and removed. Unlike the dbt docs lineage DAG, which only shows you the current state of the DAG, Recce shows you how the DAG differs from both before and after your changes.</p> See modified, added, and removed dbt models with breaking change analysis","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#data-profile-diff-and-value-diff","title":"Data profile diff and value diff","text":"<p>Perform holistic checks by diffing the data profile stats for your development branch, then check the percentage of values matching for each column in a model.</p> Perform holistic checks by diffing data profile stats","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#query-diff","title":"Query diff","text":"<p>If something needs further investigation, drill down and query the data. One query will run on both environments, and you\u2019ll be able to see the difference on a row-by-row basis. Enable change-only view to see just what\u2019s changed.</p> Drill down and query the data","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#schema-and-row-count","title":"Schema and row count","text":"<p>In addition to the above diffs, you can also check the schema and row count, just to be sure you didn\u2019t lose any data, or an important column.</p> Ensure data integrity with schema and row count checks","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#youve-been-hard-at-work-time-to-show-it","title":"You\u2019ve been hard at work, time to show it","text":"<p>As you create validations in Recce, you can add them to your curated checklist with notes about what you found, and re-re-run checks if the data changes.</p> Curate a data validation checklist <p>Once you\u2019ve validated your changes, it\u2019s time to share your work. Recce lets you export your checks directly into your PR comment template, so you can provide clear, proof-of-correctness evidence. You can copy key notes, grab a screenshot of the validation results, and include only the relevant details, keeping your PR comment all-signal, no noise.</p> <p>For reviewers, this means they can quickly see the queries and results of your data spot-checks, making it easy to assess the impact of your changes. With all the context at hand, they can either ask for further investigation or confidently approve the PR.</p> Level up your PR comments <p>## Getting started with Recce</p> <p>Ready to revolutionize your data review process? Recce is open-source and easy to integrate into your workflow.</p> <p>Recce OSS is available on GitHub. Follow the instructions in our Getting Started guide to start using Recce to validate your data modeling changes.</p> <ul> <li>GitHub: DataRecce/Recce</li> <li>Docs: DataRecce.io/docs</li> <li>Discord: Recce Community</li> </ul>","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#try-recce-online","title":"Try Recce Online","text":"<p>If you want to try Recce out without having to install, check out our demo instance.</p>","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#demo","title":"Demo","text":"<p>The demo PR makes a simple change to the dbt\u2019s Jaffle Shop project and changes how <code>customer_lifetime_value</code> (CLV) is calculated by fixing the calculation to only evaluate completed orders.</p> Code Diff in Recce <p>The expectation from this change is that CLV will be reduced overall, and that this will also impact the customer segments downstream model. With that in mind, see if you can determine if the if the PR has any issues by checking the data in Recce:</p> <ul> <li>The PR: https://github.com/DataRecce/jaffle_shop_duckdb/pull/1</li> <li>Recce Demo instance: https://pr1.cloud.datarecce.io/</li> </ul> <p>Hint: Run a Profile Diff, then a Query Diff, on the customers model. Then check for downstream impact.</p> <p>For even more details on using Recce to perform data impact assessment, check out our hands-on guide.</p>","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-data-change-management-toolkit/#start-shipping-data-models-with-confidence-with-recce","title":"Start shipping data models with confidence with Recce","text":"<p>Data modeling changes shouldn\u2019t feel like a gamble. Whether you\u2019re the one writing the PR or the one reviewing it, you need confidence that what\u2019s changing is actually what was intended \u2014without breaking production. Recce gives you the tools to compare environments, validate your data, and surface meaningful insights, all while keeping PR comments focused and actionable. </p> <p>If you\u2019re tired of slow QA cycles, silent data errors, and bloated CI pipelines, it\u2019s time to give Recce a shot.</p>","tags":["data validaton","dbt","best practices","code review","impact assessment"]},{"location":"blog/recce-soc2-compliant/","title":"Recce Is Now SOC 2 Type 1 Compliant!","text":"<p>At Recce, we take the security and integrity of your data seriously. Today, we're excited to announce that Recce has successfully achieved SOC 2 Type 1 compliance, marking a significant milestone in our commitment to protecting your data and maintaining the highest standards of security.</p> <p>Find details in our Trust Center.</p> Recce has achieved SOC 2 Type 1 compliance!","tags":["Security","Compliance"]},{"location":"blog/recce-soc2-compliant/#what-does-soc-2-type-1-compliance-mean","title":"What Does SOC 2 Type 1 Compliance Mean?","text":"<p>Developed by the American Institute of Certified Public Accountants (AICPA), SOC 2 (System and Organization Controls) is an industry-leading standard designed to ensure companies manage customer data securely. A SOC 2 Type 1 audit evaluates and confirms that an organization has robust security controls, policies, and practices in place at a specific point in time.</p> <p>Our SOC 2 Type 1 compliance assures that Recce's security measures meet rigorous standards, validating our ongoing dedication to safeguarding your critical data.</p>","tags":["Security","Compliance"]},{"location":"blog/recce-soc2-compliant/#why-this-matters-for-recce-customers","title":"Why This Matters for Recce Customers","text":"<p>Recce helps data teams contextualize and validate data impacts through powerful and reproducible testing environments integrated directly into your development workflows. Our platform delivers actionable visibility, verifiability, and velocity for your data changes\u2014empowering you to merge with confidence.</p> <p>By obtaining SOC 2 compliance, Recce reinforces our commitment to:</p> <ul> <li>Security: Implementing stringent access controls, encryption, and continuous monitoring to protect your data.</li> <li>Availability: Ensuring our platform remains consistently accessible, reliable, and resilient.</li> <li>Confidentiality: Protecting your sensitive data with comprehensive privacy and data handling measures.</li> </ul>","tags":["Security","Compliance"]},{"location":"blog/recce-soc2-compliant/#continuous-commitment-to-security-and-transparency","title":"Continuous Commitment to Security and Transparency","text":"<p>This SOC 2 Type 1 report, verified by independent auditors, provides our customers with trusted assurance that Recce has established and maintains effective security controls.</p> <p>Achieving SOC 2 compliance demonstrates our continued focus on transparency, helping our customers confidently incorporate Recce into their data workflows, knowing their data is secure, protected, and handled responsibly.</p> SOC 2 Type 1 compliance verified by AssuranceLab","tags":["Security","Compliance"]},{"location":"blog/recce-soc2-compliant/#whats-next","title":"What's Next?","text":"<p>Our SOC 2 Type 1 report is available upon request to customers and stakeholders under NDA through our dedicated Trust Center. We\u2019re committed to continuous improvement in our security practices, including moving toward SOC 2 Type 2 compliance, to ensure we always provide industry-leading protection.</p> <p>To learn more about our security and compliance practices, visit our Trust Center or explore the Recce documentation.</p> <p>Thank you for trusting Recce with your data. We\u2019re excited to continue empowering your teams to deliver data-driven value faster and with confidence.</p> <p>\u2014 The Recce Team</p>","tags":["Security","Compliance"]},{"location":"blog/meet-recce-at-datacouncil-2025-and-the-data-reboot-data-renegades-happy-hour/","title":"Meet Recce at Data Council and Data Reboot, Data Renegades Happy Hour","text":"<p>Data Council is one of our favorite conferences: thousands of Data &amp; AI practitioners, hundreds of cutting-edge talks, and endless opportunities to connect and be inspired. If you're in the data space, it's the event you don\u2019t want to miss.</p> <p>At Recce, we love talking to people who\u2019ve encountered the challenges we\u2019re passionate about. We believe data development is fundamentally different from software development. Yes, modern data teams borrow practices from software \u2014 version control, CI/CD, testing. But data comes with ambiguity. A 0.5% change in daily active customers might not mean much without business context. Is it correct? Is it broken? It depends.</p> <p>More and more data teams are feeling this. Reviewing a SQL snippet isn\u2019t enough.Context matters. How do teams work with the business to spot the real impact of changes? How do we prevent bad data from slipping into production unnoticed?</p> <p>To explore these questions and meet fellow data practitioners, we\u2019re hosting two events at Data Council 2025:</p>"},{"location":"blog/meet-recce-at-datacouncil-2025-and-the-data-reboot-data-renegades-happy-hour/#monday-data-reboot-lightning-talks-happy-hour","title":"Monday: Data Reboot \u2014 Lightning Talks + Happy Hour","text":"Data Reboot <p>Kick off the week with short, fun, and thought-provoking talks. Then grab a drink and meet fellow attendees before the main conference starts.</p> <p>\ud83d\udcc5\u00a0Date: Mon 21st April 2025, 6-9 PM</p> <p>\ud83d\udccdPlace: Near the conference</p> <p>\ud83c\udfab\u00a0RSVP: lu.ma/ro29lcyb</p>"},{"location":"blog/meet-recce-at-datacouncil-2025-and-the-data-reboot-data-renegades-happy-hour/#tuesday-data-renegades-break-the-rules-build-the-future","title":"Tuesday: Data Renegades \u2014 Break the Rules, Build the Future","text":"Data Renegades <p>A casual happy hour with bold conversations, lessons learned, and good company. Co-hosted with Tobiko Data, Datacoves, and Bauplan, this is your chance to connect, unwind, and trade ideas with fellow rebels in data.</p> <p>\ud83d\udcc5\u00a0Date: Tue 22nd April 2025, 7-10 PM</p> <p>\ud83d\udccdPlace: Sobre Mesa</p> <p>\ud83c\udfab\u00a0RSVP: lu.ma/ctsozuun</p> <p>Throughout the conference, we\u2019d love to hear from you. Whether it\u2019s about CI/CD for data, bridging business and data gaps, or building confidence in your data PR workflow, come say hi! \ud83d\udc4b</p>"},{"location":"blog/column-level-lineage-options-for-dbt/","title":"Why Column-Level Lineage Matters for dbt: Comparing the Options","text":"<p>Column-level lineage was a hot topic during 2024, with many data platforms and tools adding this feature to their software and boasting the benefits of column-level lineage for data workflows.</p> <p>In this article, I\u2019ll take a look at how column-level lineage works in dbt, in both open-source and dbt Cloud, and through SQLMesh; and see how the usage and workflow differs between these platforms.</p> Column-level lineage in dbt Explorer showing columns as nodes <p>Seeing lineage in your transformation platform, especially during data modeling updates, is particularly important because it\u2019s at this stage that you most need to understand the impact of your actions.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#why-model-and-column-level-lineage-matter","title":"Why model and column-level lineage matter","text":"<p>Model-level lineage is already extremely valuable. It\u2019s the main interface for performing data validation in Recce (through the unique lineage diff view), and provides a holistic overview of your data project which supports the analysis of data flow for root cause and data impact assessment work.</p> <p>As data projects become more complex, the use of lineage DAGs for visualization becomes more and more important. If you\u2019re making a change to a critical model that is used in many downstream metrics, or a bottleneck model with 100s of downstream, being able to actually see those models is invaluable.</p> Example of Lineage Diff in Recce - invaluable for identifying critical models <p>While model-level lineage gives you a high-level view of data flow, it doesn\u2019t tell you how specific columns are evolving through the data project in transformations \u2014 that\u2019s where column-level lineages comes in.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#take-it-to-the-next-column-level","title":"Take it to the next (column) level","text":"<p>Column-level lineage takes it to the next level, literally, by providing a more in-depth look at how data moves through your data project. Tracking the evolution of a column, and the data it contains, helps you to analyze exactly why the numbers are the way they are, and where they\u2019re going next.</p> <p>Common use-cases for column-level lineage are:</p> <ol> <li>Source Exploration: During development, column-level lineage helps you understand how a column is derived.</li> <li>Impact Analysis: When modifying the logic of a column, column-level lineage enables you to assess the potential impact across the entire DAG.</li> <li>Root Cause Analysis: Column-level lineage helps identify the possible source of errors by tracing data lineage at the column level.</li> </ol> <p>Rather than use pros and cons for each platform, I\u2019ve highlighted the key points for each platform. This is because your specific use-case will determine whether these points are pros or cons.</p> <p>Let\u2019s dive in.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#dbt-column-level-lineage","title":"dbt column-level lineage","text":"<p>There are two main ways to access column-level lineage for dbt:</p> <ul> <li>The open-source Power User for dbt (VSCode plugin)</li> <li>dbt Cloud as part of dbt Explorer (for dbt Cloud Enterprise customers)</li> </ul> <p>Let\u2019s take a look at how column-level lineage is used via each of these methods:</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#power-user-for-dbt-vscode-dbt-power-user","title":"Power User for dbt (VSCode dbt power user)","text":"<p>Power User for dbt is an open-source VSCode extension and, as the name suggests, brings advanced dbt-related features right into your code editor. Both model and column-level lineage are available and can be viewed in a VSCode panel.</p> Power User for dbt shows column-level lineage in withing your code editor <p>The model-level lineage shows the node type, and also has a high degree of control over the number of parent and child nodes you want to show.</p> Control the number of parent and child nodes you want to show"},{"location":"blog/column-level-lineage-options-for-dbt/#how-to-enable-column-level-lineage-in-dbt-power-user","title":"How to enable column-level lineage in dbt Power user","text":"<p>To use column-level lineage you currently need to enable the beta version by clicking the <code>Show New UX (Beta)</code> button. After that, you can view column-level lineage by:</p> <ul> <li>Clicking a node in the lineage</li> <li>Clicking a column in the details panel</li> </ul> Viewing column-level lineage in Power User for dbt <p>The lineage also shows if the column was transformed, and you can view the code transformations that took place.</p> <p>If you use VSCode, having model and column-level lineage right is a huge convenience, especially as the column-level lineage is shown as part of the model lineage!</p> <p>Highlights of column-level lineage in dbt Power User</p> <ul> <li>Open-source and free to use (An API key is required from Altimate)</li> <li>Runs in VSCode so helps you stay on-task</li> <li>Part of a full-featured lineage feature</li> </ul>"},{"location":"blog/column-level-lineage-options-for-dbt/#dbt-explorer-column-level-lineage-in-dbt-cloud","title":"dbt Explorer column-level lineage in dbt Cloud","text":"<p>Column-level lineage in dbt Cloud is available in dbt Explorer, which is part of dbt\u2019s Enterprise offering. dbt positions the column-level lineage feature as a way to understand the lay of the land before making any changes to your project.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#a-lens-on-lineage","title":"A lens on lineage","text":"<p>From the model-level lineage, which contains lineage \u2018lens\u2019 to toggle various views, you can select a model and view the column evolution. As column-level lineage is viewed separately from the model lineage, it reduces the speed at which you can inspect multiple column lineages because you have to go in and out of the model details pages.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#columns-become-nodes","title":"Columns become nodes","text":"<p>Also of note is that rather than group columns by model, each column is shown as a distinct node on the lineage. For instance, in the screenshot below, there are two nodes used to represent the columns <code>EXTENDED_PRICE</code> and <code>QUANTITY</code> that are both from the one model, <code>stg_tphc_line_items</code>. As mentioned above, your personal preference will determine the usability of this implementation.</p> Column-level lineage in dbt Explorer showing columns as nodes"},{"location":"blog/column-level-lineage-options-for-dbt/#highlights-of-column-level-lineage-in-dbt-explorer","title":"Highlights of column-level lineage in dbt Explorer","text":"<ul> <li>Only available to dbt Enterprise customers</li> <li>Column-level lineage is shown separately from the model-level lineage</li> <li>Columns are shown as distinct nodes in the lineage</li> </ul>"},{"location":"blog/column-level-lineage-options-for-dbt/#recce-column-level-lineage-for-data-model-validation","title":"Recce: column-level lineage for data model validation","text":"<p>Open-source data-model validation tool, Recce, also recently added column lineage as part of its impact assessment workflow.</p> Select and follow a column\u2019s evolution from the main lineage DAG in Recce <p>Column-lineage in Recce is shown directly on the lineage DAG (in Recce it\u2019s actually a lineage DAG diff), so you can easily click about and inspect the lineage of various columns, while staying on the task of checking which models are impacted by your data modeling changes. Each column also displays the type of transformation that has taken place, such as derived (transformed), renamed, or simply passed-through.</p> <p>As mentioned, Recce positions the column-lineage as a method to help with impact assessment, which makes sense when you have a bunch of models to check after updating an upstream model \u2014 knowing which models use that column can be very useful.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#highlights-of-column-level-lineage-in-recce","title":"Highlights of column-level lineage in Recce","text":"<ul> <li>Open-source version is free to use</li> <li>Column information is displayed on the main lineage DAG</li> <li>Part of a larger data-validation workflow</li> </ul>"},{"location":"blog/column-level-lineage-options-for-dbt/#sqlmesh-column-level-lineage","title":"SQLMesh column-level lineage","text":"<p>SQLMesh, the data transformation platform that quickly emerged as the primary competitor to dbt, integrates many features into it\u2019s open-source offering.</p> <p>In addition to data transformation, impact analysis, audits, blue/green deployments, column-level lineage is also an integral part of the platform. Notably, SQLMesh actually enables column-level lineage for dbt projects.</p> SQLMesh runs in the browser with lineage segments easily accessible <p>SQLMesh runs in a browser, and accessing the related lineage for each model is as easy as clicking the model in the catalog or file view.</p> <p>The SQLMesh column-level lineage is unique from the other implementations, and more opinionated, for a few reasons.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#ctes-become-nodes","title":"CTEs become nodes","text":"<p>The first reason is that CTEs are displayed as nodes in the column-level view. It\u2019s a interesting choice, but could result in information overload if your DAG is very busy.</p> CTEs are shown as nodes in the column-level lineage view"},{"location":"blog/column-level-lineage-options-for-dbt/#heads-upstream","title":"Heads-up(stream)","text":"<p>The second is that only the upstream lineage is shown. For instance, in the screenshot below only the upstream column usage of the <code>amount</code> column is shown, the downstream is not.</p> Only the upstream lineage is shown in column-level in SQLMesh <p>This is why I described this as a more opinionated implementation. I suppose the expectation is \u2014 you\u2019re looking at a number and you want to know how that number came to be, so naturally you want to look upstream.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#refresh-refresh-refresh","title":"Refresh, refresh, refresh","text":"<p>One issue with navigating lineage in SQLMesh is the constant reloading. Each time you click a node the whole lineage is refreshed. This makes it incredibly difficult to assess lineage, while at the same time keeping track of where you are in the bigger picture.</p> Each click results in a full refresh, which can be a little disorientating <p>This refresh remains for going from project to column-level lineage. Again, making it possible to lose that thread.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#highlights-of-column-level-lineage-in-sqlmesh","title":"Highlights of Column-level lineage in SQLMesh","text":"<ul> <li>Native feature of SQLMesh</li> <li>Open-source</li> <li>The lineage reloads when selecting a node which can be distracting</li> <li>Only the upstream lineage is shown</li> <li>CTEs are shown on the lineage as nodes</li> </ul>"},{"location":"blog/column-level-lineage-options-for-dbt/#additional-considerations","title":"Additional considerations","text":"<p>Just tracking how data flows through columns is useful, but there are other considerations to take into account when exploring data impact. If and how a column was transformed can offer greater insight into which nodes you need to focus on for root cause analysis. Also, keep in mind that columns used in filtering or joins may not appear in column-level lineage as being \u2018used\u2019 in that node.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#column-transformation-type","title":"Column transformation type","text":"<p>One aspect of column-level lineage that is particularly useful for root cause analysis is knowing if a transformation occurred in the model.</p> <p>When the column passed through this model, what happened to it?</p> <ul> <li>Transformed</li> <li>Passed-through</li> <li>Renamed</li> </ul> Listing the transformations for a column in Power User for dbt (source: Power User for dbt documentation) <p>If the column was transformed, then having the ability to view those transformations can further enable your work understanding more about a columns evolution.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#where-is-the-caveat","title":"\u201cWhere\u201d is the caveat","text":"<p>If a column is used downstream in a WHERE clause, or join, this does not count as the column being used in that model. Filtering and join conditions establish model relationships, but do not modify or transform the column data. Therefore, they will not be shown on the column-level lineage in most implementations.</p>"},{"location":"blog/column-level-lineage-options-for-dbt/#conclusion","title":"Conclusion","text":"<p>Each of the column-level lineage offerings above differs in slight ways that would inform the way you use them. In that regard, which is most useful to you would depend on your particular use-case. Implementation aside, there\u2019s no denying the use of column-level lineage as a tool in impact assessment both before and after modeling changes, and in gaining a deep understanding of data flow through your data project.</p> <p>Are you using one of the many column-level lineage tools? Which one are you using, is it good? Anything to correct in the summaries above? Please leave a comment!</p>"},{"location":"docs/","title":"What is Recce?","text":"<p>Recce (/\u02c8r\u025bki/), pronounced 'reh-kee', is short for 'reconnaissance'. It's a data change management toolkit designed to enhance the pull request (PR) review process for dbt projects. Recce provides enhanced visibility into the data impact from dbt modeling changes by comparing the data in dev and production environments. Using Recce for data impact assessment before merging a PR ensures that production data remains stable and accurate.</p>"},{"location":"docs/#key-features","title":"Key Features","text":""},{"location":"docs/#manual-and-automated-data-checks","title":"Manual and Automated Data Checks","text":"<p>Recce checks help you to assess data impact and explore data change both manually and automatically. </p> <ul> <li>Manual checks - Create a Recce Checklist of data checks that help to validate your data modeling work during development, including data profile comparisons, structural comparisons, and row-level data checks.</li> <li>Automated checks - Integrate Recce Checks into your CI process and post a data impact summary automatically to your PR thread when opening a PR.</li> </ul>"},{"location":"docs/#collaboration-and-replication","title":"Collaboration and Replication","text":"<p>Share Recce checks with your team for stakeholder and PR review. Checks results can be either shared individually, or your full Recce environment can be exported and replicated with one command.</p>"},{"location":"docs/#why-recce","title":"Why Recce","text":"<p>dbt has brought software engineering best practices to data projects, but \u201cbad merges\u201d still happen, allowing erroneous data and silent errors to make their way into prod data.</p>"},{"location":"docs/#understand-data-impact","title":"Understand data impact","text":"<p>Recce provides data and analytics engineers with a toolkit to explore data impact caused by dbt data modeling changes. The varying levels of Recce checks enable holistic or fine grained impact assessment so you can drill down to find the root cause of data change.</p>"},{"location":"docs/#improved-confidence-merging","title":"Improved confidence merging","text":"<p>The improved visibility into data impact gives PR reviewers the confidence to sign-off PRs knowing that prod data will not change unexpectedly.</p>"},{"location":"docs/#how-recce-works","title":"How Recce Works","text":"<p>Recce works by comparing dbt model changes between two environments, which is essential for full impact analysis. However, you can still get started with Recce without first preparing a base environment.</p>"},{"location":"docs/#quick-start","title":"Quick start","text":"<p>Launch Recce in any dbt project in just two commands:</p> <pre><code># cd into your dbt project\npip install -U recce\nrecce server\n</code></pre> <p>In this mode, you can perform the following actions:</p> <ul> <li>Explore lineage and navigate your dbt project</li> <li>Track model changes with basic lineage diff</li> <li>Run queries with Jinja and macros</li> </ul>"},{"location":"docs/#full-comparison-mode","title":"Full comparison mode","text":"<p>To use the full suite of diffing tools in Recce, set up a base dbt environment for Recce to compare against. </p> <p>See the Getting Started page for instructions on how to do this.</p>"},{"location":"docs/#what-you-get","title":"What you get","text":""},{"location":"docs/#interactive-impact-assessment-environment","title":"Interactive impact assessment environment","text":"<p><code>recce server</code> launches a web UI with an interactive impact assessment environment. Use the tools in Recce to explore the impact to your data models from your branch changes.</p>"},{"location":"docs/#focused-data-impact-exploration","title":"Focused data impact exploration","text":"<p>The main interface to Recce is the lineage DAG, which shows modified nodes and potentially impacted downstream nodes. You can quickly see if critical nodes are within the impact radius and focus your data validation efforts.</p> <p> </p>"},{"location":"docs/#getting-started","title":"Getting Started","text":"<p>Try the 5-minute tutorial that uses dbt\u2019s Jaffle Shop project, or take the online demo for a test run, which includes an actual PR and related Recce Instance.</p>"},{"location":"docs/#what-does-recce-mean","title":"What does Recce mean?","text":"<p>Recce (/\u02c8r\u025bki/), pronounced 'reh-kee', is short for 'reconnaissance'. We chose this name as it's the perfect fit for a tool you'll use to perform a 'data reconnaissance' to discover and assess the impact of data modeling changes. Add a Data Recce to your pull request workflow and stop pushing breaking changes to production!</p>"},{"location":"docs/demo/","title":"Recce Online Demo","text":"<p>Try <code>Recce</code> without installing using this online demo. </p> <p>Recce Online Demo</p> <p>The demo showcases a pull request that fixes the <code>customer_lifetime_value</code> calculation in dbt's Jaffle Shop project to only included completed orders.</p> <p> </p> Jaffle Shop customers.sql"},{"location":"docs/demo/#examples","title":"Examples","text":"<p>Some example validation checks you might create include:</p>"},{"location":"docs/demo/#value-diff","title":"Value Diff","text":"<p>Run a Value Diff to check the percentage match of the <code>customer_lifetime_value</code> column between production and the development branch.</p> <p> </p> Value Diff - Customers Model"},{"location":"docs/demo/#profile-diff","title":"Profile Diff","text":"<p>Check the Profile Diff of the customers table to see how the <code>customer_lifetime_value</code> has been impacted.</p> <p> </p> Profile Diff - Customers Model"},{"location":"docs/demo/#query-diff","title":"Query Diff","text":"<p>Run a Query Diff to compare the the actual values in prod and dev.</p> <pre><code>select customer_id, customer_lifetime_value from {{ ref(\"customers\") }} where customer_id &lt; 50;\n</code></pre> <p> </p> Query Diff - Customers Model"},{"location":"docs/get-started-jaffle-shop/","title":"5 Minute Tutorial","text":"<p>Jaffle Shop is an example project officially provided by dbt-labs. This document uses jaffle_shop_duckdb to enable you to start using recce locally from scratch within five minutes.</p> <ol> <li>Clone the \u201cJaffle Shop\u201d dbt data project    <pre><code>git clone git@github.com:dbt-labs/jaffle_shop_duckdb.git\ncd jaffle_shop_duckdb\n</code></pre></li> <li>Prepare virtual env    <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre></li> <li>Installation    <pre><code>pip install -r requirements.txt\npip install recce\n</code></pre></li> <li>Provide additional environment to compare. Edit <code>./profiles.yml</code> to add one more target.    <pre><code>jaffle_shop:\n  target: dev\n  outputs:\n  dev:\n    type: duckdb\n    path: 'jaffle_shop.duckdb'\n    threads: 24\n+ prod:\n+   type: duckdb\n+   path: 'jaffle_shop.duckdb'\n+   schema: prod\n+   threads: 24\n</code></pre></li> <li>Prepare production environment    <pre><code>dbt seed --target prod\ndbt run --target prod\ndbt docs generate --target prod --target-path ./target-base\n</code></pre></li> <li>Prepare development environment. First, edit an existing model <code>./models/staging/stg_payments.sql</code>.    <pre><code>...\n\nrenamed as (\n         payment_method,\n\n-        -- `amount` is currently stored in cents, so we convert it to dollars\n-        amount / 100 as amount\n+        amount\n\n         from source\n)\n</code></pre>    run on development environment.    <pre><code>dbt seed\ndbt run\ndbt docs generate\n</code></pre></li> <li>Run the recce server    <pre><code>recce server\n</code></pre>    Open the link http://0.0.0.0:8000, you can see the lineage diff    </li> <li>Switch to the Query tab, run this query    <pre><code>select * from {{ ref(\"orders\") }} order by 1\n</code></pre>    Click the <code>Run Diff</code> or press <code>Cmd + Shift + Enter</code>    Click on the \ud83d\udd11 icon next to the <code>order_id</code> column to compare records that are uniquely identified by their <code>order_id</code>.    </li> <li>Click the <code>+</code> to add the query result to checklist    </li> </ol>"},{"location":"docs/get-started/","title":"Getting started","text":"<p>This guide walks you through how to use Recce, whether you're exploring for the first time or expanding to Recce Cloud.</p> <p>Use Recce to validate your dbt changes with clarity and confidence. Recce helps you:</p> <ul> <li>Explore what changed</li> <li>Validate downstream impacts</li> <li>Collaborate through shareable checklists</li> </ul> <p>For a hands-on walkthrough, check out the Jaffle Shop Tutorial.</p>"},{"location":"docs/get-started/#start-recce-with-two-commands","title":"Start Recce with two commands","text":"<p>Navigate to your dbt project and run: <pre><code>cd your-dbt-project/  # if you're not already there\npip install -U recce\nrecce server\n</code></pre></p> <p>Recce use dbt artifacts generated with every invocation. You can find these files in the <code>target/</code> folder.</p> artifacts dbt command manifest.json <code>dbt docs generate</code>, <code>dbt run</code>, .. catalog.json <code>dbt docs generate</code> <p>Tip</p> <p>The regeneration of the <code>catalog.json</code> file is not required after every <code>dbt run</code>. it is only required to regenerate this file when models or columns are added or updated.</p>"},{"location":"docs/get-started/#get-instant-visibility","title":"Get instant visibility","text":"<p>With just one environment, Recce gives you:</p> <ul> <li> <p>Lineage clarity: Trace changes down to the column level</p> </li> <li> <p>Query insights: Explore logic and run custom queries</p> </li> <li> <p>Live diffing: Reload and inspect changes as you iterate</p> </li> </ul> <p>Perfect for early exploration, root cause analysis, and faster debugging before involving others.</p>"},{"location":"docs/get-started/#unlock-diff-validation-with-two-environments","title":"Unlock diff &amp; validation with two environments","text":"<p>Not all data changes are obviously right or wrong. Comparing metrics before and after is key to confident validation.</p> <p>When you configure two dbt environments (e.g. prod and dev schemas), Recce lets you:</p> <ul> <li> <p>Explore modified models and downstream impact</p> </li> <li> <p>Validate changes via diffs or custom queries</p> </li> <li> <p>Add validation results to Checklists for review and alignment</p> </li> </ul>"},{"location":"docs/get-started/#how-to-setup-two-environments","title":"How to setup two environments","text":"<p>Setup two separate environments that refers to different schemas, e.g., prod for production and dev for development.</p> <p>Recce supports schema discovery across all major warehouses without needing to modify your <code>profiles.yml</code>.</p> <p>However, if you're using DuckDB, you\u2019ll need to explicitly define both schemas to make it run locally. For example: <pre><code>jaffle_shop:\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      path: jaffle_shop.duckdb\n      schema: dev\n    prod:\n      type: duckdb\n      path: jaffle_shop.duckdb\n      schema: main\n</code></pre></p>"},{"location":"docs/get-started/#prepare-dbt-artifacts","title":"Prepare dbt artifacts","text":"<p>Recce expects two sets of dbt artifacts to be present:</p> <ul> <li><code>target-base/</code> - dbt artifacts for to be used as the base for the comparison e.g. production</li> <li><code>target/</code> - dbt artifacts for your development branch</li> </ul>"},{"location":"docs/get-started/#generate-artifacts-for-the-base-environment","title":"Generate artifacts for the <code>base</code> environment","text":"<p>For most data warehouses, you can download the artifacts generated from the codebase of your main branch.  You don't need to re-run the whole production in your local. However, if you use duckdb, you need to generate the artifacts for the base environment. </p> <p>Checkout the <code>main</code> branch of your project and generate the required artifacts into <code>target-base</code>. You can skip <code>dbt build</code> if this environment already exists. </p> <pre><code>git checkout main\n\ndbt run --target prod\ndbt docs generate --target prod --target-path target-base/\n</code></pre>"},{"location":"docs/get-started/#generate-artifacts-for-the-target-environment","title":"Generate artifacts for the <code>target</code> environment","text":"<pre><code>git checkout feature/my-awesome-feature\n\ndbt run\ndbt docs generate\n</code></pre>"},{"location":"docs/get-started/#share-to-collaborate","title":"Share to collaborate","text":"<p>If you\u2019ve followed the steps above and are ready to share your checklist with others, Recce Cloud makes it easy.</p> <p>Just one link gives full context:</p> <ul> <li> <p>Lineage</p> </li> <li> <p>Diff results</p> </li> <li> <p>Checklist validation with comments</p> </li> </ul> <p>Built for teams, Recce Cloud includes secure, cloud-hosted sharing and collaboration features designed for fast reviews and confident sign-off.</p> <p>Sign up for Recce Cloud to unlock collaboration at scale.</p>"},{"location":"docs/installation/","title":"Installation","text":"<p>Install <code>Recce</code> in your dbt project with pip:</p> <pre><code>pip install recce\n</code></pre> <p>To take full advantage of all the features of <code>Recce</code>, ensure that dbt_profiler and audit-helper are installed via the <code>packages.yml</code> file in your dbt project .</p> <ol> <li>Add these two packages in the packages.yml</li> <li>Do <code>dbt deps</code> to install these 2 packages.</li> </ol> <pre><code>packages:\n  - package: dbt-labs/audit_helper\n    version: &lt;version&gt;\n  - package: data-mie/dbt_profiler\n    version: &lt;version&gt;\n</code></pre> <p>For full instructions on using <code>Recce</code>, check the Getting Started guide.</p>"},{"location":"docs/start-with-dbt-cloud/","title":"Start with dbt Cloud","text":"<p>dbt Cloud is a hosted service that provides a managed environment for running dbt projects by dbt Labs. This document provides a step-by-step guide to get started <code>recce</code> with dbt Cloud.</p>"},{"location":"docs/start-with-dbt-cloud/#prerequisites","title":"Prerequisites","text":"<p><code>Recce</code> will compare the data models between two environments. That means you need to have two environments in your dbt Cloud project. For example, one for production and another for development. Also, you need to provide the credentials profile for both environments in your <code>profiles.yml</code> file to let <code>Recce</code> access your data warehouse.</p>"},{"location":"docs/start-with-dbt-cloud/#suggestions-for-setting-up-dbt-cloud","title":"Suggestions for setting up dbt Cloud","text":"<p>To integrate the dbt Cloud with Recce, we suggest to set up two run jobs in your dbt Cloud project.</p>"},{"location":"docs/start-with-dbt-cloud/#production-run-job","title":"Production Run Job","text":"<p>The production run should be the main branch of your dbt project. You can trigger the dbt Cloud job on every merge to the main branch or schedule it to run at a daily specific time.</p>"},{"location":"docs/start-with-dbt-cloud/#development-run-job","title":"Development Run Job","text":"<p>The development run should be a separate branch of your dbt project. You can trigger the dbt Cloud job on every merge to the pull-request branch.</p>"},{"location":"docs/start-with-dbt-cloud/#set-up-dbt-profiles-with-credentials","title":"Set up dbt profiles with credentials","text":"<p>You need to provide the credentials profile for both environments in your <code>profiles.yml</code> file. Here is an example of how your <code>profiles.yml</code> file might look like:</p> <pre><code>dbt-example-project:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: \"{{ env_var('SNOWFLAKE_ACCOUNT') }}\"\n\n      # User/password auth\n      user: \"{{ env_var('SNOWFLAKE_USER') | as_text }}\"\n      password: \"{{ env_var('SNOWFLAKE_PASSWORD') | as_text }}\"\n\n      role: DEVELOPER\n      database: cloud_database\n      warehouse: LOAD_WH\n      schema: \"{{ env_var('SNOWFLAKE_SCHEMA') | as_text }}\"\n      threads: 4\n    prod:\n      type: snowflake\n      account: \"{{ env_var('SNOWFLAKE_ACCOUNT') }}\"\n\n      # User/password auth\n      user: \"{{ env_var('SNOWFLAKE_USER') | as_text }}\"\n      password: \"{{ env_var('SNOWFLAKE_PASSWORD') | as_text }}\"\n\n      role: DEVELOPER\n      database: cloud_database\n      warehouse: LOAD_WH\n      schema: PUBLIC\n      threads: 4\n</code></pre>"},{"location":"docs/start-with-dbt-cloud/#install-recce","title":"Install <code>Recce</code>","text":"<p>Install Recce using <code>pip</code>:</p> <pre><code>pip install -U recce\n</code></pre>"},{"location":"docs/start-with-dbt-cloud/#execute-recce-with-dbt-cloud","title":"Execute Recce with dbt Cloud","text":"<p>To compare the data models between two environments, you need to download the dbt Cloud artifacts for both environments. The artifacts include the manifest.json file and the catalog.json file. You can download the artifacts from the dbt Cloud UI.</p>"},{"location":"docs/start-with-dbt-cloud/#login-to-your-dbt-cloud-account","title":"Login to your dbt Cloud account","text":""},{"location":"docs/start-with-dbt-cloud/#go-to-the-project-you-want-to-compare","title":"Go to the project you want to compare","text":""},{"location":"docs/start-with-dbt-cloud/#download-the-dbt-artifacts","title":"Download the dbt artifacts","text":"<p>Download the artifacts from the latest run of both run jobs. You can download the artifacts from the <code>Artifacts</code> tab.</p> <p> </p>"},{"location":"docs/start-with-dbt-cloud/#setup-the-dbt-artifacts-folders","title":"Setup the dbt artifacts folders","text":"<p>Extract the downloaded artifacts and keep them in a separate folder. The production artifacts should be in the <code>target-base</code> folder and the development artifacts should be in the <code>target</code> folder.</p> <pre><code>$ tree target target-base\ntarget\n\u251c\u2500\u2500 catalog.json\n\u2514\u2500\u2500 manifest.json\ntarget-base/\n\u251c\u2500\u2500 catalog.json\n\u2514\u2500\u2500 manifest.json\n</code></pre>"},{"location":"docs/start-with-dbt-cloud/#setup-dbt-project","title":"Setup dbt project","text":"<p>Move the <code>target</code> and <code>target-base</code> folders to the root of your dbt project. You should also have the <code>profiles.yml</code> file in the root of your dbt project with the credentials profile for both environments.</p>"},{"location":"docs/start-with-dbt-cloud/#start-the-recce-server","title":"Start the <code>Recce</code> server","text":"<p>Run the <code>recce</code> command to compare the data models between the two environments.</p> <pre><code>recce server\n</code></pre>"},{"location":"docs/start-with-sqlmesh/","title":"SQLMesh","text":""},{"location":"docs/start-with-sqlmesh/#notice-regarding-sqlmesh-support","title":"Notice regarding SQLMesh Support","text":"<p>Recce has temporarily paused support for SQLMesh to prioritize our focus on dbt. As a result, Recce is not currently compatible with SQLMesh.</p> <p>While future support for SQLMesh remains a possibility, it is not included in our current development roadmap.</p> <p>We appreciate your understanding. If you have any questions, please feel free to contact us as product@datarecce.io.</p>"},{"location":"docs/agreement/cookies-policy/","title":"Cookies Policy","text":"<p>Effective date: April 26, 2021</p> <p>Recce (\"us\", \"we\", or \"our\") uses cookies on https://datarecce.io (the \"Service\"). By using the Service, you consent to the use of cookies.</p> <p>Our Cookies Policy explains what cookies are, how we use cookies, how third-parties we may partner with may use cookies on the Service, your choices regarding cookies and further information about cookies.</p>"},{"location":"docs/agreement/cookies-policy/#what-are-cookies","title":"What are cookies","text":"<p>Cookies are small pieces of text sent by your web browser by a website you visit. A cookie file is stored in your web browser and allows the Service or a third-party to recognize you and make your next visit easier and the Service more useful to you.</p> <p>Cookies can be \"persistent\" or \"session\" cookies.</p>"},{"location":"docs/agreement/cookies-policy/#how-recce-uses-cookies","title":"How Recce uses cookies","text":"<p>When you use and access the Service, we may place a number of cookies files in your web browser.</p> <p>We use cookies for the following purposes: to enable certain functions of the Service, to provide analytics, to store your preferences, to enable advertisements delivery, including behavioral advertising.</p> <p>We use both session and persistent cookies on the Service and we use different types of cookies to run the Service:</p> <ul> <li>Essential cookies: We may use essential cookies to authenticate users and prevent fraudulent use of user accounts.</li> <li>Third-party cookies: In addition to our own cookies, we may also use various third-parties cookies to report usage statistics of the Service, deliver advertisements on and through the Service, and so on.</li> </ul>"},{"location":"docs/agreement/cookies-policy/#what-are-your-choices-regarding-cookies","title":"What are your choices regarding cookies","text":"<p>If you'd like to delete cookies or instruct your web browser to delete or refuse cookies, please visit the help pages of your web browser.</p> <p>Please note, however, that if you delete cookies or refuse to accept them, you might not be able to use all of the features we offer, you may not be able to store your preferences, and some of our pages might not display properly.</p>"},{"location":"docs/agreement/cookies-policy/#where-can-your-find-more-information-about-cookies","title":"Where can your find more information about cookies","text":"<p>You can learn more about cookies and the following third-party websites:</p> <p>AllAboutCookies: http://www.allaboutcookies.org/</p> <p>Network Advertising Initiative: http://www.networkadvertising.org/</p>"},{"location":"docs/agreement/privacy-policy/","title":"Privacy Policy","text":"<p>Effective date: January 1, 2025</p> <p>This Privacy Policy explains how your personal information is collected, used and disclosed by InfuseAI Inc. (\u201cRecce,\u201d \u201cwe\u201d or \u201cus\u201d) when you use our website https://datarecce.io, online products and monitoring services (\u201cPlatform\u201d) (collectively, \u201cServices\u201d), or when you otherwise interact with us. This Privacy Policy also describes your choices regarding use, access and correction of personal information collected about you through our Services.</p> <p>By accessing or using the Services, you acknowledge that you have read and understood the content of this Privacy Policy and consent to our collection, using and sharing of your personal information. Please read this Privacy Policy carefully and ensure that you understand it before you start.</p> <p>We reserve the right to update this Privacy Policy from time to time. If we make changes, we will revise the date at the top of the Privacy Policy and, in some cases, we may provide you with additional notice (such as adding a statement to our homepage or sending you a notification). We encourage you to review the Privacy Policy whenever you access the Services or otherwise interact with us to stay informed about our information practices and the ways you can help protect your privacy.</p> <p>We do not knowingly collect or solicit personal information from anyone under the age of 16. If you are under 16, please do not attempt to register for the Services or send any personal information about yourself to us. If we learn that we have collected personal information from a child under age 16, we will delete that personal information as quickly as possible. If you believe that a child under age 16 may have provided us personal information, please contact us at product@datarecce.io.</p>"},{"location":"docs/agreement/privacy-policy/#what-personal-information-do-we-collect","title":"What Personal information do We Collect?","text":""},{"location":"docs/agreement/privacy-policy/#personal-information-you-provide-to-us","title":"Personal information You Provide to Us","text":"<p>We collect and store any personal information you provide directly to us via the Services. Personal information submitted through the Services include the details you submit when you create an account, participate in any interactive features of the Services, fill out a form, pay for subscriptions, apply for a job, communicate with us via third party social media sites, request customer support or otherwise communicate with us. The types of personal information we may collect and store include your name, email address, postal address, phone number and any other personal information you choose to provide. To the extent you provide credit card personal information through the Services, that personal information is collected and processed by our third-party payment processor pursuant to their Privacy Policy and practices.</p>"},{"location":"docs/agreement/privacy-policy/#personal-information-collected-automatically","title":"Personal information Collected Automatically","text":"<p>When you access or use the Services, we automatically collect personal information about you, including:</p> <p>Log Files: We gather certain personal information about your use of the Services, including the type of browser you use, access times, pages viewed, your IP address and the page you visited before navigating to the Services, and store it in log files.</p> <p>Personal information Collected by Cookies and Other Tracking Technologies: We use various technologies to collect personal information including cookies. For more information about cookies, and how to disable them, please see our Cookie Policy page https://datarecce.io/docs/agreement/privacy-policy/ and Your Choices below.</p>"},{"location":"docs/agreement/privacy-policy/#personal-information-we-collect-from-other-sources","title":"Personal information We Collect from Other Sources","text":"<p>We may also obtain personal information from other sources and combine that with personal information we collect through our Services for purposes of advertising and user authentication. For example, if you create or log into your account using your Google Apps credentials via single sign-on, we will have access to certain personal information such as your name and email address as authorized in your Google Apps profile settings.</p>"},{"location":"docs/agreement/privacy-policy/#where-do-we-store","title":"Where do We Store","text":"<p>We store your personal information by utilizing Amazon Web Services (AWS) in the US. You can find AWS\u2019s privacy features at https://aws.amazon.com/compliance/privacy-features/?nc1=h_ls</p>"},{"location":"docs/agreement/privacy-policy/#our-use-of-your-personal-information","title":"Our Use of Your Personal Information","text":"<p>We may use your personal information to:</p> <ul> <li>Enable you to have full access to the Services;</li> <li>Provide, maintain and improve the Services;</li> <li>Provide and deliver the products and services you request, process transactions and send you related information, including confirmations and invoices;</li> <li>Send you technical notices, updates, security alerts, and support and administrative messages;</li> <li>Respond to your comments, questions and requests, and provide customer support;</li> <li>Create your account and identify you when you sign-in to your account in accordance with your agreement with us;</li> <li>Communicate with you about products, services, offers, promotions, rewards, and events offered by us and others, and provide news and information we think will be of interest to you;</li> <li>Monitor and analyze trends, usage and activities in connection with the Services;</li> <li>Detect, investigate and prevent fraud and other illegal activities and protect the rights and property of us and others;</li> <li>Personalize and improve the Services and provide advertisements, content or features that match user profiles or interests;</li> <li>Notify you about important changes to the Services, including changes or updates to this Privacy Policy;</li> <li>Facilitate contests, sweepstakes and promotions and process and deliver entries and rewards;</li> <li>Link or combine with personal information we get from others to help understand your needs and provide you with better service;</li> <li>Consider you for possible employment at Recce in connection with an application that you submit; and</li> <li>Carry out any other purpose described to you at the time the personal information was collected.</li> </ul>"},{"location":"docs/agreement/privacy-policy/#our-sharing-of-personal-information","title":"Our Sharing of Personal Information","text":"<p>We may share your personal information as follows or as otherwise described in this Privacy Policy:</p> <p>With vendors, consultants and other service providers we have vetted and approved who need access to such personal information to carry out work on our behalf only to the extent necessary for the performance of any contract we enter into with you. This includes companies providing the following services for our Website and/or Platform: hosting services, authentication services, cyber security and anti-fraud services, and advertising;</p> <p>In response to a request for personal information if we believe disclosure is permitted by, in accordance with, or required by, any applicable law, regulation or legal process such as to comply with a subpoena or applicable court order;</p> <p>With any person to whom disclosure is necessary to enable us to enforce our rights under this Privacy Policy or under any agreement we enter into with you or to protect the rights, property and safety of Recce or third parties;</p> <p>In connection with, or during negotiations of, any merger, sale of Recce assets, financing or acquisition of all or a portion of our business by another company;</p> <p>Between and among Recce and all companies affiliated with Recce who may act for us for any of the purposes set out in this Privacy Policy, including our current and future parents, affiliates, subsidiaries and other companies under common control and ownership;</p> <p>With analytics and search engine providers that assist us in the improvement and optimization of our Website, subject to our Cookies Policy; and</p> <p>With your consent or at your direction.</p> <p>We may also share aggregated or de-identified personal information, which cannot reasonably be used to identify you.</p>"},{"location":"docs/agreement/privacy-policy/#basis-for-processing","title":"Basis for Processing","text":"<p>The lawful bases upon which we process your personal information are as follows:</p> <p>Where it is necessary to obtain your prior consent to the processing concerned in order for us to be allowed to do it, for instance in relation to direct marketing, we will obtain and rely on your consent in relation to the processing concerned.</p> <p>Otherwise, we will process your personal information only where the processing is necessary for compliance with a legal obligation to which we are a subject; or</p> <p>For the purposes of the legitimate interests pursued by us in promoting our business, providing the Platform to our business customers pursuant to or legal agreements with them, and in ensuring the security, accessibility and improvement of our Website and Platform and the development of new technology and services.</p>"},{"location":"docs/agreement/privacy-policy/#external-links","title":"External Links","text":"<p>The Website may, from time to time, contain links to external sites. If you follow a link to any of these websites, please note that these websites have their own privacy policies and that we do not accept any responsibility or liability for these policies. Please check these policies before you submit any personal information to these websites. We are not responsible for the privacy policies or the content of such sites.</p>"},{"location":"docs/agreement/privacy-policy/#marketing","title":"Marketing","text":"<p>Where permitted in our legitimate interest or with your prior consent where required by law, we will use your personal information for marketing and to provide you with promotional update communications by email about our products/services. You can object to further marketing by contacting us.</p>"},{"location":"docs/agreement/privacy-policy/#security","title":"Security","text":"<p>We take reasonable steps, including physical, technical and organizational measures, to protect your personal information from unauthorized access and against unlawful processing, accidental loss, destruction and damage. Although we do our best to protect your personal information, we cannot guarantee the security of your personal information submitted to us.</p> <p>Your personal information will be retained by us for the duration of your account and may be retained for a period after this time as necessary and relevant to our legitimate interests, our terms of agreement with you and in accordance with applicable legal obligations. This may include retention necessary to meet our tax reporting requirements as well as time required to enforce the relevant terms of agreement or to identify, issue or resolve legal proceedings.</p> <p>We may retain a record of your stated objection to the processing of your personal information, including in respect of an objection to receiving marketing communications, for the sole legitimate purpose of ensuring that we can continue to respect your wishes and not contact you further, during the term of your objection.</p>"},{"location":"docs/agreement/privacy-policy/#your-rights","title":"Your Rights","text":"<p>You may have the following rights pertaining to your personal information depending on the law that we are subject to:</p> <ol> <li>The right to withdraw consent;</li> <li>The right of access;</li> <li>The right to erasure;</li> <li>The right to rectification;</li> <li>The right to personal information portability;</li> <li>The right to object;</li> <li>The right to be notified of personal information breaches; and</li> <li>The right to lodge a complaint with a supervisory authority.</li> </ol>"},{"location":"docs/agreement/privacy-policy/#your-choices","title":"Your Choices","text":""},{"location":"docs/agreement/privacy-policy/#account-information","title":"Account Information","text":"<p>You may access, update or change personal information you have provided by logging into the Services or emailing us at product@datarecce.io. You can always opt not to disclose information to us, but we would like to remind you that some personal information may be needed for registration. Shall you decide not to provide such personal information, we might not be able to provide the Services to you.</p> <p>You may be able to add, update or delete your personal information, but we will maintain a copy of the unrevised personal information in our records. You may request deletion of your account or personal information by sending a request to us. Some personal information may remain in our records after the deletion as necessary to comply with our legal obligations or for legitimate business purposes, such as to resolve disputes or enforce our agreements. We may also retain cached or archived copies of personal information for a certain period of time.</p>"},{"location":"docs/agreement/privacy-policy/#cookies","title":"Cookies","text":"<p>Most web browsers are set to accept cookies by default. If you prefer, you can usually choose to set your browser to remove or reject browser cookies. Please note that if you choose to remove or reject cookies, this could affect the availability and functionality of our Services. For more information, please see Recce\u2019s Cookie Policy at https://datarecce.io/docs/agreement/cookies-policy/ </p>"},{"location":"docs/agreement/privacy-policy/#promotional-communications","title":"Promotional Communications","text":"<p>You may opt out of receiving promotional emails from Recce by following the instructions in those emails or by emailing us If you opt out, we may still send you non-promotional emails, such as those about your account or our ongoing business relations.</p>"},{"location":"docs/agreement/privacy-policy/#contact-us","title":"Contact Us","text":"<p>If you have questions or concerns about this Privacy Policy, please send us a detailed message to product@datarecce.io.</p>"},{"location":"docs/agreement/terms-of-use/","title":"Terms of Use","text":""},{"location":"docs/agreement/terms-of-use/#1-introduction","title":"1. INTRODUCTION","text":"<p>Welcome to https://datarecce.io (and sub-domains). The website is owned and operated by InfuseAI, Inc. (\u201cRecce\u201d or \u201cwe\u201d or \u201cus\u201d). Please read these Terms of Use (\u201cTerms of Use\u201d) carefully as it constitutes a legally binding contract between you and Recce governing your use of the websites owned and/or operated by us and any other online services (collectively, the \u201cServices\u201d). By accessing or using the Services, you agree to be legally bound by these Terms of Use whether as a visitor or as a registered user. If you do not agree with any of these Terms of Use, please do not use or access the Services in any manner.</p> <p>You warrant and represent to Recce that you are able to enter into</p> <p>contracts by any and all applicable laws and regulations. If you are entering into these Terms of Use for an entity, such as the company you work for, you warrant and represent that you have the authority to bind that entity and you agree that \u201cyou\u201d as used in these Terms of Use includes both you personally and the entity you represent. You and Recce are collectively referred to as the \u201cParties\u201d and each is a \u201cParty\u201d</p>"},{"location":"docs/agreement/terms-of-use/#2-definitions","title":"2. DEFINITIONS","text":"<p>2.1 Authorized User: Any of your current employees, consultants, or agents whom you authorize to access and use the Services. You are responsible for the acts and omissions of your Authorized Users and any other person who accesses or uses the Services using any of your or your Authorized Users\u2019 access credentials.</p> <p>2.2 Beta Features: Any pre-release features, functionalities, or modules of the Platform that are made available to you to use and evaluate.</p> <p>2.3 Billing Information: Any of your billing information, including, without limitation, bank account numbers, credit card or debit card numbers, account details, ACH information, and similar data.</p> <p>2.4 Confidential Information: (i) With respect to Recce, the Platform, the Website, and any and all source code relating thereto and any other non-public information or material regarding our legal or business affairs, financing, customers, properties, pricing, or data; (ii) with respect to you, your Data and any other non-public information or material regarding your legal or business affairs, financing, customers, properties, or data; and (iii) with respect to each Party, the terms and conditions of the Services.</p> <p>2.5 Data: (i) Any data that you or your Authorized Users submit by using the Services and (ii) Any data on the Servers that you or your Authorized Users query, transform, process or otherwise access.</p> <p>2.6 Derived Metadata: Data we have derived from the Data that provides information about the content or structure of the Data but does not contain the Data itself.</p> <p>2.7 Destructive Elements: Computer code, programs, or programming devices that are intentionally designed to disrupt, modify, access, delete, damage, deactivate, disable, harm, or otherwise impede in any manner, including aesthetic disruptions or distortions, the operation of the website or any other associated software, firmware, hardware, computer system, or network (including, without limitation, \u201cTrojan horses,\u201d \u201cviruses,\u201d \u201cworms,\u201d \u201ctime bombs,\u201d \u201ctime locks,\u201d \u201cdevices,\u201d \u201ctraps,\u201d \u201caccess codes,\u201d or \u201cdrop dead\u201d or \u201ctrap door\u201d devices) or any other harmful, malicious, or hidden procedures, routines or mechanisms that would cause the website to cease functioning or to damage or corrupt data, storage media, programs, equipment, or communications, or otherwise interfere with operations.</p> <p>2.8 Fees: Any Services Fees, and any other fees we charge for our products, services, or data</p> <p>2.9 Sensitive Personal Information: Personal information, the loss of which would trigger a data breach notification requirement, and includes, but is not limited to, personally-identifiable Billing Information, financial information, health information, or country identification number (e.g. Social Insurance Number, Social Security Number, or other governmentally-issued identification number such as driver\u2019s license or passport number).</p> <p>2.10 Trial Period: Any period during which we provide you the Service on a trial basis.</p>"},{"location":"docs/agreement/terms-of-use/#3-trial-period-and-services","title":"3. TRIAL PERIOD AND SERVICES","text":"<p>3.1 Trial Period</p> <p>If you register for a free trial, we will make one or more free subscription services available to you on a trial basis free of charge until the earlier of (a) the end of the free trial period for which you registered to use the applicable Services, (b) the start date of any payable Services ordered by you or (c) termination by Recce at our own discretion.</p> <p>During the Trial Period, our representations and warranties herein shall not apply, and we will not be liable to you for damages of any kind related to the Services, including, without limitation, your use of, or inability to use, the Services. You are at your own risk during the Trial Period.</p> <p>3.2 Beta Features</p> <p>From time to time, we may invite you to try Beta Features. You may accept or decline any such trial at your own discretion. An important part of this beta process is getting real-world testing of the Beta Features before a general release. If you agree to participate in a beta trial, the following additional terms and conditions will apply:</p> <p>You agree to use and test the Beta Features and to provide timely feedback, comments, and suggestions to our team.</p> <p>You agree that we shall be free to use, reproduce, disclose, and otherwise exploit any and all such Feedback without compensation or attribution to you.</p> <p>Unless otherwise stated, any Beta Feature trial period will expire upon the date that a version of the Beta Feature becomes generally available or we elect to discontinue such Beta Feature.</p> <p>We may discontinue Beta Features at any time in our sole discretion with or without notice and may never make them generally available.</p> <p>YOU ACKNOWLEDGE THAT WE MAKE NO WARRANTY OF ANY KIND, WHETHER EXPRESS, IMPLIED, STATUTORY OR OTHERWISE IN CONNECTION WITH THESE FREE TRIAL WITHOUT LIMITING THE FOREGOING, WE ARE PROVIDING THE SERVICES ON AN \u201cAS IS\u201d AND \u201cAS AVAILABLE\u201d BASIS AND WE DO NOT MAKE, AND HEREBY EXPRESSLY DISCLAIMS, TO THE FULLEST EXTENT PERMITTED BY APPLICABLE LAW, ALL REPRESENTATIONS, WARRANTIES AND CONDITIONS, EXPRESS OR IMPLIED, WITH RESPECT TO THE SERVICES OR THEIR PERFORMANCE, INCLUDING THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN PARTICULAR, WE DO NOT WARRANT THAT THE SERVICES WILL MEET YOUR EXPECTATIONS OR BE SECURE, ACCURATE, ERROR-FREE, OR OPERATE ON AN UNINTERRUPTED BASIS OR IN COMBINATION WITH ANY OTHER HARDWARE, SOFTWARE OR SYSTEM. WITHOUT LIMITING THE FOREGOING, WE WILL NOT BE LIABLE FOR ANY PROBLEMS WITH THE SUBSCRIPTION SERVICES ATTRIBUTABLE TO THE INTERNET, FORCE MAJEURE OR YOUR OR ANY AUTHORIZED USER\u2019S NETWORK OR ABILITY TO ACCESS THE INTERNET.</p>"},{"location":"docs/agreement/terms-of-use/#4-fees-and-payment","title":"4. FEES AND PAYMENT","text":"<p>The Fees and any applicable taxes shall be paid by you before we can provide the Services. If we are required to pay any taxes on your behalf, you shall reimburse us upon receiving our notice.</p> <p>You hereby agree to defend, indemnify, and hold harmless us and our officers, directors, managers, employees, and agents from any and all liabilities, costs, and expenses (including reasonable attorneys\u2019 fees) in connection with any taxes and related costs, interest, and penalties paid or payable by us on your behalf.</p> <p>You will keep your contact information, Billing Information, and credit card information (where applicable) up to date. Changes may be made on your billing page on the Website.</p> <p>All Fees and taxes payable under the Services are non-cancelable, and all payments made are non-refundable.</p>"},{"location":"docs/agreement/terms-of-use/#5-term-termination-and-suspension","title":"5. TERM, TERMINATION, AND SUSPENSION","text":"<p>These Terms of Use commence on the date you first accept them and continue until terminated pursuant to this Clause (the \u201cTerm\u201d).</p> <p>Each Party can terminate these Terms of Use at any time on written notice to the other Party provided there are no active Services then in effect. Either Party may terminate the Services: (i) during the Trial Period, if any, in which case the termination shall take immediate effect; (ii) for Services paid monthly, at any time outside the Trial Period, in which case the termination shall take effect on the next monthly anniversary date; (iii) for Services that are not paid monthly, on written notice to the other Party if such other Party has breached the Terms of Use and failed to cure such breach within thirty (30) days of receiving written notice thereof; or (iv) at any time if the other Party becomes the subject of a petition in bankruptcy or any other proceeding relating to insolvency, liquidation, or assignment for the benefit of creditors, in which case the termination shall take immediate effect.</p> <p>We may suspend the Services immediately if any undisputed payment due to us is over thirty (30) days past due. If the Services are suspended for non-payment, we may charge a reactivation fee to reinstate them. You will promptly reimburse us for any reasonable expenses of collection, including costs, disbursements, and reasonable outside legal fees we incur.</p> <p>Upon termination or expiration of the Services, we will stop providing the Services and you will stop all access to and use of the Website. Upon written request, each Party shall either return to the other Party all documents, computer files, and other materials containing any of such other Party\u2019s Confidential Information that are in its possession or control.</p> <p>The following provisions will survive expiration or termination of the Services: Definitions, Beta Features, Fees and Payment, Confidentiality, Intellectual Property, Limitation of Liability, Indemnification, and this provision.</p>"},{"location":"docs/agreement/terms-of-use/#6-confidentiality","title":"6. CONFIDENTIALITY","text":"<p>A Party will (i) protect the confidentiality of the other Party\u2019s Confidential Information using the same degree of care that it uses with its own confidential information of similar nature, but with no less than reasonable care; (ii) not use any of the other Party\u2019s Confidential Information for any purpose outside the scope of the Services; and (iii) not disclose the other Party\u2019s Confidential Information to any party other than its employees, contractors, advisors, and agents, who are bound by obligations of confidentiality as restrictive as those set forth herein.</p> <p>If a Party is legally compelled to disclose any of the other Party\u2019s Confidential Information, to the extent permitted by applicable law, a prior written notice of such requirement shall be delivered to the other Party so that the other Party may seek a protective order or other appropriate remedy and/or waive compliance.</p>"},{"location":"docs/agreement/terms-of-use/#7-use-of-your-data","title":"7. USE OF YOUR DATA","text":"<p>7.1 Your Data</p> <p>We will only use your Data only to provide the Services and only as permitted by our privacy policy, located at Privacy Policy. You acknowledge and agree that in order to provide and/or improve the Services, we may: (i) query, transform, process and otherwise access your Data that you store on your Servers or on a third-party service to which you facilitate our access via an application programming interface (\u201cAPI\u201d) or other means; (ii) analyze the Data to determine which other products and services may be relevant to you and to inform the improvement and development of our products and services; and (iii) create Derived Metadata from such analysis. We may also retain a copy of your Data for a reasonable period of time in order to provide the Services or as otherwise required by applicable law. You shall have sole responsibility for the accuracy, quality, and legality of your Data. Unless we are managing an instance with our third-party hosting provider on your behalf, we will store your Data only as long as needed to provide the Services.</p> <p>7.2 Sensitive Personal information</p> <p>You agree to remove or anonymize all Sensitive Personal information before transferring or providing access to your personal information to us. We will not have any liability that may result from your disclosure of such information to us.</p> <p>7.3 Data Security</p> <p>We shall employ commercially reasonable physical, administrative, and technical safeguards to secure your Data provided by you or collected by us from unauthorized use or disclosure. Some of the Data may be subject to governmental regulation or otherwise may require security measures beyond those set forth herein. Without prior written agreement for such extra security measures, we shall have no obligation to do so or any liability in connection therewith</p>"},{"location":"docs/agreement/terms-of-use/#8-intellectual-property","title":"8. INTELLECTUAL PROPERTY","text":"<p>Unless otherwise provided in these Terms of Use, all right, title and interest in and to the Services and all proprietary rights therein shall be and remain our sole and exclusive property. Subject to the preceding clause in relation to Your Data, all right, title, and interest in and to your Data, including all modifications, improvements, adaptations, enhancements, or translations made thereto, and all proprietary rights therein, shall be and remain your sole and exclusive property.</p>"},{"location":"docs/agreement/terms-of-use/#9-use-and-limitations-of-use","title":"9. USE AND LIMITATIONS OF USE","text":"<p>9.1 Restrictions on Use</p> <p>You will not (and will not authorize, permit, or encourage any third party to), directly or indirectly: (i) allow anyone other than Authorized Users to access and use the Services; (ii) reverse engineer, decompile, disassemble, or otherwise attempt to discern the source code or interface protocols of the Services; (iii) modify, adapt, or translate the Services; (iv) make any copies of the Services; (v) resell, distribute, or sublicense the Services without our prior written permission in each instance, which we may withhold in our sole and absolute discretion; (vi) remove or modify any proprietary marking or restrictive legends placed on the Services; (vii) use the Services in violation of any applicable law or regulation, in order to build a competitive product or service, or for any purpose not specifically permitted in the Services; or (viii) introduce, post, upload, transmit, or otherwise make available to or from the Services.</p> <p>9.2 Compliance</p> <p>We have the right, but not the obligation, to monitor your compliance with the Terms of Use. If any such monitoring reveals that you have exceeded any usage limitations or otherwise are not using the Services in compliance with the Terms of Use, then you will remedy any such non-compliance within five (5) business days upon receiving notice from us, including, if applicable, through the payment of additional fees, which we may automatically charge and process in accordance with the Fee and Payments Clause.</p> <p>9.3 Onboarding of Authorized Users</p> <p>Authorized Users must log into the Website. During the initial registration, Authorized User will be prompted to create an account, which includes a sign-in name (\u201cSign-In Name\u201d), a password (\u201cPassword\u201d), an email address, and perhaps certain additional information that will assist in authenticating the Authorized User\u2019s identity when the person logs in in the future (\u201cUnique Identifiers\u201d). When creating the account, Authorized Users must provide true, accurate, current, and complete information. You are solely responsible for the confidentiality and use of Authorized Users\u2019 Sign-In Names, Passwords, and Unique Identifiers, as well as for any use, misuse, or communications entered through the Website.</p> <p>You will promptly inform us of any need to deactivate a Password or Sign-In Name or change any Unique Identifier. We reserve the right to delete or change Authorized Users\u2019 Passwords, Sign-In Names, or Unique Identifiers at any time and for any reason, with or without notice. We will not be liable for any loss or damage caused by any unauthorized use of an Authorized User\u2019s account.</p>"},{"location":"docs/agreement/terms-of-use/#10-representations-and-warranties-disclaimer","title":"10. REPRESENTATIONS AND WARRANTIES; DISCLAIMER","text":"<p>10.1 Mutual Representations and Warranties</p> <p>Each Party represents and warrants to the other Party that: (i) it is duly organized, validly existing, and in good standing under its jurisdiction of organization and has the right to enter into the Services; (ii) the execution, delivery, and performance of the Services and the consummation of the transactions contemplated hereby are within the corporate powers of such Party and have been duly authorized by all necessary corporate action on the part of such Party, and constitute a valid and binding agreement of such Party; and (iii) it has the full power, authority, and right to perform its obligations and grant the rights it grants hereunder.</p> <p>10.2 Disclaimer</p> <p>EXCEPT AS EXPRESSLY SET FORTH HEREIN, THE SERVICES, THE WEBSITE, ANY BETA FEATURES, ANY FREE TRIALS, THEIR COMPONENTS, THE DOCUMENTATION, THE SUPPORT SERVICES, AND ANY OTHER MATERIALS PROVIDED HEREUNDER ARE PROVIDED \u201cAS IS\u201d AND \u201cAS AVAILABLE,\u201d AND NEITHER PARTY MAKES ANY WARRANTIES WHATSOEVER AND HEREBY DISCLAIMS ANY AND ALL EXPRESS, IMPLIED,OR STATUTORY WARRANTIES, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF NON-INFRINGEMENT, MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AVAILABILITY, ERROR-FREE OR UNINTERRUPTED OPERATION, AND ANY WARRANTIES ARISING FROM A COURSE OF DEALING, COURSE OF PERFORMANCE, OR USAGE OF TRADE. TO THE EXTENT THAT EITHER PARTY MAY NOT AS A MATTER OF APPLICABLE LAW DISCLAIM ANY IMPLIED WARRANTY, THE SCOPE AND DURATION OF SUCH WARRANTY WILL BE THE MINIMUM PERMITTED UNDER SUCH LAW.</p>"},{"location":"docs/agreement/terms-of-use/#11-limitation-of-liability","title":"11. LIMITATION OF LIABILITY","text":"<p>IN NO EVENT SHALL WE BE LIABLE FOR ANY LOSS OR DAMAGE, OF ANY KIND, DIRECT OR INDIRECT, IN CONNECTION WITH OR ARISING FROM USE OF THE SERVICES, CONTENT, OR FROM THESE TERMS OF USE, INCLUDING, WITHOUT LIMITATION, COMPENSATORY, CONSEQUENTIAL, INCIDENTAL, INDIRECT, SPECIAL OR PUNITIVE DAMAGES, DAMAGES FOR LOSS OF PROFITS, LOSS OF DATA, BUSINESS INTERRUPTION OR ANY OTHER COMMERCIAL DAMAGES OR LOSSES, ARISING OUT OF OR RELATED TO THESE TERMS OF USE AND/OR YOUR USE OF OR ACCESS TO OR INABILITY TO USE OR ACCESS THE SERVICES, HOWEVER CAUSED, REGARDLESS OF THE THEORY OF LIABILITY (CONTRACT, TORT OR OTHERWISE), WHETHER OR NOT WE HAVE BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"docs/agreement/terms-of-use/#12-indemnification","title":"12. INDEMNIFICATION","text":"<p>We will defend, indemnify, and hold harmless you and your officers, directors, managers, members, and employees from any and all liabilities, costs, and expenses (including reasonable attorneys\u2019 fees) in connection with any third-party action, claim, or proceeding that the use of the Services infringes or misappropriates any third-party copyrights or trade secrets; provided, however, that the foregoing obligations shall be subject to your: (i) promptly notifying us of the claim; (ii) providing us, at our expense, with reasonable cooperation in the defense of the claim; and (iii) providing us with sole control over the defense and negotiations for a settlement or compromise, provided such settlement or compromise does not result in any liability for you or your officers, directors, managers, members or employees.</p> <p>We are not obligated to indemnify, defend, or hold you or any third party harmless hereunder to the extent: (i) the claim arises from or is based upon your or your Authorized Users\u2019 use of: (a) the Beta Features and/or during the Trial Period; (b) the Website not in accordance with these Terms of Use; or (c) any unauthorized modifications, alterations, or implementations of the Website made by you or at your request (other than by us); (ii) the claim arises from use of the Website in combination with unauthorized modules, apparatus, hardware, software, or services not supplied or specified in writing by us; or (iii) the claim arises from any use of the Website for which they were not designed.</p> <p>In the event that we reasonably determine that the Website is likely to be the subject of a claim of infringement or misappropriation of third-party rights, we shall have the right (but not the obligation), at our own expense and option, to: (i) procure for you the right to continue to use the Website as set forth hereunder; (ii) replace the infringing components of the Services with other components with the equivalent functionality; or (iii) suitably modify the Services so that it is non-infringing and functionally equivalent. If none of the foregoing options are available to us on commercially reasonable terms, we may terminate the applicable Services without further liability to you, and we shall refund to you an amount equal to a pro rata portion of any Fees prepaid by you for the Services. This Clause together with the Indemnity Clause, states your sole and exclusive remedy, and our sole and exclusive liability, regarding infringement or misappropriation of any intellectual property rights of a third party.</p> <p>You will defend, indemnify, and hold harmless us and our officers, directors, managers, members, and employees from any and all liabilities, costs, and expenses (including reasonable attorneys\u2019 fees) in connection with any third-party action, claim, or proceeding arising from your or any of your Authorized Users\u2019 breach or violation of the Terms of Use; provided, however, that the foregoing obligations shall be subject to our: (i) promptly notifying you of the claim; (ii) providing you, at your expense, with reasonable cooperation in the defense of the claim; and (iii) providing you with sole control over the defense and negotiations for a settlement or compromise, provided such settlement or compromise does not result in any liability for us or our officers, directors, managers, members or employees.</p>"},{"location":"docs/agreement/terms-of-use/#13-general-provisions","title":"13. GENERAL PROVISIONS","text":"<p>13.1 Assignment</p> <p>Neither Party may assign or otherwise transfer any of its rights or obligations under the Services without the prior, written consent of the other Party; provided, however, that a Party may, upon written notice to the other Party and without the consent of the other Party, assign or otherwise transfer the Services: (i) to any of its Affiliates; or (ii) in connection with a change of control transaction (whether by merger, consolidation, sale of equity interests, sale of all or substantially all assets, or otherwise), provided that in all cases, the assignee agrees in writing to be bound by the terms and conditions of the Services. Any assignment or other transfer in violation of this Section will be null and void.</p> <p>13.2 Waiver</p> <p>No failure or delay by either Party in exercising any right or remedy under the Services shall operate or be deemed as a waiver of any such right or remedy.</p> <p>13.3 Governing Law</p> <p>Any dispute arising from the Services shall be governed by and construed in accordance with the laws of the Republic of China (Taiwan)without regard for choice of law provisions thereof.</p> <p>13.4 Jurisdiction</p> <p>The Parties hereby consent and agree to the exclusive jurisdiction of the Taiwan Taipei District Court as the court of first instance for all suits, actions, or proceedings directly or indirectly arising out of or relating to the Services, and waive any and all objections to such courts, including but not limited to, objections based on improper venue or inconvenient forum, and each Party hereby irrevocably submits to the exclusive jurisdiction of such courts in any suits, actions, or proceedings arising out of or relating to any Services.</p> <p>13.5 Modifications</p> <p>we may modify these Terms of Use at any time by posting such modification on the Website and providing you notice of such update, and any such modification shall automatically go into effect thirty (30) days after it is so posted. In the event that you do not agree to the terms of any such modification, your sole remedy shall be to provide us with written notice during such thirty (30)-day period of your objection and desire to terminate the applicable Services, in which case the applicable Services shall terminate on the last day of such thirty (30)-day period and we shall refund to you an amount equal to a pro rata portion of any Fees prepaid by you for the Services. By continuing to use the Services after any such modification goes into effect, you agree to the terms of any such modification.</p> <p>13.6 Notices</p> <p>All notices must be in writing in one of the following forms: personal delivery, e-mail, national overnight courier or postal service, postage prepaid. Notices shall be effective upon: (i) actual delivery to the other Party, if delivered in person, or by email, or by national overnight courier; or (ii) five (5) business days after being mailed via postal service, postage prepaid.</p>"},{"location":"docs/architecture/overview/","title":"Overview","text":"<p>The core concepts in Recce are Tasks, Runs, and Checks.</p>"},{"location":"docs/architecture/overview/#tasks","title":"Tasks","text":"<p>A task represents the implementation of specific functionality. For example, tasks such as <code>QueryDiffTask</code> and <code>ValueDiffTask</code> demonstrate different types of operational processes.</p> <pre><code>def submit_run(task_type, params) -&gt; Run:\n    task = find_task(task_type)\n    result, error = task(params)\n\n    return Run(\n        type=task_type,\n        params=params,\n        result=result,\n        error=error,\n    )\n</code></pre> <p>A task is responsible for:</p> <ol> <li>Implementing the logic of the task, primarily involving accepting parameters and ultimately generating a result.</li> <li>Implementing cancellation.</li> <li>Implementing multi-threading.</li> <li>Reporting progress, including progress percentage and messages.</li> </ol>"},{"location":"docs/architecture/overview/#runs","title":"Runs","text":"<p>A run documents a single execution of a Task, capturing details such as the type of execution, parameters used, and the final outcomes. There are two methods to execute a run:</p> <ol> <li>Directly specify the type along with the corresponding parameters.     <pre><code>curl 'http://localhost:8000/api/runs' \\\n-H \"Content-Type: application/json\" \\\n-d @- &lt;&lt;EOF\n{\n    \"type\": \"query_diff\",\n    \"params\": {\n        \"sql_template\": \"select * from {{ ref('customers') }}\"\n    }\n}\nEOF\n</code></pre></li> <li>Execute via a specific check.     <pre><code>curl 'http://localhost:8000/api/checks/e2a44206-8568-44e9-8d36-da8856df477d/run' \\\n-H 'Content-Type: application/json' \\\n--data-raw '{}'\n</code></pre></li> </ol> <p>In the Recce instance, each run is recorded, so in theory, it is possible to access the run history. However, the current UI does not offer this feature. You can see the complete run history from the exported recce state.</p>"},{"location":"docs/architecture/overview/#checks","title":"Checks","text":"<p>A Check is an item recorded in the user\u2019s Checklist. A Check can be generated through a run or automatically created from preset checks when the Recce is initiated.</p> <p>A single Check can execute multiple runs; therefore, in theory, it is possible to obtain the run history of a Check, although the current UI does not yet implement this feature.</p> <p>Additionally, a Check records information in view_options, which is used to determine how the run results are displayed.</p>"},{"location":"docs/features/breaking-change-analysis/","title":"Breaking Change Analysis","text":"<p>Breaking Change Analysis examines modified models and categorizes changes into three types:</p> <ul> <li>Breaking changes</li> <li>Partial breaking changes</li> <li>Non-breaking changes</li> </ul> <p>It's generally assumed that any modification to a model\u2019s SQL will affect all downstream models. However, not all changes have the same level of impact. For example, formatting adjustments or the addition of a new column should not break downstream dependencies. Breaking change analysis helps you assess whether a change affects downstream models and, if so, to what extent.</p>"},{"location":"docs/features/breaking-change-analysis/#categories-of-change","title":"Categories of change","text":"Category Downstream Impact Examples Non-breaking change No downstream models are affected New column, formatting SQL, adding comments Partial breaking change Only downstream models referencing certain columns are affected Removing, renaming, or modifying the definition of a column Breaking change All downstream models are affected Changing filter conditions (e.g. <code>WHERE</code>), sort order (<code>ORDER BY</code>), or other SQL logic"},{"location":"docs/features/breaking-change-analysis/#usage","title":"Usage","text":"<p>To enable Breaking Change Analysis, click the toggle on the  Lineage page. </p> <p>All modified models display their change category directly on the node. Additionally, partial breaking changes are highlighted with a dashed orange border to indicate that they may not impact downstream models.</p> DisabledEnabled <p></p> <p></p>"},{"location":"docs/features/breaking-change-analysis/#column-level-lineage","title":"Column-Level Lineage","text":"<p>In models classified as non-breaking or partial breaking -  added, removed, or modified columns will be listed. Click on a column to open its Column-Level Lineage</p> <p></p>"},{"location":"docs/features/breaking-change-analysis/#limitations","title":"Limitations","text":"<p>The current implementation of breaking change analysis is still very conservative. As a result, a modified model may be classified as a breaking change when it is actually non breaking or partial breaking changes. Common cases include:</p> <ol> <li>Logical equivalence in operations, such as changing <code>a + b</code> to <code>b + a</code>.</li> <li>Adding a <code>LEFT JOIN</code> to a table and selecting columns from it. This is often used to enrich the current model with additional dimension table data without affecting existing downstream tables.</li> <li>All modified python models or seeds are treated as breaking change.</li> </ol>"},{"location":"docs/features/breaking-change-analysis/#technology","title":"Technology","text":"<p>Breaking Change Analysis is powered by the SQL analysis and AST diff capabilities of SQLGlot to  compare two SQL semantic trees.</p>"},{"location":"docs/features/checklist/","title":"Checklist","text":"<p>Save your validation checks to the Recce checklist with a description of your findings.</p> <p>These checks can later be added to your pull request comment as proof-of-correctness for your modeling changes.</p> <p> </p> Checklist"},{"location":"docs/features/column-level-lineage/","title":"Column-Level Lineage","text":"<p>Column-Level Lineage provides visibility into the upstream and downstream relationships of a column. </p> <p>Common use-cases for column-level lineage are</p> <ol> <li>Source Exploration: During development, column-level lineage helps you understand how a column is derived.</li> <li>Impact Analysis: When modifying the logic of a column, column-level lineage enables you to assess the potential impact across the entire DAG.</li> <li>Root Cause Analysis: Column-level lineage helps identify the possible source of errors by tracing data lineage at the column level.</li> </ol>"},{"location":"docs/features/column-level-lineage/#usage","title":"Usage","text":"<ol> <li> <p>Select a node in the lineage DAG, then click the eye icon next to the column you want to view.</p> <p></p> </li> <li> <p>The column-level lineage for the selected column will be displayed.</p> <p></p> </li> </ol>"},{"location":"docs/features/column-level-lineage/#transformation-types","title":"Transformation Types","text":"<p>The transformation type is also displayed for each column, which will help you understand how the column was generated or modified.</p> Type Description Pass-through The column is directly selected from the upstream table. Renamed The column is selected from the upstream table but with a different name. Derived The column is created through transformations applied to upstream columns, such as calculations, conditions, functions, or aggregations. Source The column is not derived from any upstream data. It may originate from a seed/source node, literal value, or data generation function. Unknown We have no information about the transformation type. This could be due to a parse error, or other unknown reason."},{"location":"docs/features/column-level-lineage/#limitation","title":"Limitation","text":"<p>Column-level lineage only displays column selection operations. It does not indicate if a column has been used in filters (WHERE clauses), with grouping (GROUP BY), joins, or other transformations.</p>"},{"location":"docs/features/lineage/","title":"Lineage","text":"<p>The Lineage Diff is the main interface to Recce and allows you to quickly see the potential area of impact from your dbt data modeling changes.</p>"},{"location":"docs/features/lineage/#lineage-diff","title":"Lineage Diff","text":"<p>It's from the Lineage Diff that you will determine which models to investigate further; and also perform the various data validation checks that will serve as proof-of-correctness of your work.</p> <p> </p> Lineage Diff"},{"location":"docs/features/lineage/#node-summary","title":"Node Summary","text":"<p>Models are color-coded to indicate their status:</p> <ul> <li><code>Added</code> models are green.</li> <li><code>Removed</code> models are red.</li> <li><code>Modified</code> models are orange.</li> </ul> <p>The two icons at the bottom right of each node indicate if a <code>row count</code> or <code>schema</code> change has been detected. Grayed out icons indicate no change.</p> <p> </p> Model with Schema Change detected <p>Note: A row count changed icon is only shown if there is row count diff executed on this node.</p> <p> </p> Open the node details panel <p>Click a model to open the node details panel and perform other data validation checks.</p>"},{"location":"docs/features/lineage/#filter-nodes","title":"Filter Nodes","text":"<p>In the top control bar, you can change the rule to filter the nodes:</p> <ol> <li>Mode:<ul> <li>Changed Models: Modified nodes and their downstream + 1st degree of their parents.</li> <li>All: Show all nodes.</li> </ul> </li> <li>Package: Filter by dbt package names.</li> <li>Select: Select nodes by node selection.</li> <li>Exclude: Exclude nodes by node selection.</li> </ol>"},{"location":"docs/features/lineage/#select-nodes","title":"Select Nodes","text":"<p>Click a node to select it, or click the  Select nodes button at the top-right corner to select multiple nodes for further operations. For detail, see the Multi Nodes Selections section </p>"},{"location":"docs/features/lineage/#row-count-diff","title":"Row Count Diff","text":"<p>A row count diff can be performed on nodes selected using the <code>select</code> and <code>exclude</code> options:</p> <p></p> <p>After selecting nodes, run the row count diff by:</p> <ol> <li>Clicking the 3 dots (...) button at the top-right corner.</li> <li>Clicking Row Count Diff by Selector.</li> </ol>"},{"location":"docs/features/lineage/#node-details","title":"Node Details","text":"<p>The node details panel shows information about a node, such as node type, schema and row count changes, and allows you to perform diffs on the node using the options accessed via the <code>Explore Change</code> button. </p>"},{"location":"docs/features/lineage/#schema-diff","title":"Schema Diff","text":"<p>Schema Diff shows added, removed, and renamed columns. Click a model in the Lineage Diff to open the node details and view the Schema Diff.</p> <p>Note</p> <p>Schema Diff requires <code>catalog.json</code> in both environments.</p> <p> </p> Schema Diff <p> </p> Schema Diff showing renamed column"},{"location":"docs/features/lineage/#row-count-diff_1","title":"Row Count Diff","text":"<p>Row Count Diff shows the difference in row count between the base and current environments.</p> <ol> <li>Click the model in the Lineage DAG.</li> <li>Click the <code>Explore Change</code> button in the node details panel.</li> <li>Click <code>Row Count Diff</code>.</li> </ol> <p> </p> Row Count Diff - Single model"},{"location":"docs/features/lineage/#code-diff","title":"Code Diff","text":"<p>Code Diff shows the model code that has changed for a particular model.</p> <p></p> <ol> <li>Click the model in the Lineage DAG.</li> <li>Click the <code>Explore Change</code> button in the node details panel.</li> <li>Click <code>Code Diff</code>.</li> </ol>"},{"location":"docs/features/lineage/#value-diff","title":"Value Diff","text":"<p>Value Diff shows the matched count and percentage for each column in the table. It uses the primary key(s) to uniquely identify the records between the model in both environments.</p> <p>The primary key is automatically inferred by the first column with the unique test. If no primary key is detected at least one column is required to be specified as the primary key.</p> <p> </p> Value Diff <ul> <li>Added: Newly added PKs.</li> <li>Removed: Removed PKs.</li> <li>Matched: For a column, the count of matched value of common PKs.</li> <li>Matched %: For a column, the ratio of matched over common PKs.</li> </ul> <p>Note</p> <p>Value Diff uses the <code>compare_column_values</code> from audit-helper. To use Value Diff, ensure that <code>audit-helper</code> is installed in your project.</p> <pre><code>packages:\n  - package: dbt-labs/audit_helper\n    version: &lt;version&gt;\n</code></pre> <p>View mismatched values at the row level by clicking the <code>show mismatched values</code> option on a column name:</p> <p></p>"},{"location":"docs/features/lineage/#profile-diff","title":"Profile Diff","text":"<p>Profile Diff compares the basic statistic (e.g. count, distinct count, min, max, average) for each column in models between two environments.</p> <ol> <li>Select the model from the Lineage DAG.</li> <li>Click the <code>Expore Change</code> button.</li> <li>Click <code>Profile Diff</code>.</li> </ol> <p> </p> Profile Diff <p>Please refer to the dbt-profiler documentation for the definitions of profiling stats.</p> <p>Note</p> <p>Profile diff uses the <code>get_profile</code> from dbt-profiler. To use Profile Diff, ensure that dbt-profiler is installed in your project.</p> <pre><code>packages:\n  - package: data-mie/dbt_profiler\n    version: &lt;version&gt;\n</code></pre>"},{"location":"docs/features/lineage/#histogram-diff","title":"Histogram Diff","text":"<p>Histogram Diff compares the distribution of a numeric column in an overlay histogram chart.   </p> <p> </p> Histogram Diff <p>A Histogram Diff can be generated in two ways.</p> <p>Via the Explore Change button menu:</p> <ol> <li>Select the model from the Lineage DAG.</li> <li>Click the <code>Explore Change</code> button.</li> <li>Click <code>Histogram Diff</code>.</li> <li>Select a column to diff.</li> <li>Click <code>Execute</code>.</li> </ol> <p>Via the column options menu:</p> <ol> <li>Select the model from the Lineage DAG.</li> <li>Hover over the column in the Node Details panel.</li> <li>Click the vertical 3 dots <code>...</code></li> <li>Click <code>Histogram Diff</code>.</li> </ol> <p> </p> Generate a Recce Histogram Diff from the column options"},{"location":"docs/features/lineage/#top-k-diff","title":"Top-K Diff","text":"<p>Top-K Diff compares the distribution of a categorical column. The top 10 elements are shown by default, which can be expanded to the top 50 elements.</p> <p> </p> Recce Top-K Diff <p>A Top-K Diff can be generated in two ways.</p> <p>Via the Explore Change button menu:</p> <ol> <li>Select the model from the Lineage DAG.</li> <li>Click the <code>Explore Change</code> button.</li> <li>Click <code>Top-K Diff</code>.</li> <li>Select a column to diff.</li> <li>Click <code>Execute</code>.</li> </ol> <p>Via the column options menu:</p> <ol> <li>Select the model from the Lineage DAG.</li> <li>Hover over the column in the Node Details panel.</li> <li>Click the vertical 3 dots <code>...</code></li> <li>Click <code>Top-K Diff</code>.</li> </ol> <p> </p> Generate a Recce Top-K Diff"},{"location":"docs/features/lineage/#multi-node-selection","title":"Multi-Node Selection","text":"<p>Multiple nodes can be selected in the Lineage DAG. This enables actions to be performed on multiple nodes at the same time such as Row Count Diff, or Value Diff.</p>"},{"location":"docs/features/lineage/#select-nodes-individually","title":"Select Nodes Individually","text":"<p>To select multiple nodes individually, click the check box on the nodes you wish to select.</p> <p> </p> Select multiple nodes individually"},{"location":"docs/features/lineage/#select-parent-or-child-nodes","title":"Select Parent or Child nodes","text":"<p>To select a node and all of its parents or children:</p> <ol> <li>Click the checkbox on the node.</li> <li>Right click the node.</li> <li>Click to select either parent or child nodes.</li> </ol> <p> </p> Select a node and its parents or children"},{"location":"docs/features/lineage/#perform-actions-on-multiple-nodes","title":"Perform actions on multiple nodes","text":"<p>After selecting the desired nodes, use the Actions menu at the top right of the screen to perform diffs or add checks.</p> <p> </p> Perform actions on multiple nodes"},{"location":"docs/features/lineage/#example-row-count-diff","title":"Example - Row Count Diff","text":"<p>An example of selecting multiple nodes to perform a multi-node row count diff:</p> <p> </p> Perform a Row Count Diff on multiple nodes"},{"location":"docs/features/lineage/#example-value-diff","title":"Example - Value Diff","text":"<p>An example of selecting multiple nodes to perform a multi-node Value Diff:</p> <p> </p> Perform a Value Diff on multiple nodes"},{"location":"docs/features/lineage/#screenshot","title":"Screenshot","text":"<p>In the diff result, we can find a Copy to Clipboard button. it's a handy feature to copy the result image to clipboard and paste in your PR comment.</p> <p> </p> Copy a diff result screenshot to the clipboard and paste to GitHub <p>Note</p> <p>FireFox does not support to copy image to clipboard. Recce show a modal instead. You can download the image to local or right-click on the image to copy the image.</p>"},{"location":"docs/features/lineage/#add-to-checklist","title":"Add to Checklist","text":"<p>The Recce Checklist provides a way to record the results of a data check during change exploration. The purpose of adding Checks to the Checklist is to enable you to:</p> <ul> <li>Save Checks with notes of your interpretation of the data.</li> <li>Re-run checks following further data modeling changes.</li> <li>Share Checks as part of PR or stakeholder review.</li> </ul>"},{"location":"docs/features/lineage/#schema-and-lineage-diff","title":"Schema and Lineage Diff","text":"<p>From the Lineage DAG, click the Actions dropdown menu and click Lineage Diff or Schema Diff from the Add to Checklist section. This will add:</p> <ul> <li>Lineage Diff: The current Lineage view, dependent on your node selection options.</li> <li>Schema Diff: A diff of all nodes if none are selected, or specific selected nodes.</li> </ul> <p> </p> Add a Lineage Diff Check or Schema Check via the Actions dropdown menu"},{"location":"docs/features/lineage/#diffs-performed-via-the-explore-change-dropdown-menu","title":"Diffs performed via the Explore Change dropdown menu","text":"<p>For the majority of diffs, which are performed via the Explore Change dropdown menu, the Check can be added by clicking the Add to Checklist button in the results panel:</p> <p> </p> Add a Check by clicking the Add to Checklist button in the diff results panel <p>An example performing a Top-K diff and adding the results to the Checklist:</p> <p> </p> Example adding a Top-K Diff to the Checklist"},{"location":"docs/features/node-selection/","title":"Node Selection","text":"<p>Recce supports dbt node selection in the lineage diff. This enables you to target specific resources with data checks by selecting or excluding nodes.</p>"},{"location":"docs/features/node-selection/#supported-syntax-and-methods","title":"Supported syntax and methods","text":"<p>Since Recce uses dbt's built-in node selector, it supports most of the selecting methods. Here are some examples:</p> <ul> <li>Select a node: <code>my_model</code></li> <li>select by tag: <code>tag:nightly</code></li> <li>Select by wildcard: <code>customer*</code></li> <li>Select by graph operators:  <code>my_model+</code>, <code>+my_model</code>, <code>+my_model</code>, <code>1+my_model+</code></li> <li>Select by union: <code>model1 model2</code></li> <li>Select by intersection: <code>stg_invoices+,stg_accounts+</code></li> <li>Select by state: <code>state:modified</code>, <code>state:modified+</code></li> </ul>"},{"location":"docs/features/node-selection/#use-state-method","title":"Use <code>state</code> method","text":"<p>In dbt, you need to specify the <code>--state</code> option in the CLI. In Recce, we use the base environment as the state, allowing you to use the selector on the fly.</p>"},{"location":"docs/features/node-selection/#removed-nodes","title":"Removed nodes","text":"<p>Another difference is that in dbt, you cannot select removed nodes. However, in Recce, you can select removed nodes and also find them using the graph operator. This is a notable distinction from dbt's node selection capabilities.</p>"},{"location":"docs/features/node-selection/#supported-diff","title":"Supported Diff","text":"<p>In addition to lineage diff, other types of diff also support node selection. You can find these features in the ... button at the top right corner. Currently supported diffs include:</p> <ul> <li>Lineage diff</li> <li>Row count diff</li> <li>Schema diff</li> </ul> <p></p>"},{"location":"docs/features/node-selection/#limitation","title":"Limitation","text":"<ul> <li>\"result\" method not supported</li> <li>\"source_status\" method not supported.</li> <li>YAML selectors not supported.</li> </ul>"},{"location":"docs/features/preset-checks/","title":"Preset Checks","text":"<p>In a dbt project, there may be some checks that need to be conducted for every PR. For example, this could be an SQL query, or checking whether an important model has had a schema change.</p> <p>Preset checks can be the fixed checks that are generated every time a new Recce instance is initiated.</p>"},{"location":"docs/features/preset-checks/#configure-the-preset-check","title":"Configure the Preset Check","text":"<p>To configure the preset checks, add the settings to the recce config file.</p> <ol> <li>Add a check to your checklist     </li> <li>Open the menu for the check and select Get Preset Check Template.</li> <li> <p>Copy the yaml config from the dialog     </p> </li> <li> <p>Paste the config into the <code>recce.yml</code> file located at the root of the project:</p> <pre><code># recce.yml\nchecks:\n  - name: Query diff of customers\n    description: |\n      This is the demo preset check.\n\n      Please run the query and paste the screenshot to the PR comment.\n    type: query_diff\n    params:\n      sql_template: select * from {{ ref(\"customers\") }}\n    view_options:\n      primary_keys:\n        - customer_id\n</code></pre> </li> </ol>"},{"location":"docs/features/preset-checks/#create-the-preset-checks","title":"Create the Preset Checks","text":""},{"location":"docs/features/preset-checks/#recce-server","title":"Recce Server","text":"<ol> <li>When a new Recce instance is launched, all preset checks are automatically set up, but these checks are not executed at this time.     </li> <li>When the Run Query button is pressed, the check will be executed.</li> </ol>"},{"location":"docs/features/preset-checks/#recce-run","title":"Recce Run","text":"<ol> <li>Running <code>recce run</code> executes all preset checks. The default output file is recce_state.json.     <pre><code>$ recce run\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 DBT Artifacts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nBase:\n    Manifest: 2024-04-10 08:54:41.546402+00:00\n    Catalog:  2024-04-10 08:54:42.251611+00:00\nCurrent:\n    Manifest: 2024-04-22 03:24:11.262489+00:00\n    Catalog:  2024-04-10 06:15:13.813125+00:00\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Preset checks \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                            Recce Preset Checks\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nStatus      Name                 Type         Execution Time   Failed Reason\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n[Success]   Query of customers   Query Diff   0.10 seconds     N/A\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nThe state file is stored at [recce_state.json]\n</code></pre></li> <li>You can view the check results by launching the recce server.     <pre><code>recce server recce_state.json\n</code></pre></li> <li>You can show the summary of the state by the recce summary command     <pre><code>recce summary recce_state.json\n</code></pre></li> </ol>"},{"location":"docs/features/query/","title":"Query","text":"<p>Query page provides an AdHoc query interface to run arbitrary query or diff the query result between two environments. If you're a dbt user, you can use any dbt macros that are installed in your project.</p>"},{"location":"docs/features/query/#execute-query","title":"Execute Query","text":"<pre><code>select * from {{ ref(\"mymodel\") }}\n</code></pre> <p>Actions</p> <ul> <li>Run: performs query in the current environment.</li> <li>Run Diff: performs the same query in both environments and diffs the results.</li> </ul> <p>Form</p> <ul> <li>Primary key: select the primary key(s) used to compare the query results.</li> </ul> <p>Note</p> <p>If the primary key(s) is specified, the query will occur in the warehouse; otherwise, the query will happen across two environments and the comparison will take place on the client side.</p> <p></p> <p>Tip</p> <p>In Mac, you can use <code>\u2318 Enter</code> to run a query or use <code>\u2318 \u21e7 Enter</code> to run a query diff.</p>"},{"location":"docs/features/query/#query-result","title":"Query Result","text":"<ul> <li>Primary Key: When comparison occurs on the client side, we can select the primary key by clicking the <code>key</code> icon. The primary key columns are used to be identified as the same record for both sides. If no primary key is specified, the records is compared by the row's index.</li> <li>Pinned Column: The pinned column would show first in the column list.</li> <li>Changed Only: By selecting the Changed only, we can show only the changed rows and columns. The pinned columns are always shown even they are not changed.</li> </ul>"},{"location":"docs/features/query/#shortcut-to-query-a-model","title":"Shortcut to query a model","text":"<ol> <li>In the lineage page, select a model</li> <li>Click the Query button</li> <li>Then the query page is shown and filled with the query for this model</li> </ol>"},{"location":"docs/features/query/#add-to-checklist","title":"Add to Checklist","text":"<p>Click the <code>+</code> button in the result pane, then you can add the query result to the checklist.</p>"},{"location":"docs/features/query/#how-query-diff-works","title":"How Query Diff Works","text":"<p>In the current version, Recce provides two ways to compare the query result between two environments.</p> <p>Query diff occurs in the client side:</p> <p>Without providing primary key(s) upfront, AdHoc query compare in the client side. That is, Recce fetches the first 2,000 rows and compare in the client side. The advantage is it has more flexibility to query sql for no PK, especially when column structures differ or no clear primary key exists. However, the limitation is that we cannot find the mismatched rows in a big query result.</p> <p>Query diff occurs in the warehouse:</p> <p>With primary key(s) given, it can perform a query diff in the warehouse. It only displays changed, added, or removed rows. Therefore, if only one record is different among a million, that specific record will be visible. Hence, it also reduces the amount of data transferred.</p> <p>Another similar feature is Value Diff. Value diff is based on a chosen model, so you don\u2019t need to write SQL to operate it, though it naturally offers less flexibility. Additionally, value diff can show a summary or actual diff records, whereas query diff only shows the actual diff records.</p>"},{"location":"docs/features/recce-run/","title":"Run","text":"<p>Recce provides a Web UI for interactively exploring and comparing two environments in depth. However, in the CI workflow, using the command line interface is more appropriate.</p> <p>To execute Recce from the command line:</p> <pre><code>recce run\n</code></pre> <p>or to specify an output file:</p> <pre><code>recce run -o recce_pr123.json\n</code></pre> <p>This command will:</p> <ol> <li>Collect the dbt artifacts for both the base and current environments.</li> <li>Run the preset checks: Preset checks are checks that have been configured specifically for this dbt project.</li> <li>Output the result. The default output path is <code>recce_state.json</code>.</li> </ol>"},{"location":"docs/features/recce-summary-example/","title":"Recce Summary","text":""},{"location":"docs/features/recce-summary-example/#lineage-graph","title":"Lineage Graph","text":"<pre><code>graph LR\nmodel.jaffle_shop.customers[\"customers\n\n[What's Changed]\nCode, Schema, Value Diff\"]\nstyle model.jaffle_shop.customers stroke:#ffa502\nmodel.jaffle_shop.customers----&gt;model.jaffle_shop.customer_segments\nmodel.jaffle_shop.customers----&gt;model.jaffle_shop.customer_order_pattern\nmodel.jaffle_shop.customer_segments[\"customer_segments\"]\nmodel.jaffle_shop.customer_order_pattern[\"customer_order_pattern\"]\n</code></pre>"},{"location":"docs/features/recce-summary-example/#checks-summary","title":"Checks Summary","text":"Total Checks Data Mismatch Detected 5 3"},{"location":"docs/features/recce-summary-example/#checks-of-data-mismatch-detected","title":"Checks of Data Mismatch Detected","text":"Name Type Related Models Model schema of customers Schema Diff customers Value diff of customers Value Diff customers Query diff of customers avg lifetime value Query Diff N/A"},{"location":"docs/features/recce-summary/","title":"Summary","text":"<p>Recce <code>Summary</code> command is used to generate a summary based on the input state file. In the previous section, the <code>Run</code> command was used to generate a state file based on the two environments. It provides a way to integrate Recce into your CI/CD pipeline. The <code>Summary</code> command is used to generate a summary based on the output of <code>Run</code> command. You can also integrate the <code>Summary</code> command into your CI/CD pipeline to generate a summary based on the state file generated by the <code>Run</code> command. Therefor, the generated summary can be posted to your repository hosting platform, such as GitHub, GitLab, or Bitbucket.</p>"},{"location":"docs/features/recce-summary/#usage","title":"Usage","text":"<pre><code>recce summary &lt;Path-of-recce-state-file&gt;\n</code></pre>"},{"location":"docs/features/recce-summary/#example","title":"Example","text":"<pre><code>recce summary recce-state.json\n</code></pre>"},{"location":"docs/features/recce-summary/#output","title":"Output","text":"<p>The output of the <code>summary</code> command will be markdown format. The markdown output will contain the following sections:</p> <ul> <li>Lineage Graph - A graph that shows the lineage of the models that are impacted by the modified models.</li> <li>Checks Summary - A summary of the checks that are detected mismatch between <code>base</code> and <code>current</code> environments.</li> </ul>"},{"location":"docs/features/recce-summary/#example-output","title":"Example Output","text":"<pre><code># Recce Summary\n\n## Lineage Graph\n\n```mermaid\ngraph LR\nmodel.jaffle_shop.customers[\"customers\n\n[What's Changed]\nCode, Schema, Value Diff\"]\nstyle model.jaffle_shop.customers stroke:#ffa502\nmodel.jaffle_shop.customers----&gt;model.jaffle_shop.customer_segments\nmodel.jaffle_shop.customers----&gt;model.jaffle_shop.customer_order_pattern\nmodel.jaffle_shop.customer_segments[\"customer_segments\"]\nmodel.jaffle_shop.customer_order_pattern[\"customer_order_pattern\"]\n\n```\n\n## Checks Summary\n\n| Total Checks | Data Mismatch Detected |\n| ------------ | ---------------------- |\n| 5            | 3                      |\n\n### Checks of Data Mismatch Detected\n\n| Name                                       | Type        | Related Models |\n| ------------------------------------------ | ----------- | -------------- |\n| Model schema of customers                  | Schema Diff | customers      |\n| Value diff of customers                    | Value Diff  | customers      |\n| Query diff of customers avg lifetime value | Query Diff  | N/A            |\n</code></pre> <p>The rendered output will look like this. Example Output</p>"},{"location":"docs/features/recce-summary/#content-of-the-summary","title":"Content of the Summary","text":""},{"location":"docs/features/recce-summary/#lineage-graph","title":"Lineage Graph","text":"<p>The lineage graph shows the lineage of the models that are impacted by the modified models. The graph is generated using the <code>mermaid</code> library. The graph is a directed graph that shows the relationship between the models. The graph is generated based on the modified models and their's children models. If the model is modified or impacted by the modified model, it will be highlighted in the <code>[What's Changed]</code> section.</p>"},{"location":"docs/features/recce-summary/#example-of-lineage-graph","title":"Example of Lineage Graph","text":"<pre><code>graph LR\nmodel.jaffle_shop.customers[\"customers\n\n[What's Changed]\nCode, Schema, Value Diff\"]\nstyle model.jaffle_shop.customers stroke:#ffa502\nmodel.jaffle_shop.customers----&gt;model.jaffle_shop.customer_segments\nmodel.jaffle_shop.customers----&gt;model.jaffle_shop.customer_order_pattern\nmodel.jaffle_shop.customer_segments[\"customer_segments\"]\nmodel.jaffle_shop.customer_order_pattern[\"customer_order_pattern\"]</code></pre>"},{"location":"docs/features/recce-summary/#checks-summary","title":"Checks Summary","text":"<p>Shows the total number of checks and the number of data mismatched checks. The table will contain the following columns:</p> <ul> <li>Total Checks - The total number of checks.</li> <li>Data Mismatch Detected - The number of checks which data mismatched between <code>base</code> and <code>current</code> environments.</li> </ul>"},{"location":"docs/features/recce-summary/#checks-of-data-mismatch-detected","title":"Checks of Data Mismatch Detected","text":"<p>Shows the checks that are detected data mismatch between <code>base</code> and <code>current</code> environments. If the check is detected data mismatch, we will suggest the PR reviewer should take a look at the check and investigate the data mismatch is expected or not. The table will contain the following columns:</p> <ul> <li>Name - The name of the check.</li> <li>Type - The type of the check.</li> <li>Related Models - The models that are related to the check. If a check is not related to any models, it will be <code>N/A</code>.</li> </ul> <p>If all the check between <code>base</code> and <code>current</code> environments are matched, the <code>Checks of Data Mismatch Detected</code> section will not be shown.</p>"},{"location":"docs/features/recce-summary/#example-of-data-mismatch-detected","title":"Example of Data Mismatch Detected","text":"Name Type Related Models Model schema of customers Schema Diff customers Value diff of customers Value Diff customers Query diff of customers avg lifetime value Query Diff N/A"},{"location":"docs/features/recce-summary/#how-to-integrate-with-cicd-pipeline","title":"How to Integrate with CI/CD Pipeline","text":"<p>The generated summary can be posted to any kinds of repository hosting platform, such as GitHub, GitLab, or Bitbucket. Please refer to the Recce CI integration with GitHub Action section for more information on how to integrate Recce with your CI/CD pipeline.</p>"},{"location":"docs/features/state-file/","title":"Recce State File","text":""},{"location":"docs/features/state-file/#introduction","title":"Introduction","text":"<p>The state file represents the serialized state of a Recce instance. It is a JSON-formatted file containing the following information:</p> <ul> <li>Checks: Data from the checks added to the checklist on the Checklist page.</li> <li>Runs: Each diff execution in Recce corresponds to a run, similar to a query in a data warehouse. Typically, a single run submits a series of queries to the warehouse and retrieves the final results.</li> <li>Environment Artifacts: Includes <code>manifest.json</code> and <code>catalog.json</code> files for both the base and current environments.</li> <li>Runtime Information: Metadata such as Git branch details and pull request (PR) information from the CI runner.</li> </ul>"},{"location":"docs/features/state-file/#how-to-save-the-state-file","title":"How to Save the State File","text":"<p>There are multiple ways to save the state file.</p> <ol> <li> <p>Save from the Web UI: Click the Save button at the top of the app. Recce will continuously write updates to the state file, effectively working like an auto-save feature, and persist the state until the Recce instance is closed. The file is saved with the specified filename in the directory where the recce server command is run.</p> </li> <li> <p>Export from the Web UI: Click the Export button located in the top-right corner to download the current Recce state to any location on your machine.    </p> </li> <li> <p>Start Recce from a State File: You can provide a state file as an argument when launching Recce. If the file does not exist, Recce will create a state file and start with an empty state. If the file exists, Recce will load the state and continue working from it.    <pre><code>recce server my_recce_state.json\n</code></pre></p> </li> <li> <p>Use the <code>run</code> Command: For more complex dbt projects with a CI/CD process, where dbt transformations are executed and results are placed in a PR-specific environment, you can integrate the recce run command into your workflow. This allows reviewers to easily audit results and decide whether a merge can proceed.    <pre><code># create state file at recce_state.json\nrecce run\n</code></pre></p> </li> </ol>"},{"location":"docs/features/state-file/#how-to-use-the-state-file","title":"How to Use the State File","text":"<p>The state file can be used in several ways:</p> <ol> <li>Continue the state: Launch Recce with the specified state file.    <pre><code>recce server my_recce_state.json\n</code></pre></li> <li>Review the state: Running Recce with the <code>--review</code> option enables review mode. In this mode, Recce uses the dbt artifacts in the state file instead of those in the <code>target/</code> and <code>target-base/</code> directories. This option is useful for distinguishing between development and review purposes.    <pre><code>recce server --review my_recce_state.json\n</code></pre></li> <li>Import checklist from file: To preserve favorite checks across different branches, you can import a checklist by clicking the Import button at the top of the checklist.</li> <li>Continue the state from <code>recce run</code>: This will execute the checks in the specified state file.    <pre><code>recce run --state-file my_recce_state.json\n</code></pre></li> </ol>"},{"location":"docs/features/state-file/#scenario-development","title":"Scenario: Development","text":"<p>In the development workflow, the state file acts as a session for developing a feature. It allows you to store checks to verify the diff results against the base environment.</p> <p>Common development workflow:</p> <ol> <li>Run the recce server without a state file.     <pre><code>recce server\n</code></pre></li> <li>Add checks to the checklist. </li> <li>Save the state by clicking the Save or Export button.</li> <li>Resume your session by launching Recce with the specific state file.     <pre><code>recce server recce_issue_1.json\n</code></pre></li> </ol> <p></p>"},{"location":"docs/features/state-file/#scenario-pr-review","title":"Scenario: PR Review","text":"<p>During the PR review process, the state file serves as a communication medium between the submitter and the reviewer.</p> <ol> <li>Start the Recce server without a state file.     <pre><code>recce server\n</code></pre></li> <li>Add checks to the checklist. </li> <li>Save the state by clicking the Save or Export button.</li> <li>Share the state file with the reviewer or attach it as a comment in the pull request.</li> <li> <p>The reviewer reviews the results using the state file</p> <pre><code>recce server --review recce_issue_1.json\n</code></pre> </li> </ol> <p></p>"},{"location":"docs/features/state-file/#recce-cloud","title":"Recce Cloud","text":"<p>Note</p> <p>Currently, Recce Cloud is still under development. If you are interested, please sign up for a Recce Cloud invite or contact us in the dbt slack #tool-recce channel</p> <p>Although a state file can store the state, it is not very suitable for recording the latest review status of a PR. Especially since a PR may include the submitter, reviewer, and the automated processes in the CI workflow. The purpose of Recce Cloud is to solve the PR review status management issue.</p> <p>Prerequisites</p> <p>Before uploading the state file to Recce Cloud, you need to define a password to encrypt the state file. The password is used to encrypt the state file before uploading it to Recce Cloud. It will also be used to decrypt the state file when you download it from Recce Cloud. The password is not stored in Recce Cloud, so you need to keep it safe.</p>"},{"location":"docs/features/state-file/#pr-review-workflow","title":"PR Review Workflow","text":"<p>This is a most common workflow: the submitter pushes commits to the GitHub PR, and the reviewer is responsible for reviewing/auditing the PR. This includes checking code changes, ensuring requirements are met, and assessing whether there is any impact on existing models.</p> <p>As a submitter</p> <ol> <li>Push changes to the remote</li> <li>Create PR for review</li> <li>Run dbt to prepare the PR review environment    <pre><code>dbt run\n</code></pre></li> <li>Launch recce server for this PR branch     <pre><code>recce server --cloud --password &lt;password-to-encrypt-state-file&gt;\n</code></pre></li> <li>Add recce checks for review</li> <li>Leave description and screenshots in the PR comments</li> </ol> <p>As a reviewer</p> <ol> <li>Checkout the PR branch</li> <li>Launch recce server for this PR branch in the review mode    <pre><code>recce server --review --cloud --password &lt;password-to-encrypt-state-file&gt;\n</code></pre></li> <li>If all checks are good, mark all checks as Approved. Otherwise, leave comment in the PR comment or the recce check description.</li> </ol>"},{"location":"docs/features/state-file/#pr-review-workflow-with-ci","title":"PR Review Workflow with CI","text":"<p>For more mature projects, we introduce CI automation to standardize the process and reduce human-caused variability or errors. In the workflow, we will do the following two things in the CI:</p> <p>Execute dbt to create a PR environment. Execute recce to update dbt artifacts, rerun check runs, and update the PR status to Recce Cloud.</p> <p>As a submitter</p> <ol> <li>Push changes to the remote</li> <li>Create PR for review</li> </ol> <p>In the CI workflow of the PR push event</p> <ol> <li>The github action workflow is triggered by the push event</li> <li>Checkout the PR branch</li> <li>Fetch the dbt artifacts for the base environment</li> <li>Run dbt for the PR environment    <pre><code>dbt run\n</code></pre></li> <li>Run recce for the current PR and upload state to the recce cloud.     <pre><code>recce run --cloud --password &lt;password-to-encrypt-state-file&gt;\n</code></pre></li> </ol> <p>As a submitter and reviewer, collaborate the state in the review mode recce server</p> <ol> <li>Checkout the PR branch</li> <li>Launch recce server for this PR branch in the review mode    <pre><code>recce server --review --cloud --password &lt;password-to-encrypt-state-file&gt;\n</code></pre></li> </ol>"},{"location":"docs/guides/best-practices-prep-env/","title":"Best Practices for Preparing Environments","text":"<p>Recce is designed to compare two environments in your data project. To use it effectively, it is crucial to prepare environments through CI.</p> <p>However, there are many challenges in preparing environments.</p> <ol> <li>Your source data might be continuously updating.</li> <li>Your transformations might be time-consuming.</li> <li>The base branch may have other PRs merged at any time.</li> <li>The generated environment will leave data in the warehouse, which also needs to be properly managed.</li> </ol> <p>This article will not focus on how to use Recce but rather on how to effectively prepare environments for Recce use.</p>"},{"location":"docs/guides/best-practices-prep-env/#best-practices","title":"Best Practices","text":""},{"location":"docs/guides/best-practices-prep-env/#use-schema-to-manage-your-environments","title":"Use schema to manage your environments","text":"<p>In dbt, you can leverage profiles and targets to specify the credentials for your database connections. By using profiles and dynamically setting the schema, you can direct the transformation results to different schemas, effectively creating separate environments. Here's how you can achieve this using <code>env_var</code> or <code>arg</code> to dynamically change the schema:</p> <ol> <li> <p>Define Your Profile and Target: Your <code>profiles.yml</code> should have different targets that you can switch between. Here\u2019s an example of a <code>profiles.yml</code> file:</p> <pre><code>my_profile:\n  target: dev\n  outputs:\n    dev:\n      type: postgres\n      host: localhost\n      user: db_user\n      password: db_pass\n      port: 5432\n      dbname: my_db\n      schema: \"{{ env_var('DBT_SCHEMA') }}\"\n    prod:\n      type: postgres\n      host: prod_host\n      user: prod_user\n      password: prod_pass\n      port: 5432\n      dbname: prod_db\n      schema: public\n</code></pre> </li> <li> <p>Run dbt with the Specified Schema: Now, when you run dbt commands, the <code>schema</code> setting will dynamically use the value of the <code>DBT_SCHEMA</code> environment variable.</p> <pre><code>DBT_SCHEMA=pr_env dbt run\n</code></pre> </li> <li> <p>Using <code>args</code> in dbt: You can also pass arguments directly in your dbt commands to dynamically set variables. For example:</p> <pre><code>dbt run --vars '{\"schema_name\": \"pr_123\"}'\n</code></pre> <p>And modify your <code>profiles.yml</code> to use this variable:</p> <pre><code>my_profile:\n  target: dev\n  outputs:\n    dev:\n      type: postgres\n      host: localhost\n      user: db_user\n      password: db_pass\n      port: 5432\n      dbname: my_db\n      schema: \"{{ var('schema_name') }}\"\n</code></pre> </li> </ol> <p>This approach allows you to dynamically create different environments by changing the schema on-the-fly. This is particularly useful for creating isolated environments for different PRs or testing scenarios, ensuring that your transformations are scoped to the correct schema and avoiding conflicts between different environments.</p>"},{"location":"docs/guides/best-practices-prep-env/#prepare-single-base-environment-for-all-prs-to-compare","title":"Prepare single base environment for all PRs to compare","text":"<p>Using the production environment as the base environment is a straightforward choice. However, to make Recce more efficient, using the staging environment might be more suitable.</p> <p>This staging environment can have the following characteristics:</p> <ol> <li>Ensure that the transformed results reflect the latest commit of the base branch.</li> <li>Use the same source data as the PR environment.</li> <li>Use the same transformation logic as the PR environment.</li> </ol> <p>The basic principle is that the staging environment's configuration should be as close as possible to the PR environments, except for using a different git commit.</p>"},{"location":"docs/guides/best-practices-prep-env/#prepare-per-pr-environment","title":"Prepare per-PR environment","text":"<p>A moderately sized data project may have multiple branches in development simultaneously. To avoid interference, it is recommended that each PR have its own isolated PR environment. The schema name can be <code>pr_&lt;number&gt;</code></p>"},{"location":"docs/guides/best-practices-prep-env/#reduce-the-update-frequency-of-the-source-data-used-by-both-the-base-and-pr-environments","title":"Reduce the update frequency of the source data used by both the base and PR environments","text":"<p>Some data projects may have source data that updates every hour or even every second. This can result in different transformation outcomes due to varying source data at different times, leading to Recce comparison results lacking discernibility.</p> <p>Currently, some data warehouses support zero-copy clone (snowflake, bigquery, databricks), which allows us to freeze the source data at a specific point in time. Considering updating the source data weekly can significantly reduce the variability in environments caused by source data changes.</p> <p></p>"},{"location":"docs/guides/best-practices-prep-env/#limit-the-data-range-used-in-transformations","title":"Limit the data range used in transformations","text":"<p>Most data is temporal. When preparing the base and PR environments, we can use only the data from the last month. This can greatly reduce the data volume while still verifying correctness.</p> <p>If zero-copy clone for the source data is not supported and the source data continues to update, you can consider excluding the current week's data. This approach can ensure that transformations yield consistent results regardless of when they are executed.</p> <p>For example, you can design the transformation to only use data from the last month, up to Sunday at 00:00. This approach combines the benefits of shorter execution times and reduced data volatility.</p> <p></p> <pre><code>SELECT\n    *\nFROM\n    {{ source('your_source_name', 'orders') }}\n{% if target.name != 'prod' %}\nWHERE\n    order_date &gt;= DATEADD(month, -1, CURRENT_DATE)\n    AND order_date &lt; DATE_TRUNC('week', CURRENT_DATE)\n{% endif %}\n</code></pre>"},{"location":"docs/guides/best-practices-prep-env/#ensure-that-the-base-environment-is-always-up-to-date","title":"Ensure that the base environment is always up-to-date","text":"<p>There are two scenarios that may cause the base environment to be out of date:</p> <ol> <li>New Source Data Changes: If you update your data weekly, ensure that your base environment is updated at least once a week as well.</li> <li>New PRs Merged into base branch: You can trigger a base environment update on merge events to ensure it remains current.</li> </ol>"},{"location":"docs/guides/best-practices-prep-env/#ensure-the-pr-branch-is-in-sync-with-the-base-branch","title":"Ensure the PR branch is in sync with the base branch","text":"<p>If the PR is executed after the base branch has been updated, the comparison with the base environment will mix the changes from the PR with the changes from other PRs merged into the base branch. This results in comparison outcomes that do not accurately reflect the impact of the current PR.</p> <p></p> <p>GitHub can automatically detect whether a PR is in sync. You need to enable this feature in the repository settings. Once enabled, you will see whether the PR is up-to-date directly on the PR page.</p> <p></p> <p>You can also to check if the PR is up-to-date in the CI workflow before preparing the PR environment. Here is an example in github action <pre><code>- name: Check if PR is up-to-date\n  if: github.event_name == 'pull_request'\n  run: |\n    git fetch origin main\n    UPSTREAM=${GITHUB_BASE_REF:-'main'}\n    HEAD=${GITHUB_HEAD_REF:-${GITHUB_REF#refs/heads/}}\n    if [ \"$(git rev-list --left-only --count ${HEAD}...origin/${UPSTREAM})\" -eq 0 ]; then\n      echo \"Branch is up-to-date\"\n    else\n      echo \"Branch is not up-to-date\"\n      exit 1\n    fi\n</code></pre></p>"},{"location":"docs/guides/best-practices-prep-env/#consider-how-to-obtain-your-artifacts-for-environments","title":"Consider how to obtain your artifacts for environments","text":"<p>Recce relies on the base and current environment artifacts to find the corresponding tables in the data warehouse for comparison. So, the question is how to obtain the artifacts of the environments to be compared.</p> <p>Here are a few methods you can choose:</p> <ol> <li>In CI, upload the generated artifact to the cloud storage (e.g., AWS S3).</li> <li>For dbt Cloud users, you can download artifacts for the latest run of a given job.</li> <li>For GitHub Actions users, you can use the GitHub CLI (gh) to download artifacts for the latest run of a given workflow.</li> </ol> <p>If the methods mentioned above are too complex, a stateless approach is to directly check out the base branch and run <code>dbt docs generate</code> to generate the artifacts.</p>"},{"location":"docs/guides/best-practices-prep-env/#cleaning-up-pr-environments-on-pr-closed","title":"Cleaning up PR environments on PR closed","text":"<p>As the number of PRs in a project increases, automatically generated environments also grow. To manage this, you can create a workflow that listens for PR close events and performs cleanup actions. Additionally, you can schedule periodic cleanups to remove outdated environments, such as those not used for a week.</p> <p>In dbt, you can use the <code>dbt run-operation</code> command to clear a specific schema corresponding to an environment. This can be especially useful for environments named in a pattern such as <code>pr_{env}</code>.</p> <p>Here\u2019s how you can define a macro to clear an environment schema:</p> <pre><code>{% macro clear_schema(schema_name) %}\n{% set drop_schema_command = \"DROP SCHEMA IF EXISTS \" ~ schema_name ~ \" CASCADE;\" %}\n{% do run_query(drop_schema_command) %}\n{% endmacro %}\n</code></pre> <p>Run the macro</p> <pre><code>dbt run-operation clear_schema --args \"{'schema_name': 'pr_123'}\"\n</code></pre>"},{"location":"docs/guides/best-practices-prep-env/#example","title":"Example","text":"Environments Schema Name When to run # of environments Data range Production <code>public</code> Daily 1 All Staging <code>staging</code> Daily + On Merge 1 1 month, excluding this week PR <code>pr_&lt;number&gt;</code> On Push # of opened PR 1 month, excluding this week <ul> <li>Automate environment generation using GitHub Actions.</li> <li>PR Environment will only be generated automatically when the PR is up-to-date.</li> <li>Artifacts will be stored under the workflow\u2019s artifacts.</li> <li>PR environments are removed on PR closed.</li> <li>Use staging environment as the base environment for Recce.</li> </ul>"},{"location":"docs/guides/scenario-ci/","title":"Recce CI integration with GitHub Action","text":"<p>Recce provides the <code>recce run</code> command for CI/CD pipeline. You can integrate Recce with GitHub Actions (or other CI tools) to compare the data models between two environments when a new pull-request is created. The below image describes the basic architecture.</p> <p></p> <p>The following guide demonstrates how to configure Recce in GitHub Actions.</p>"},{"location":"docs/guides/scenario-ci/#prerequisites","title":"Prerequisites","text":"<p>Before integrating Recce with GitHub Actions, you will need to configure the following items:</p> <ul> <li> <p>Set up two environments in your data warehouse. For example, one for base and another for pull request.</p> </li> <li> <p>Provide the credentials profile for both environments in your <code>profiles.yml</code> so that Recce can access your data warehouse. You can put the credentials in a <code>profiles.yml</code> file, or use environment variables.</p> </li> <li> <p>Set up the data warehouse credentials in your GitHub repository secrets.</p> </li> </ul>"},{"location":"docs/guides/scenario-ci/#set-up-recce-with-github-actions","title":"Set up Recce with GitHub Actions","text":"<p>We suggest setting up two GitHub Actions workflows in your GitHub repository. One for the base environment and another for the PR environment.</p> <ul> <li> <p>Base environment workflow: Triggered on every merge to the <code>main branch</code>. This ensures that base artifacts are readily available for use when a PR is opened.</p> </li> <li> <p>PR environment workflow: Triggered on every push to the <code>pull-request branch</code>. This workflow will compare base models with the current PR environment.</p> </li> </ul>"},{"location":"docs/guides/scenario-ci/#base-workflow-main-branch","title":"Base Workflow (Main Branch)","text":"<p>This workflow will perform the following actions:</p> <ol> <li>Run dbt on the base environment.</li> <li>Upload the generated DBT artifacts to github workflow artifacts for later use.</li> </ol> <pre><code>name: Recce CI Base Branch\n\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - main\n\nconcurrency:\n  group: recce-ci-base\n  cancel-in-progress: true\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v2\n        with:\n          python-version: \"3.10.x\"\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n\n      - name: Run DBT\n        run: |\n          dbt deps\n          dbt seed --target ${{ env.DBT_BASE_TARGET }}\n          dbt run --target ${{ env.DBT_BASE_TARGET }}\n          dbt docs generate --target ${{ env.DBT_BASE_TARGET }}\n        env:\n          DBT_BASE_TARGET: \"prod\"\n\n      - name: Upload DBT Artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: target\n          path: target/\n</code></pre> <p>Note</p> <p>Please place the above file in <code>.github/workflows/dbt_base.yml</code>. This workflow path will also be used in the next PR workflow. If you place it in a different location, please remember to make the corresponding changes in the next step.</p>"},{"location":"docs/guides/scenario-ci/#pr-workflow-pull-request-branch","title":"PR Workflow (Pull Request Branch)","text":"<p>This workflow will perform the following actions:</p> <ol> <li>Run dbt on the PR environment.</li> <li>Download previously generated base artifacts from base workflow.</li> <li>Use Recce to compare the PR environment with the downloaded base artifacts.</li> <li>Use Recce to generate the summary of the current changes and post it as a comment on the pull request. Please refer to the Recce Summary for more information.</li> </ol> <pre><code>name: Recce CI PR Branch\n\non:\n  pull_request:\n    branches: [main]\n\njobs:\n  check-pull-request:\n    name: Check pull request by Recce CI\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - name: Merge Base Branch into PR\n        uses: DataRecce/PR-Update@v1\n        with:\n          baseBranch: ${{ github.event.pull_request.base.ref }}\n          autoMerge: false\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.10.x\"\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install recce\n      - name: Prepare dbt Base environment\n        run: |\n          gh repo set-default ${{ github.repository }}\n          base_branch=${{ github.base_ref }}\n          run_id=$(gh run list --workflow ${WORKFLOW_BASE} --branch ${base_branch} --status success --limit 1 --json databaseId --jq '.[0].databaseId')\n          echo \"Download artifacts from run $run_id\"\n          gh run download ${run_id} -n target -D target-base\n        env:\n          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          WORKFLOW_BASE: \".github/workflows/dbt_base.yml\"\n      - name: Prepare dbt Current environment\n        run: |\n          git checkout ${{ github.event.pull_request.head.sha }}\n          dbt deps\n          dbt seed --target ${{ env.DBT_CURRENT_TARGET}}\n          dbt run --target ${{ env.DBT_CURRENT_TARGET}}\n          dbt docs generate --target ${{ env.DBT_CURRENT_TARGET}}\n        env:\n          DBT_CURRENT_TARGET: \"dev\"\n\n      - name: Run Recce CI\n        run: |\n          recce run --github-pull-request-url ${{ github.event.pull_request.html_url }}\n\n      - name: Upload DBT Artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: target\n          path: target/\n\n      - name: Upload Recce State File\n        uses: actions/upload-artifact@v4\n        id: recce-artifact-uploader\n        with:\n          name: recce-state-file\n          path: recce_state.json\n\n      - name: Prepare Recce Summary\n        id: recce-summary\n        run: |\n          recce summary recce_state.json &gt; recce_summary.md\n          cat recce_summary.md &gt;&gt; $GITHUB_STEP_SUMMARY\n          echo '${{ env.NEXT_STEP_MESSAGE }}' &gt;&gt; recce_summary.md\n\n          # Handle the case when the recce summary is too long to be displayed in the GitHub PR comment\n          if [[ `wc -c recce_summary.md | awk '{print $1}'` -ge '65535' ]]; then\n            echo '# Recce Summary\n          The recce summary is too long to be displayed in the GitHub PR comment.\n          Please check the summary detail in the [Job Summary](${{github.server_url}}/${{github.repository}}/actions/runs/${{github.run_id}}) page.\n          ${{ env.NEXT_STEP_MESSAGE }}' &gt; recce_summary.md\n          fi\n\n        env:\n          NEXT_STEP_MESSAGE: |\n            ## Next Steps\n            If you want to check more detail information about the recce result, please download the [artifact](${{ steps.recce-artifact-uploader.outputs.artifact-url }}) file and open it by [Recce](https://pypi.org/project/recce/) CLI.\n\n            ### How to check the recce result\n            ```bash\n            # Unzip the downloaded artifact file\n            tar -xf recce-state-file.zip\n\n            # Launch the recce server based on the state file\n            recce server --review recce_state.json\n\n            # Open the recce server http://localhost:8000 by your browser\n            ```\n\n      - name: Comment on pull request\n        uses: thollander/actions-comment-pull-request@v2\n        with:\n          filePath: recce_summary.md\n          comment_tag: recce\n</code></pre>"},{"location":"docs/guides/scenario-ci/#review-the-recce-state-file","title":"Review the Recce State File","text":"<p>Review the downloaded Recce state file with the following command:</p> <pre><code>recce server --review recce_state.json\n</code></pre> <p>In the Recce server <code>--review</code> mode, you can review the comparison results of the data models between the base and current environments. It will contain the row counts of modified data models, and the results of any Recce Preset Checks.</p>"},{"location":"docs/guides/scenario-dev/","title":"Development","text":"<p>In developing a project with dbt, there are numerous methods available to help you query warehouse data for validation. Recce, in particular, allows for further comparison with production or a specific baseline environment.</p>"},{"location":"docs/guides/scenario-dev/#prepare-the-environments","title":"Prepare the environments","text":"<p>In order to enable Recce to compare the base and current environment, you need to prepare artifacts for both environments.</p> <p>For base environment, put the dbt artifacts in your <code>target-base/</code> path. You can have the following options</p> <ol> <li>Download the artifacts from remote storage: If you use dbt cloud, you can download the latest artifacts in your production environment. For non dbt cloud case, you can upload the latest artifacts to cloud storage (e.g. s3), and write a scripts to download artifacts.</li> <li>Generate the artifacts for the production environment: <pre><code>dbt docs generate --target prod --target-path target-base/\n</code></pre></li> </ol> <p>For current developing environment, for most of the dbt command, it would generate the <code>manifest.json</code>. If you want to update the schema information, you have to run the <code>dbt docs generate</code> to generate the <code>catalog.json</code>.</p> <p>Recce also watch the the <code>target/</code> and <code>target-base/</code> folders. If there is artifact file changed, the recce web ui would reload to the latest version.</p>"},{"location":"docs/guides/scenario-dev/#development-cycle","title":"Development Cycle","text":"<p>The common development cycle is</p> <ol> <li>Write the code, validate the change, commit your code</li> <li>Push the commits to remote</li> <li>Review the impacts of your changes</li> </ol> <p>Here, I assume your pull request hasn't been marked as \"ready for review\" yet, and you're still in the process of development, verifying correctness on your own. In this scenario, Recce can assist you in conducting this validation.</p>"},{"location":"docs/guides/scenario-dev/#check-the-lineage","title":"Check the Lineage","text":"<p>DBT provides a method to identify modified models using <code>dbt ls -s state:modified+,</code> but this is obviously not usable within dbt docs. While you can determine how many models are affected using this command, you can't visualize these results.</p> <p>In Recce, you can conduct an initial assessment of your impact scope by Lineage diff, which may help you identify potential unintended impacts.</p>"},{"location":"docs/guides/scenario-dev/#validate-the-models-metadata","title":"Validate the Models' Metadata","text":"<p>With lineage diff, you can start from the modified models to confirm your impact. An inexpensive method is to examine the impact scope of the affected models' metadata.</p> <p>Firstly, you can start by examining Schema diff to see if any changes are detected in each model's schema. Sometimes, a change from Integer to Text, or from Decimal to Numeric, may have subtle impacts on your downstream models.</p> <p>Additionally, whether the models with schema changes have only added columns is worth noting, as this might not significantly affect your downstream processes. However, if columns are removed, it's essential to pay special attention to ensure it's the expected outcome.</p> <p>Next, you can examine the Row count diff for the affected models. Typically, row counts are stored in the warehouse's metadata, meaning you can obtain row count information without much cost. This allows you to quickly determine if row counts are the same or if there are significant changes. Common issues may arise from an erroneous join resulting in unexpected data volumes and erroneous outcomes. Row count diff provides a fast method to identify similar errors.</p> <p>Observing the summary of each node can help you quickly review schema and row count changes. By using the lineage diff graph, conducting basic checks on schema and row count, you can already gain a basic level of confidence in the changes made during your development process.</p> <p></p>"},{"location":"docs/guides/scenario-dev/#validate-the-columns-summary","title":"Validate the Column's Summary","text":"<p>Apparently, model metadata alone is insufficient. Sometimes, we need to assess the magnitude of impact that the changes currently in development have on the critical Marts models.</p> <p>Recce provides 4 powerful diff tools to compare the data level changes.</p> <ol> <li>Value Diff: You can use value diff to observe the matched percentage for each column.</li> <li>Profile Diff: You can use profile diff to compare basic statistical values for each column, such as count, distinct count, min, max, and average.</li> <li>Histogram Diff: You can use histogram diff to examine the distribution changes of numeric columns.</li> <li>Top-K Diff: You can use top-k diff to analyze distribution changes of categorical columns.</li> </ol> <p>It's important to note that these queries may take longer to execute and require reading larger amounts of data. Please choose the appropriate method based on the data volume of each model.</p>"},{"location":"docs/guides/scenario-dev/#validate-by-adhoc-query","title":"Validate by Adhoc Query","text":"<p>If you want to choose the most flexible method, Query diff is the way to go. You can compare individual records, perform complex operations like where, group by, order by. Or even query multiple models with joins.</p> <p>AdHoc queries also support the use of dbt macros, providing the highest level of flexibility for validation. However, the downside is that you'll need to write the queries yourself.</p>"},{"location":"docs/guides/scenario-dev/#check-driven-development","title":"Check Driven Development","text":"<p>Test Driven Development (TDD) is a common development pattern where you write tests first, then begin development, validating until the tests pass.</p> <p>When developing in dbt, of course, you can implement the TDD process through dbt tests. However, writing tests is not the only method. First, tests require very precise validation logic, and second, sometimes we don't want to impact the original data definitions. In such cases, what we want to verify is that the data doesn't change too much, rather than a specific logic.</p> <p>For example, if we want to make slight adjustments to the definition of \"revenue\". In the concept of TDD, we would consider what the input data is and what the output should be. But more often, what we want to verify is just ensuring that the changes in revenue for each month are within an expected range.</p> <p>In recce, we can a simple query for your validation code</p> <pre><code>SELECT\n    date_trunc('month', order_date) AS month,\n    SUM(amount) AS revenue\nFROM\n    orders\nGROUP BY\n    month\nORDER BY\n    month desc\n</code></pre> <p>Next, you can add this check to your checklist. After modifying your code each time, rerun this check until it meets your requirements.</p>"},{"location":"docs/guides/scenario-dev/#save-your-state","title":"Save Your State","text":"<p>Switching branches is often unavoidable during development. To preserve the current state for future use, save or export the state file. To resume the state, start the Recce server with the state file as an argument:</p> <pre><code>recce server recce_issue_123.json\n</code></pre> <p></p>"},{"location":"docs/guides/scenario-dev/#import-checklist","title":"Import Checklist","text":"<p>You can import a checklist from a state file by following these steps:</p> <ol> <li>Go to the Checklist page.</li> <li>Click the Import icon at the top of the checklist.</li> <li>Select the state file you want to import.</li> </ol> <p>This is particularly useful for preserving your favorite checks across different branches.</p>"},{"location":"docs/guides/scenario-pr-review/","title":"PR Review","text":"<p>When you've finished developing your feature, we should turn the pull request (PR) into ready for review state. At this point, the PR submitter needs to provide the necessary information for the PR reviewer. Some projects may offer a PR template. In the dbt official blog, this article provides an excellent example.</p> <p>Recce at this stage aims to assist the submitter in gathering more information to ensure that the reviewer can merge the pull request (PR) with greater confidence.</p>"},{"location":"docs/guides/scenario-pr-review/#screenshots","title":"Screenshots","text":""},{"location":"docs/guides/scenario-pr-review/#lineage","title":"Lineage","text":"<p>Firstly, the Lineage DAG is crucial information within a dbt project as it helps us understand the dependencies between models. In dbt docs, it provides lineage diagrams. Usually, we can paste this diagram into the PR comment to help the reviewer understand the latest lineage status.</p> <p></p> <p>However, during PR reviews, we may be more interested in understanding what changes have been made and presenting them through the Lineage DAG. At this point, you can utilize recce to capture a screenshot of the Lineage diff and embed it within your PR comment.</p> <p></p>"},{"location":"docs/guides/scenario-pr-review/#checks","title":"Checks","text":"<p>Another core feature of Recce is its various checks, which allow us to compare key models with the base environment. The typical workflow is as follows:</p> <ol> <li>Generate the various diffs you need.</li> <li>Identify the query and its result that you want to present to the reviewer.</li> <li>Add it to the checklist.</li> <li>Click the Copy to Clipboard button and paste it in the corresponding position within the PR comment.</li> <li>Write a description of your check, including explanations and intentions.</li> <li>Click the Copy markdown button     </li> <li>paste it in the corresponding position within the PR comment.     </li> </ol>"},{"location":"docs/guides/scenario-pr-review/#share-the-recce-file","title":"Share the Recce File","text":"<p>If you want the reviewer to access your environment, you can also attach the Recce state file to the PR comment.</p> <p>As a Submitter</p> <ol> <li>Save or export the recce state file</li> <li>Attach the state file into the PR comment</li> </ol> <p>As a Reviewer</p> <ol> <li>Download the state file</li> <li>In your dbt project folder, run this command    <pre><code>recce server --review &lt;recce state file&gt;\n</code></pre></li> </ol> <p>By adding the <code>--review</code> option, the Recce server will use the DBT artifacts from the state file to connect to both the base and the pull request (PR) environments.</p> <p>Note</p> <p>Although the artifacts are from the Recce state, you still need to provide the <code>profiles.yml</code> and <code>dbt_project.yml</code> files so that Recce knows which credentials to use to connect to the data warehouse.</p>"},{"location":"docs/recce-cloud/","title":"Overview","text":"<p>Note</p> <p>Recce Cloud is currently in private alpha and scheduled for general availability later this year.  Sign up to the Recce newsletter to be notified, or email product@datarecce.io to join our design partnership program for early access.</p>"},{"location":"docs/recce-cloud/#what-is-recce-cloud","title":"What is <code>Recce Cloud</code>?","text":"<p>Recce Cloud is a service specifically designed for streamlining the DBT PR Review workflow. </p> <p>Recce Cloud primarily operates through Recce and integrates GitHub Pull Requests, consolidating the review status of PRs within the Cloud. Without Recce Cloud, we use the state file to store PR review states. However, this method is not very suitable for collaboration or integration with CI because our review states are not stored in a fixed location. Recce Cloud is designed to solve this problem.</p>"},{"location":"docs/recce-cloud/#prerequisite","title":"Prerequisite","text":"<ol> <li>Recce Cloud requires Recce. Please make sure that you have understood how use Recce in your dbt project.</li> <li>Prepare the github personal access token with the <code>repo</code> permission. Please see the GitHub document. And set it to your environment variable.    <pre><code>export GITHUB_TOKEN=&lt;token&gt;\n</code></pre>     Or you can set the <code>--cloud-token &lt;GITHUB_TOKEN&gt;</code> command option.</li> <li>Prepare the Recce state password. The Recce state password is used to encrypt/decrypt the state file before uploading/downloading. The password is not stored in Recce Cloud, so you need to keep it safe.    <pre><code>export RECCE_STATE_PASSWORD=&lt;password&gt;\n</code></pre>    Or you can set the <code>--password &lt;password&gt;</code> or <code>-p &lt;password&gt;</code> command option.</li> </ol>"},{"location":"docs/recce-cloud/#getting-started","title":"Getting Started","text":"<p>The following instructions give an overview of the process of using Recce in your dbt project. For a hands-on tutorial, please check the Jaffle Shop Tutorial for Cloud.</p>"},{"location":"docs/recce-cloud/#sign-up-the-recce-cloud","title":"Sign Up the Recce Cloud","text":"<ol> <li>Go to the recce cloud</li> <li>Sign in by github account</li> <li>Click Install button to install Recce Cloud github app to your personal or organization account.</li> <li>Authorize the repositories to the github app.</li> </ol>"},{"location":"docs/recce-cloud/#launch-the-recce-server-in-the-cloud-mode","title":"Launch the <code>recce server</code> in the cloud mode","text":"<ol> <li>Create a branch for developing.    <pre><code>git checkout -b &lt;my-awesome-feature&gt;\n</code></pre></li> <li>Develop your features and prepare the dbt artifacts for the base (<code>target-base/</code>) and current (<code>target/</code>) environments.</li> <li>Create a pull request for this branch. Recce Cloud requires an open pull request in your GitHub repository. It also stores the latest Recce state for each pull request.</li> <li>Launch the Recce instance in the cloud mode. It will use the dbt artifacts in the local <code>target</code> and <code>target-base</code> and initiate a new review state if necessary.    <pre><code>recce server --cloud\n</code></pre></li> </ol> <p>Note</p> <p>Here we assume the you have set the <code>GITHUB_TOKEN</code> and <code>RECCE_STATE_PASSWORD</code> in your environment variables.</p>"},{"location":"docs/recce-cloud/#review-in-the-cloud-mode","title":"Review in the cloud mode","text":"<p>If the review state is already available for this PR, you can open the Recce instance to review.</p> <ol> <li>Checkout the branch for the reviewed PR.</li> <li>Launch the Recce instance to review this PR     <pre><code>recce server --review --cloud\n</code></pre></li> </ol>"},{"location":"docs/recce-cloud/#usage","title":"Usage","text":"<p>All the commands requires the following settings.</p> Name Environment Variables CLI Options Description Cloud token <code>GITHUB_TOKEN</code> <code>--cloud-token</code> Used for  1. Get the pull request from github2. Used as the access token to the recce cloud State password <code>RECCE_STATE_PASSWORD</code> <code>--password</code> Used to encrypt/decrypt the state in the recce cloud <p>Recce Cloud is used for pull request reviews. Before interacting with the cloud state, you should switch to a branch that has an open PR on the remote.</p>"},{"location":"docs/recce-cloud/#recce-server","title":"Recce server","text":"<p>Initiate the review session of the PR. It would use the local dbt artifacts in the <code>target/</code> and <code>target-base/</code> directories to sync the state with the cloud.</p> <pre><code>git checkout &lt;pr-branch&gt;\nrecce sever --cloud\n</code></pre>"},{"location":"docs/recce-cloud/#recce-server-review-mode","title":"Recce server (Review mode)","text":"<p>Review a PR with the remote state.</p> <pre><code>git checkout &lt;pr-branch&gt;\nrecce sever --cloud --review\n</code></pre>"},{"location":"docs/recce-cloud/#recce-run","title":"Recce run","text":"<p>Run or rerun the PR's checks and sync the state with cloud.</p> <pre><code>git checkout &lt;pr-branch&gt;\nrecce run --cloud\n</code></pre>"},{"location":"docs/recce-cloud/#recce-summary","title":"Recce summary","text":"<p>Generate the summary markdown</p> <pre><code>git checkout &lt;pr-check&gt;\nrecce summary --cloud &gt; summary.md\n</code></pre>"},{"location":"docs/recce-cloud/#recce-cloud","title":"Recce cloud","text":"<p>The cloud subcommand in recce provides functionality for managing state files in cloud storage.</p>"},{"location":"docs/recce-cloud/#purge","title":"purge","text":"<p>You can purge the state from your current PR. It is useful when</p> <ol> <li>You forgot the password</li> <li>You would like to reset the state of this PR.</li> </ol> <pre><code>git checkout &lt;pr-branch&gt;\nrecce cloud purge\n</code></pre>"},{"location":"docs/recce-cloud/#upload","title":"upload","text":"<p>If you already have the state file for the PR, you can upload it to the cloud.</p> <pre><code>git checkout &lt;pr-branch&gt;\nrecce cloud upload &lt;recce-state-file&gt;\n</code></pre>"},{"location":"docs/recce-cloud/#download","title":"download","text":"<p>You can download the recce state file of the PR from cloud as well.</p> <pre><code>git checkout &lt;pr-branch&gt;\nrecce cloud download\n</code></pre>"},{"location":"docs/recce-cloud/#github-pull-request-status-check","title":"GitHub Pull Request Status Check","text":"<p>Recce Cloud integrate with the GitHub Pull Request Status Check. If there is recce review state synced to a PR, the PR would has a recce cloud check status. Once all checks in recce are approved, the check status would change to passed and ready to be merged.</p> <p></p>"},{"location":"docs/recce-cloud/expose-recce-instance-visibility/","title":"Share Recce Instance Access","text":"<p>Note</p> <p>Recce Cloud is currently in private alpha and scheduled for general availability later this year.  Sign up to the Recce newsletter to be notified, or email product@datarecce.io to join our design partnership program for early access.</p> <p>As a Recce Cloud user, you can launch a Recce Instance in Cloud Mode or use GitHub Codespaces. However, both of these methods require a GitHub Access Token, which restricts the usage of Recce to those with GitHub accounts.</p> <p>For situations in which you would like to share your Recce Instance with non-GitHub users, such as stakeholders or other teams, we currently recommend the use of one of the following third-party utilities:</p> <ul> <li>Ngrok </li> <li>Tailscale</li> </ul> <p>These services provide an endpoint for your Recce Instance, with optional authentication, that will enable other users to access Recce.</p> <p>These approaches serve as a workaround to expose a Recce instance to non-GitHub users. We are currently working on official support for enabling this feature without the need for third-party tools.</p>"},{"location":"docs/recce-cloud/expose-recce-instance-visibility/#provide-external-access-to-a-recce-instance","title":"Provide external access to a Recce Instance","text":"<p>Note</p> <p>Using these tools requires registering additional accounts, and you may need to subscribe to a paid plan to accommodate your usage and data transfer volume. For details, please refer to the Pricing Plans of ngrok and tailscale.</p>"},{"location":"docs/recce-cloud/expose-recce-instance-visibility/#ngrok","title":"ngrok","text":"<p>After installing the Ngrok client, you can create an endpoint for the Recce Instance that will allow other users to participate in the dbt PR review process, without any additional tools or setup.</p> <ol> <li>Setup the ngrok agent</li> </ol> <p>ngrok supports multiple platforms, including macOS, Linux, and Windows. Please refer to the official installation guide for details.</p> <ol> <li> <p>Connect your ngrok agent to your ngrok account.     <pre><code>ngrok config add-authtoken &lt;TOKEN&gt;\n</code></pre></p> </li> <li> <p>Put the Recce Instance online     <pre><code>ngrok http &lt;recce-instance-port&gt;\n</code></pre></p> <p></p> </li> <li> <p>Secure access with authentication</p> <p>ngrok provides a range of authentication options, from basic methods to integration with multiple OAuth providers. <pre><code># basic auth\nngrok http &lt;recce-instance-port&gt; --basic-auth 'username:password'\n\n# OAuth\nngrok http &lt;recce-instance-port&gt; --oauth=google --oauth-allow-email=user@example.com\n</code></pre></p> <p>For the full usage of settings and options, please refer to the ngrok http docs for details.</p> </li> </ol>"},{"location":"docs/recce-cloud/expose-recce-instance-visibility/#tailscale","title":"Tailscale","text":"<p>Tailscale enables you to create your own private network (called a 'Tailnet') and invite members to join it. Once set up, you can easily expose your Recce instance, making it accessible to all devices within the Tailnet.</p> <ol> <li> <p>Setup tailscale</p> <p>To create a Tailnet, first create an account, then download Tailscale and follow the official guide to continue set-up. Once configured, you can then invite other members to join.</p> <p></p> <p>It also supports integration with GitHub Codespaces.</p> </li> <li> <p>Connect your device to your account     <pre><code>tailscale up --authkey &lt;AUTH_KEY&gt;\n</code></pre></p> </li> <li> <p>Put the Recce Instance online</p> <p>The devices within your Tailnet can access the Recce Instance now.</p> <pre><code>tailscale serve &lt;recce-instance-port&gt;\n</code></pre> <p></p> <p>If you need a more fine-grained access control policy, please refer to the Tailscale docs.</p> </li> </ol>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/","title":"Demo Tutorial","text":"<p>Estimated Time: 20 minutes</p> <p>Note</p> <p>Recce Cloud is currently in private alpha and scheduled for general availability soon.  Sign up to the Recce newsletter to be notified, or email product@datarecce.io to join our design partnership program for early access.</p> <p>The following guide uses the official Jaffle Shop DuckDB project from dbt-labs, and provides everything you need to get started with Recce Cloud. By the end of the guide you'll be able to create and sync Recce checks with a GitHub PR via Recce Cloud.</p> <p>To see what you'll get, check out the first section from the following Loom:</p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#clone-jaffle-shop-to-your-own-private-repository","title":"Clone Jaffle Shop to your own private repository","text":"<ol> <li>Create a private repository in your GitHub account.</li> <li>Clone the Jaffle Shop DuckDB dbt data project:    <pre><code>git clone git@github.com:dbt-labs/jaffle_shop_duckdb.git\ncd jaffle_shop_duckdb\n</code></pre></li> <li>Change the remote url to the repository you just created:    <pre><code>git remote set-url origin git@github.com:&lt;owner&gt;/&lt;repo&gt;.git\n</code></pre></li> <li>Push to your newly created repository:    <pre><code>git push\n</code></pre></li> </ol>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#authorize-recce-cloud-to-access-the-repository","title":"Authorize Recce Cloud to access the repository","text":"<p>Recce Cloud needs access to your data project's repository in order to sync your checks status to the pull request thread.</p> <ol> <li>Visit Recce Cloud. If it is your first time logging in, click the Continue with Github button to authorize the Recce Cloud integration to access your GitHub account.     </li> <li>Click the Install button to install the Recce Cloud GitHub app to your personal or organization account.    </li> <li>On the app installation page, authorize Recce Cloud to access the repository you created in the previous section.    </li> <li>Authorized repositories will then be shown in your Recce Cloud account.    </li> </ol>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#configure-the-jaffle-shop-duckdb-data-project","title":"Configure the Jaffle Shop DuckDB data project","text":"<p>Set up the Jaffle Shop project and install Recce.</p> <ol> <li>Create a new Python virtual env:    <pre><code>python -m venv venv\nsource venv/bin/activate\n</code></pre></li> <li>Install the requirements and Recce:    <pre><code>pip install -r requirements.txt\npip install recce\n</code></pre></li> <li>Add a production environment to the project by editing <code>./profiles.yml</code> and adding the following target:    <pre><code>jaffle_shop:\n  target: dev\n  outputs:\n    dev:\n      type: duckdb\n      path: 'jaffle_shop.duckdb'\n      threads: 24\n+   prod:\n+     type: duckdb\n+     path: 'jaffle_shop.duckdb'\n+     schema: prod\n+     threads: 24\n</code></pre></li> <li>Add the following packages required by Recce for some features (highly recommended). Create a <code>./packages.yml</code> file in the root of your project with the following packages:    <pre><code>packages:\n- package: dbt-labs/audit_helper\n  version: 0.12.0\n- package: data-mie/dbt_profiler\n  version: 0.8.2\n</code></pre>    Install the packages:    <pre><code>dbt deps\n</code></pre></li> </ol>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#prepare-the-base-environment","title":"Prepare the base environment","text":"<p>Recce requires to two environments to compare. The <code>base</code> represents your point of reference (the known-good base), and <code>target</code> represents your PR/development branch.</p> <ol> <li>Prepare production (base) environment. (Note the custom <code>--target-path</code>):    <pre><code>dbt seed --target prod\ndbt run --target prod\ndbt docs generate --target prod --target-path ./target-base\n</code></pre></li> <li>Add the <code>target-base/</code> folder to the <code>.gitignore</code> file:    <pre><code> target/\n+target-base/\n dbt_packages/\n dbt_modules/\n logs/\n</code></pre></li> <li>Remove the existing GitHub action workflows:    <pre><code>rm -rf .github/\n</code></pre></li> <li>Push the changes to remote:    <pre><code>git add .\ngit commit -m 'Configure project and prep for Recce'\ngit push \n</code></pre></li> </ol> <p>Important</p> <p>By default, Recce expects the dbt artifacts for the base environment to be located in a folder named <code>target-base</code>. </p> <p>The base environment preparation is now complete. The data in the <code>prod</code> schema, and artifacts in the <code>target-base</code> folder, represent stable (production) data. </p> <p>As a PR author, you'll be working on data models, making changes to the project, and validating your work for correctness. </p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#prepare-the-review-state-for-the-pr","title":"Prepare the review state for the PR","text":"<p>In this section, you'll make a new branch, update a data model, and create a pull request.</p> <ol> <li> <p>Checkout a branch:    <pre><code>git checkout -b feature/recce-getting-started\n</code></pre></p> </li> <li> <p>Edit the staging model located in <code>./models/staging/stg_payments.sql</code> as follows:    <pre><code>...\n\nrenamed as (\n         payment_method,\n\n-        -- `amount` is currently stored in cents, so we convert it to dollars\n-        amount / 100 as amount\n+        amount\n\n         from source\n)\n</code></pre></p> </li> <li> <p>Run dbt on the development environment (the default target):    <pre><code>dbt seed\ndbt run\ndbt docs generate\n</code></pre></p> </li> <li> <p>Commit the change:    <pre><code>git add models/staging/stg_payments.sql\ngit commit -m 'Update the model'\ngit push -u origin feature/recce-getting-started \n</code></pre></p> </li> <li> <p>Create a pull request for this branch in your GitHub repository.</p> </li> </ol> <p>Important</p> <p>Don't forget to create a branch for the commit above, before continuing with this tutorial. </p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#launch-a-recce-instance-to-validate-your-change","title":"Launch a Recce Instance to validate your change","text":"<p>In this section, you will launch a Recce Instance, create validation checks, and sync those checks with Recce Cloud so they can be reviewed by your PR reviewer.</p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#prepare-a-github-token-and-recce-state-password","title":"Prepare a GitHub Token and Recce State password","text":"<p>To access the repository, your local Recce Instance will require a GitHub Token (Classic).</p> <ol> <li>Prepare a GitHub Token (Classic) in your account. Ensure you provide <code>repo</code> permission for the new token.    </li> <li>Ensure you have configured these environment variables.    <pre><code>export GITHUB_TOKEN=&lt;github-token&gt;\nexport RECCE_STATE_PASSWORD=mypassword\n</code></pre></li> </ol>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#run-recce-in-cloud-mode","title":"Run Recce in Cloud Mode","text":"<p>Run Recce instance in the cloud mode</p> <pre><code>recce server --cloud\n</code></pre> <p>Open the link to the Recce Instance in your browser. By default it should be http://0.0.0.0:8000</p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#create-a-recce-check","title":"Create a Recce Check","text":"<p>Switch to the Query tab and paste the following query:    <pre><code>select * from {{ ref(\"orders\") }} order by 1\n</code></pre>    Enter the primary key as <code>order_id</code> and click the <code>Run Diff</code> button.     1. Click the <code>Add to Checklist</code> button to add the query result to your Checklist 1. On the <code>Checklist</code> page you'll find that there are three checks. The Row count diff and Schema diff are default Preset Checks, and the Query diff is your newly added check. Leave the checks as unapproved. 1. Go back to the command line and terminate the Recce instance. Your Recce State file, containing your checklist and other artifacts will be encrypted and uploaded to Recce Cloud. 1. Go to the PR page in your GitHub repository and scroll to the bottom.  Notice that Recce Cloud shows that check are not approved:    </p> <p>Note</p> <p>Recce checks sync in realtime. However, due to the overhead of encrypting, compressing, and tranferring the State file, the sync may be slightly delayed. Ensure that you always terminate your Recce Instance on the CLI, and wait for the State to be synced. This will ensure your checks are saved to Recce Cloud.</p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#review-the-pr","title":"Review the PR","text":"<p>As a PR author, your job is to review and approve the Checks created by the PR author. Once approved, the PR can be merged. </p> <p>Note</p> <p>As this tutorial uses DuckDB, which is a file-based database, the reviewer needs to have the same DuckDB file to continue the reviewers journey.</p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#run-recce-is-review-mode","title":"Run Recce is <code>Review</code> mode","text":"<p>The PR reviewer should prepare their own GitHub token, but ensure to use the same password as the PR author. (The password is used to unencrypt the State file and so must be the same.)</p> <ol> <li>Checkout the PR branch    <pre><code>git checkout feature/recce-getting-started\n</code></pre></li> <li>Configured the required environment variables.    <pre><code>export GITHUB_TOKEN=&lt;github-token&gt;\nexport RECCE_STATE_PASSWORD=mypassword\n</code></pre></li> <li>Run Recce in <code>--cloud</code> and <code>--review</code> mode.    <pre><code>recce server --cloud --review\n</code></pre></li> </ol>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#approve-recce-checks","title":"Approve Recce Checks","text":"<p>When Recce loads, click the Checklist tab to review the Checks that have been prepared by the PR author.</p> <p>Approve all the checks if everything looks good to you</p> <p></p> <p>The approval status of the check is automatically synced to Recce Cloud.</p>"},{"location":"docs/recce-cloud/getting-started-recce-cloud/#merge-the-pr","title":"Merge the PR","text":"<p>Back on the GitHub PR page, you'll notice that the Recce Cloud check status has automatically been updated showing that \"All checks are approved\".</p> <p></p> <p>In a real-world situation you'd now be able to merge the PR with the confidence that the PR author had checked their work, and the reviewer both understands and has signed-off on any changes.</p>"},{"location":"docs/recce-cloud/setup-gh-actions/","title":"Continue Integration (CI)","text":"<p>Note</p> <p>Recce Cloud is currently in private alpha and scheduled for general availability later this year.  Sign up to the Recce newsletter to be notified, or email product@datarecce.io to join our design partnership program for early access.</p> <p>Continuous Integration(CI) and Continuous Delivery(CD) are best practices in software development. Through CI automation, a dbt project can systematically and continuously deliver and integrate high-quality results.</p> <p>To automate the process, we can use GitHub Actions and GitHub Codespaces to provide an automated and reusable workspace. The following diagram describes the entire ci/cd architecture.</p> <p></p> <p>We suggest setting up two GitHub Actions workflows in your GitHub repository. One for the base environment and another for the PR environment.</p> <ul> <li> <p>Base environment workflow: Triggered on every merge to the <code>main branch</code>. This ensures that base dbt artifacts are readily available for use.</p> </li> <li> <p>PR environment workflow: Triggered on every push to the <code>pull-request branch</code>. This workflow will compare base models with the current PR environment.</p> </li> </ul>"},{"location":"docs/recce-cloud/setup-gh-actions/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Per-PR Environment: To ensure that each PR has its own isolated environment, it is recommended to put <code>profile.yml</code> under source control in the repository and use environment variables to change the schema name. In the workflow, we can generate the corresponding schema name based on the PR number.</p> <pre><code>myprofile:\n  outputs:\n    pr:\n      type: snowflake\n      ...\n      schema: \"{{ env_var('DBT_SCHEMA') | as_text }}\"\n    prod:\n      type: snowflake\n      ...\n      schema: PUBLIC\n</code></pre> </li> <li> <p>GitHub Token and Recce State Password: As mentioned here, please ensure that the two secrets are available when running recce commands. You can add <code>GH_TOKEN</code> and <code>RECCE_STATE_PASSWORD</code> to the GitHub Actions Secrets. Then we can use them in the Github Actions workflow file.     <pre><code>env:\n  GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}\n  RECCE_STATE_PASSWORD: ${{ secrets.RECCE_STATE_PASSWORD }}\n</code></pre></p> </li> </ol> <p>Warning</p> <p>You cannot use the automatic generated token here, because we need the personal access token (PAT) to verify if the user has PUSH permission of the repository.</p> <pre><code>env:\n  GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Don't use 'secrets.GITHUB_TOKEN' here\n</code></pre>"},{"location":"docs/recce-cloud/setup-gh-actions/#set-up-recce-with-github-actions","title":"Set up Recce with GitHub Actions","text":""},{"location":"docs/recce-cloud/setup-gh-actions/#prepare-base-dbt-artifacts","title":"Prepare base dbt artifacts","text":"<p>For dbt Core users:</p> <ol> <li>Run dbt on the base environment.</li> <li>Use the GitHub Action workflow below.<ol> <li>Build base dbt artifacts</li> <li>Upload dbt artifacts to Recce Cloud</li> </ol> </li> </ol> <p>Upload the base dbt artifact to Recce Cloud by <code>recce cloud upload-artifacts</code>.  You and other developers on your team can then download by <code>recce cloud download-base-artifacts</code> without building base artifacts every time. </p> <p>Note</p> <p>Please place the above file in <code>.github/workflows/dbt_base.yml</code>. This workflow path will also be used in the next PR workflow. If you place it in a different location, please remember to make the corresponding changes in the next step.</p> <pre><code>name: Daily Job\n\non:\n  workflow_dispatch:\n  schedule:\n    - cron: \"0 0 * * *\"\n  push:\n    branches:\n      - main\n\nconcurrency:\n  group: recce-ci-base\n  cancel-in-progress: true\nenv:\n  # Credentials used by dbt profiles.yml\n  DBT_USER: ${{ secrets.DBT_USER }}\n  DBT_PASSWORD: ${{ secrets.DBT_PASSWORD }}\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10.x\"\n          cache: \"pip\"\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install recce\n\n      - name: Run DBT\n        run: |\n          dbt deps\n          dbt seed --target ${{ env.DBT_BASE_TARGET }}\n          dbt run --target ${{ env.DBT_BASE_TARGET }}\n          dbt docs generate --target ${{ env.DBT_BASE_TARGET }}\n        env:\n          DBT_BASE_TARGET: \"prod\"\n\n      - name: Upload DBT Artifacts\n        run: |\n          recce cloud upload-artifacts\n        env:\n          GITHUB_TOKEN: ${{ secrets.RECCE_CLOUD_TOKEN }}\n          RECCE_STATE_PASSWORD: ${{ secrets.RECCE_STATE_PASSWORD}}\n</code></pre> <p>For dbt Cloud users running CI/CD:</p> <ol> <li>For unscheduled production jobs: Use a GitHub Action workflow (coming soon) to trigger the job. The workflow will download dbt artifacts and upload them to Recce Cloud.</li> <li>For scheduled production jobs: Use a GitHub Action workflow (coming soon) to download dbt artifacts and upload them to Recce Cloud.<ul> <li>Note: The GitHub Action workflow is triggered after the dbt Cloud job, so there may be some latency.</li> </ul> </li> </ol>"},{"location":"docs/recce-cloud/setup-gh-actions/#pr-workflow-pull-request-branch","title":"PR Workflow (Pull Request Branch)","text":"<p>This workflow will perform the following actions:</p> <ol> <li>Run dbt on the PR environment.</li> <li>Download previously generated base artifacts from base workflow.</li> <li>Use Recce to compare the PR environment with the downloaded base artifacts.</li> <li>Use Recce to generate the summary of the current changes and post it as a comment on the pull request. Please refer to the Recce Summary for more information.</li> </ol> <pre><code>name: Recce CI PR Branch\n\non:\n  pull_request:\n    branches: [main]\n\nenv:\n  WORKFLOW_BASE: \".github/workflows/dbt_base.yml\"\n  # Credentials used by dbt profiles.yml\n  DBT_USER: ${{ secrets.DBT_USER }}\n  DBT_PASSWORD: ${{ secrets.DBT_PASSWORD }}\n  DBT_SCHEMA: \"PR_${{ github.event.pull_request.number }}\"\n  # Credentials used by recce\n  GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}\n  RECCE_STATE_PASSWORD: ${{ secrets.RECCE_STATE_PASSWORD }}\njobs:\n  check-pull-request:\n    name: Check pull request by Recce CI\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout repository\n        uses: actions/checkout@v4\n        with:\n          fetch-depth: 0\n\n      - name: Merge Base Branch into PR\n        uses: DataRecce/PR-Update@v1\n        with:\n          baseBranch: ${{ github.event.pull_request.base.ref }}\n          autoMerge: false\n\n      - name: Set up Python\n        uses: actions/setup-python@v5\n        with:\n          python-version: \"3.10.x\"\n          cache: pip\n\n      - name: Install dependencies\n        run: |\n          pip install -r requirements.txt\n          pip install recce\n\n      - name: Download artifacts for the base environment\n        run: |\n          recce cloud download-base-artifacts\n        env:\n          GITHUB_TOKEN: ${{ secrets.RECCE_CLOUD_TOKEN }}\n          RECCE_STATE_PASSWORD: ${{ secrets.RECCE_STATE_PASSWORD}}\n\n      - name: Prepare the PR environment\n        run: |\n          dbt deps\n          dbt seed --target ${{ env.DBT_CURRENT_TARGET}}\n          dbt run --target ${{ env.DBT_CURRENT_TARGET}}\n          dbt docs generate --target ${{ env.DBT_CURRENT_TARGET}}\n        env:\n          DBT_CURRENT_TARGET: \"pr\"\n\n      - name: Run Recce\n        run: |\n          recce run --cloud\n\n      - name: Upload DBT Artifacts\n        run: |\n          recce cloud upload-artifacts\n        env:\n          GITHUB_TOKEN: ${{ secrets.RECCE_CLOUD_TOKEN }}\n          RECCE_STATE_PASSWORD: ${{ secrets.RECCE_STATE_PASSWORD}}\n\n      - name: Prepare Recce Summary\n        id: recce-summary\n        run: |\n          recce summary --cloud &gt; recce_summary.md\n          cat recce_summary.md &gt;&gt; $GITHUB_STEP_SUMMARY\n          echo '${{ env.NEXT_STEP_MESSAGE }}' &gt;&gt; recce_summary.md\n\n          # Handle the case when the recce summary is too long to be displayed in the GitHub PR comment\n          if [[ `wc -c recce_summary.md | awk '{print $1}'` -ge '65535' ]]; then\n            echo '# Recce Summary\n          The recce summary is too long to be displayed in the GitHub PR comment.\n          Please check the summary detail in the [Job Summary](${{github.server_url}}/${{github.repository}}/actions/runs/${{github.run_id}}) page.\n          ${{ env.NEXT_STEP_MESSAGE }}' &gt; recce_summary.md\n          fi\n\n        env:\n          RECCE_STATE_PASSWORD: ${{ secrets.RECCE_STATE_PASSWORD }}\n          NEXT_STEP_MESSAGE: |\n            ## Next Steps          \n            If you want to check more detail information about the recce result, please follow this instruction.\n\n            ```bash\n            # Checkout to the PR branch\n            git checkout ${{ github.event.pull_request.head.ref }}\n\n            # Launch the recce server based on the state file\n            recce server --cloud --review\n\n            # Open the recce server http://localhost:8000 by your browser\n            ```\n      - name: Comment on pull request\n        uses: thollander/actions-comment-pull-request@v2\n        with:\n          filePath: recce_summary.md\n          comment_tag: recce\n</code></pre>"},{"location":"docs/recce-cloud/setup-gh-actions/#pr-workflow-with-dbt-cloud","title":"PR workflow with dbt Cloud","text":"<p>We can download the dbt artifacts from dbt Cloud for Recce if CI/CD on dbt Cloud is configured. The basic scenario is to download the latest artifacts of the deploy job for the base environment and the artifacts of the CI job for the current environment. We can archieve it via dbt Cloud API and we need:</p> <ol> <li>dbt Cloud Token</li> <li>dbt Cloud Account ID: Check out your \"Account settings\" on dbt Cloud</li> <li>dbt Cloud Deploy Job ID: Check out \"API trigger\" of the deploy job</li> <li>dbt Cloud CI Job ID: Check out \"API trigger\" of the CI job</li> </ol> <p></p> <p>We prepare a GitHub Action \"Recce dbt Cloud Action\" to do the following steps:</p> <ol> <li>Trigger the CI job on dbt Cloud</li> <li>Wait the CI job to finish</li> <li>Download the dbt artifacts from the deploy job to <code>./target-base</code> directory</li> <li>Download the dbt artifacts from the deploy job to <code>./target</code> directory</li> </ol> <p>Check out the GitHub Action to configure the GitHub workflow.</p> <p>Note</p> <p>Please ensure <code>Generate docs on run</code> is toggled in the \"Execution settings\" of deploy job and \"Advanced settings\" of CI job. </p>"},{"location":"docs/recce-cloud/setup-gh-actions/#review-the-recce-state-file","title":"Review the Recce State File","text":"<p>Review locally</p> <pre><code>git checkout &lt;pr-branch&gt;\nrecce server --cloud --review\n</code></pre> <p>Review in the GitHub codespace</p> <p>Please see GitHub Codespaces integration.</p>"},{"location":"docs/recce-cloud/setup-gh-codespaces/","title":"GitHub Codespaces","text":"<p>Note</p> <p>Recce Cloud is currently in private alpha and scheduled for general availability later this year.  Sign up to the Recce newsletter to be notified, or email product@datarecce.io to join our design partnership program for early access.</p> <p>GitHub Codespaces is a development environment provided by GitHub that allows developers to have identical and isolated environments for development. The GitHub Codespaces uses VS Code Server technology. We can launch it from a GitHub pull request page, and once it is started, the Recce instance will run and port forwarding will be set up. </p>"},{"location":"docs/recce-cloud/setup-gh-codespaces/#setup-recce-cloud-in-github-codespaces","title":"Setup Recce Cloud in GitHub Codespaces","text":"<ol> <li> <p>Prepare the two files in your repository     <pre><code>.devcontainer\n\u2514\u2500\u2500 recce\n    \u251c\u2500\u2500 Dockerfile\n    \u2514\u2500\u2500 devcontainer.json\n</code></pre></p> </li> <li> <p>Configure the <code>.devcontainer/recce/devcontainer.json</code> <pre><code>{\n    \"name\": \"Recce CodeSpace\",\n    \"build\": {\n        \"dockerfile\": \"Dockerfile\",\n    },\n    \"containerEnv\": {\n        \"RECCE_STATE_PASSWORD\": \"${{ secrets.RECCE_STATE_PASSWORD }}\",\n        \"DBT_USER\": \"${{ secrets.DBT_USER }}\",\n        \"DBT_PASSWORD\": \"${{ secrets.DBT_PASSWORD }}\",\n    },\n    \"forwardPorts\": [8000],\n    \"postStartCommand\": \"recce server --cloud --review\"\n}\n</code></pre>     The secrets are Github Codespaces secrets. You can configure them in</p> <ul> <li>Account specific codespaces secrets </li> <li>or Repository-level or organization-level codespaces secrets</li> </ul> </li> <li> <p>Prepare the <code>.devcontainer/Dockerfile</code> <pre><code>FROM mcr.microsoft.com/vscode/devcontainers/python:3.11\n\nRUN pip install dbt-bigquery~=1.7.0 recce~=0.34\n</code></pre></p> </li> </ol> <p>Note</p> <p>The GitHub token generated by codespace is sufficient for Recce's use. It's not necessary to configure the <code>GITHUB_TOKEN</code> separately.</p>"},{"location":"docs/recce-cloud/setup-gh-codespaces/#how-to-use","title":"How to use","text":"<p>Once you complete Recce Cloud setup, you can launch GitHub Codespaces from Recce Cloud's pull request page, and once it is started, the Recce instance will run and port forwarding will be set up.</p> <ol> <li>Go to Recce Cloud and click the repository you want to make changes.      </li> <li>Click the pull request that you want to use Recce instance. </li> <li>Click \"Create in GitHub Codespaces.\"     </li> <li>The Codespaces creation may take 1 to more than 5 mintues depeding on your Codespaces settings. And the Recce instance should take less than 1 minute to launch. <ul> <li>Please view FAQ for how to speed up.</li> </ul> </li> <li>You can see the \"State\" of the progress; And the Action you can take in each state.<ul> <li>Codespace Queued: the Codespace is creating      </li> <li>Codespace Provisioning: the Codespace is provisioning</li> <li>Codespace Available: the Codespace is ready</li> <li>Recce launching: Recce instance is launching</li> <li>Stop: stop launching Recce instance</li> <li>Recce active: Recce instance is launched successfully. <ul> <li>Open: open the Recce instance</li> <li>Stop: stop the Codespace </li> </ul> </li> <li>Codespace ShuttingDown: the Codespace is shutting down</li> <li>Stopped: the Codespace is stopped and the Recce instance is closed.<ul> <li>Restart: restart the Codespace </li> <li>Delete: delete the Codespace<ul> <li>It\u2019s recommended to delete the Codespace once your PR is merged. </li> </ul> </li> </ul> </li> </ul> </li> </ol>"},{"location":"docs/recce-cloud/setup-gh-codespaces/#troubleshooting","title":"Troubleshooting","text":"<p>If this is your first time setting up a Codespace, it\u2019s recommended to first test locally with the following commands:</p> <pre><code>git checkout feature/recce-getting-started\nexport GITHUB_TOKEN=&lt;github-token&gt;\nexport RECCE_STATE_PASSWORD=mypassword\nrecce server --cloud --review\n</code></pre> <p>Ensure it runs correctly locally. If it does, then the remaining issues within the Codespace are likely related to its configuration.</p> <p>If your Codespace configration is correct, other common causes might include:</p> <ol> <li>The current branch does not have a corresponding pull request. This usually happens if you launch Codespace directly form GitHub main branch. Recce instance cannot assoicate with the main branch. </li> <li>The pull request does not have an uploaded Recce state file. In review mode, the state file must be prepared via CI or locally before proceeding.</li> <li>The <code>RECCE_STATE_PASSWORD</code> mentioned above is not set or the password is incorrect.</li> <li>Other issues are preventing the Recce server from starting at all.</li> </ol> <p>When you\u2019ve opened a Codespace but are unable to connect to the Recce instance, you can troubleshoot by following these steps:</p> <ol> <li>Check Codespace instance in GitHub.      </li> <li>If the Codesapce you created from Recce Cloud is active, click \"Open in Browser\".      </li> <li>Click on the blue block in the lower left corner of the status bar, which usually shows \"Codespaces: instance name\"  </li> <li>Select \"View creation log\" or ppen the VS Code Command Palette and type <code>Codespaces: View Creation Log</code>. </li> <li>At this point, you should be able to see the reason why the Recce server failed to start.  </li> <li>If you cannot find any issue from the Codespace creation log, and belive your Codespace configration is correct. Please stop the Codespace and launch Recce instance again. </li> <li>If you still have problem, please contact us via slack or email product@datarecce.io. We are happy to help.</li> </ol>"},{"location":"docs/recce-cloud/setup-gh-codespaces/#faq","title":"FAQ","text":"<p>Q: How long does Codespace generally take to start?</p> <p>The typical startup time is around 1 to 2 minutes if you have prebuid. If not, it may take more than 5 mintues. However, this depends on how your Dockerfile is configured. Codespace builds your image every time it starts, so if your Dockerfile includes multiple pip install , it may take longer. <p>Once your Codespace instance is already running, you won\u2019t need to wait again when you return to it.</p> <p>Q: Is there a way to optimize the startup speed?</p> <p>Codespace offers a prebuild feature, which can significantly improve startup speed. In our sample project, we found it's helpful to reduce prebuild when we set prebuild available to only sepecific regions. However, you need to ensure that the image is up-to-date. To strike a balance between speed and update frequency, you can consider scheduling a weekly image rebuild. </p> <p>Q: Can a Codespace environment be shared? Can different people access the same Codespace instance?</p> <p>A Codespace environment is tied to each individual GitHub user account. Therefore, a Codespace instance opened by User A cannot be directly accessed by User B. However, User A can set a specific port to be public and share the URL with others. However, the instance is still running under User A's account.</p> <p>Q: Personal Codespace vs. Organization Codespace? How is Codespace billed?</p> <p>By default, Codespace usage is billed to a GitHub personal account. GitHub offers a free tier, allowing each user 120 core hours per month for free.</p> <p>For GitHub organization accounts, you can configure all Codespace charges to be billed to the organization. In this case, billing is attributed to the organization rather than personal accounts.</p> <p>For more details on billing, please refer to the official CodeSpace billing documentation.</p> <p>Q: How to configure codespaces?</p> <p>Codespace utilizes VSCode Dev Containers technology, which can be executed either locally (via Docker) or in the cloud (via GitHub Codespace). The configuration above provided are primarily recommendations for the Recce Cloud setup. For more advanced configuration options, you can refer to the VSCode dev containers or the containers.dev documentation.</p> <p>The default configuration for GitHub Codespace is explained in the documentation. If you're looking to set up a development environment for dbt/Recce, you can also refer to the Python project configuration documentation.</p> <p>Q: Can I have multiple devcontainer configurations? Which one is selected by default?</p> <p>Yes, you can. Please refer to the Codespaces documentation for more details. Recce Cloud will prioritize <code>.devcontainer/recce/devcontainer.json</code> if it is available. If not, it will default to <code>.devcontainer/devcontainer.json</code>, and then any other available configurations.</p> <p>Q: Should I delete Codespace after PR merged?</p> <p>Yes. When you merged a PR, you'll see the Delete codespace message. You can delete the Codespace to save the free usage.  </p> <p>You can also delete Codespace in your GitHub main branch or in Recce Cloud PR page.  </p>"},{"location":"docs/recce-cloud/share-recce-session-securely/","title":"Share Recce Session","text":""},{"location":"docs/recce-cloud/share-recce-session-securely/#share-your-validation-results-with-anyone-no-setup-needed","title":"Share your validation results with anyone, no setup needed","text":"<p>If you've already used Recce to validate your PR and prepared checks, but stakeholders might not have the environment to run Recce.</p> <p>Recce Cloud allows you to share your Recce validation results with full context, using a simple link. Stakeholders can open a read-only Recce view directly in their browser. No installation, no configuration, just instant access.</p> <p>Note</p> <p>Please note that anyone with the link can visit your shared Recce after sign in Recce Cloud. If you need to restrict access, please contact us.</p>"},{"location":"docs/recce-cloud/share-recce-session-securely/#how-to-use","title":"How to use","text":"<p>When you're ready to share your lineage exploration, query results, or validation checklist, simply click Share in Recce.</p> <p>The first time you do this, you'll need to connect your local Recce to Recce Cloud. This requires signing in and setting up an API token. Once connected, Recce Cloud will host your state file securely, allowing you to share a link that others can open in their browsers.</p> <ol> <li> <p>Enable Recce Sharing</p> <p>To start sharing, launch Recce server and click the Enable Sharing button.</p> <p></p> </li> <li> <p>Sign in to Recce Cloud and get your API token</p> <p>Copy your API token from the personal settings page in Recce Cloud.</p> <p></p> </li> <li> <p>Add the token to <code>.recce/profile.yml</code></p> <p>For convenience, you can add your API token in <code>.recce/profile.yml</code>, located by default in your home directory.  <pre><code>user_id: &lt;your_user_id&gt;\napi_token: &lt;your_api_token&gt;\n</code></pre> Alternatively, for one-time use, you can use <code>--api-token</code> flag with commend. e.g., <pre><code>recce server --api-token &lt;your_api_token&gt;\n</code></pre></p> </li> <li> <p>Relaunch Recce server</p> <p>After adding the API token, restarting Recce server is required to load the new configuration. Once it's running, you'll see the Share button, then you can click it to get the link on top.</p> <p></p> <p>You can also use the <code>recce share</code> command. If you already have a prepared Recce state file, you can obtain a share link directly through the command line. <pre><code>recce share &lt;your_state_file&gt;\nrecce share --api-token &lt;your_api_token&gt; &lt;your_state_file&gt;  # for one-time use\n</code></pre> </p> </li> </ol>"},{"location":"docs/recce-cloud/architecture/","title":"Recce Cloud Architecture","text":"<p>In this section, we will describe the architecture of Recce Cloud.</p>"},{"location":"docs/recce-cloud/architecture/security/","title":"Security","text":"<p>Recce Cloud is designed to be secure by default. We take security seriously and consider the data privacy from the beginning of the design phase. In this document, we will describe how we handle your data on Recce Cloud.</p>"},{"location":"docs/recce-cloud/architecture/security/#state-file-in-recce-cloud","title":"State File in Recce Cloud","text":"<p>When users execute <code>recce run</code> or <code>recce server</code> with option <code>--cloud</code>, Recce will upload the state file into the Recce Cloud. In the meanwhile, Recce will also request users provide the encryption password by option <code>--password</code> or <code>-p</code>. Recce will use the password to encrypt the state file when uploading and decrypt the state file when downloading. The password will not be stored in Recce Cloud, so users need to keep it safe.</p>"},{"location":"docs/recce-cloud/architecture/security/#how-recce-encrypts-the-state-file","title":"How Recce Encrypts the State File","text":"<p>Once users choose to upload the state file to Recce Cloud, Recce will store the state file in the cloud storage. Recce Cloud will use AWS S3 to store the state file and enable the server-side encryption with customer-provided keys (SSE-C). Recce Cloud will use the encryption password provided by users to encrypt the state file before uploading it to the S3 bucket. The encryption algorithm is AES-256. The state file will be decrypted with the same password when downloading it from the S3 bucket. </p> <p></p> <p>Based on the above flow chart, the state file will be encrypted and decrypted on the AWS S3 side. Recce Cloud server will only in charge of generating the pre-signed URL for uploading and downloading the state file. Which means Recce Cloud server will not have access to the state file content and the password key. </p>"},{"location":"docs/recce-cloud/architecture/self-hosted/","title":"Self Hosted Recce Instance","text":"<p>Note</p> <p>The Self-Hosted Recce Instance is currently under development.</p>"},{"location":"docs/recce-cloud/architecture/self-hosted/#self-hosted-recce-instance","title":"Self-Hosted Recce Instance","text":"<p>In a collaborative dbt project, we often want each in-progress PR to have a corresponding Recce server for review, which we call a Recce Instance. There are three solutions to create a Recce Instance.</p> <ul> <li> <p>GitHub Codespaces: Run the Recce Instance in the user's GitHub Codespace. Due to design limitations of codespaces, each codespace must be opened under a specific user, so each user needs to launch their own Recce Instance. This method also requires every user to have a GitHub account, which might not be suitable for some teams.</p> </li> <li> <p>Self-Hosted Recce Instance: Run the Recce Instance on your own cloud infrastructure (AWS, GCP, Azure, or on-premise machines). Since it is self-hosted, it offers greater flexibility compared to GitHub Codespaces, and because the server is hosted on your own infrastructure, it can provide better privacy and security. However, this comes at the cost of additional management overhead.</p> </li> <li> <p>Recce-Managed Recce Instance: We would host the Recce Instance on Recce's own cloud infrastructure, directly connecting to your warehouse. The advantage is minimal setup cost, but there are privacy and security considerations. Currently, we do not offer this method.</p> </li> </ul> <p>In this document, we will explain the design and architecture of the Self-hosted Recce Instance solution.</p>"},{"location":"docs/recce-cloud/architecture/self-hosted/#components","title":"Components","text":"<ol> <li>Recce Cloud: Provides an API to manage instances through an user interface.</li> <li>Recce Cloud Agent: A service running on your own cloud infrastructure. It communicates with Recce Cloud, creates and manages Recce Instances. The agent requires configuration with the Recce Cloud API token and the necessary data warehouse credentials for the Recce Instances.</li> <li>Recce Instance: An instance running Recce that requires a publicly accessible URL.</li> </ol>"},{"location":"docs/recce-cloud/architecture/self-hosted/#recce-cloud-agent-type","title":"Recce Cloud Agent Type","text":"<ul> <li> <p>Docker: The Docker agent runs instances within Docker containers, making it suitable for a single VM serving multiple Recce Instances through different ports. However, if too many instances are run concurrently, it may lead to insufficient VM resources.</p> </li> <li> <p>GCP Cloud Run: This approach runs the Recce Instances on GCP Cloud Run. Since GCP Cloud Run can scale the number of instances based on demand, it avoids the resource limitations faced by a single VM with Docker agents.</p> </li> </ul>"},{"location":"docs/recce-cloud/architecture/self-hosted/#recce-instance-lifecycle","title":"Recce Instance Lifecycle","text":"<ol> <li>The user creates a Recce Instance for a PR.</li> <li>The agent receives the request and provisions the Recce Instance.</li> <li>Once the instance is ready, the URL endpoint becomes accessible.</li> <li>The user can stop or start an instance as needed.</li> <li>The user can delete a Recce Instance, and the agent will handle the deletion of the underlying container.</li> </ol>"},{"location":"docs/recce-cloud/architecture/self-hosted/#access-control","title":"Access Control","text":"<p>Recce Instances allow you to connect directly to the database through the interface, making access control a key consideration. Here are several options:</p> <ul> <li>Virtual Private Network: If your Recce Instance is running inside a VPC and is not accessible externally, this is the most secure and reliable approach.</li> <li>Basic Authentication: The Recce Instance is publicly accessible but protected via HTTP Basic Authentication. Accessing the Recce Instance requires providing a user/password.</li> <li>Tunnel-based Authentication: The Recce Instance runs within a VPC but can be exposed for external access using services such as ngrok or Tailscale. These services provide access control features to secure the instance.</li> <li>Recce Cloud Authentication: Utilize Recce Cloud's login state to access the Recce Instance. The Recce Instance acts as an OIDC app within Recce Cloud, authenticating users through the OIDC protocol.</li> </ul>"},{"location":"docs/recce-cloud/architecture/self-hosted/#faq","title":"FAQ","text":""},{"location":"docs/recce-cloud/architecture/self-hosted/#q-does-recce-cloud-make-requests-to-the-agents-endpoint","title":"Q: Does Recce Cloud make requests to the Agent's endpoint?","text":"<p>No. All actions are initiated by the agent making requests to the Recce Cloud API.</p>"},{"location":"docs/recce-cloud/architecture/self-hosted/#q-do-i-need-to-store-my-credentials-in-recce-cloud","title":"Q: Do I need to store my credentials in Recce Cloud?","text":"<p>No. All credentials are configured within the agent, which uses these settings to start a Recce Instance. This ensures you do not encounter any credential leakage issues.</p>"},{"location":"docs/recce-cloud/architecture/self-hosted/#q-does-recce-agent-manage-the-entire-organization-or-a-single-repository","title":"Q: Does Recce Agent manage the entire organization or a single repository?","text":"<p>It manages the entire organization. However, support for single repository configurations may be available in the future.</p>"},{"location":"docs/recce-cloud/architecture/self-hosted/#q-do-i-need-to-define-what-container-image-the-recce-instance-uses-can-i-customize-my-own-container-image","title":"Q: Do I need to define what container image the Recce Instance uses? Can I customize my own container image?","text":"<p>We provide an official Recce container that will check out your branch and install the necessary dependencies. You can also customize your own container image to speed up startup times or provide more flexible initialization processes.</p>"},{"location":"docs/recce-cloud/architecture/self-hosted/#q-how-do-i-run-a-recce-cloud-agent-where-should-my-cloud-run-agent-be-hosted","title":"Q: How do I run a Recce Cloud Agent? Where should my Cloud Run Agent be hosted?","text":"<p>The Recce Cloud Agent is a Python application. The recommended execution method varies depending on the agent type. The Docker agent runs within a Docker container, while the GCP Cloud Run agent runs within a Cloud Run container.</p>"},{"location":"docs/reference/configuration/","title":"Configuration","text":"<p>The config file for Recce is located in recce.yml. Currently, only preset checks are configurable.</p>"},{"location":"docs/reference/configuration/#preset-checks","title":"Preset Checks","text":"<p>Preset checks can be generated when executing <code>recce server</code> or <code>recce run</code>.</p> <pre><code># recce.yml\nchecks:\n  - name: Query diff of customers\n    description: |\n        This is the demo preset check.\n\n        Please run the query and paste the screenshot to the PR comment.\n    type: query_diff\n    params:\n        sql_template: select * from {{ ref(\"customers\") }}\n    view_options:\n        primary_keys:\n        - customer_id\n</code></pre> Field Description Type Required <code>name</code> the title of the check string Yes <code>description</code> the description of the check string <code>type</code> the type of the check string Yes <code>params</code> the parameters for running the check object Yes <code>view_options</code> the options for presenting the run result object"},{"location":"docs/reference/configuration/#check-params","title":"Check Params","text":""},{"location":"docs/reference/configuration/#row-count-diff","title":"Row Count Diff","text":"Field Description Type Required <code>node_names</code> List of node names. <code>string[]</code> *1 <code>node_ids</code> List of node ides <code>string[]</code> *1 <code>select</code> The node selection syntax to select. See more <code>string</code> <code>exclude</code> the node selection syntax to exclude. See more <code>string</code> <code>packages</code> The package filter <code>string[]</code> <code>view_mode</code> The quick filter to select changed model and 1st degree of upstream. <code>all</code>, <code>changed_model</code> <p>Notes</p> <p>*1: If <code>node_ids</code> or <code>node_names</code> is specified, it will be used; otherwise, nodes will be selected using the criteria defined by <code>select</code>, <code>exclude</code>, <code>packages</code>, and <code>view_mode</code>.</p> <p>Examples</p> <p>Run row count by node selector <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: row_count_diff\n    params:\n       select: state:modified,config.materialized:table\n       exclude: tag:dev \n</code></pre></p> <p>Run row count by node names <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: row_count_diff\n    params:\n      node_names: ['customers', 'orders']\n</code></pre></p>"},{"location":"docs/reference/configuration/#schema-diff","title":"Schema Diff","text":"Field Description Type Required <code>node_id</code> The node id or list of node ids to check. <code>string[]</code> *1 <code>select</code> The node selection syntax to select. See more <code>string</code> <code>exclude</code> the node selection syntax to exclude. See more <code>string</code> <code>packages</code> The package filter <code>string[]</code> <code>view_mode</code> The quick filter to select changed model and 1st degree of upstream. <code>all</code>, <code>changed_model</code> <p>Notes</p> <p>*1: If <code>node_id</code> is specified, it will be used; otherwise, nodes will be selected using the criteria defined by <code>select</code>, <code>exclude</code>, <code>packages</code>, and <code>view_mode</code>.</p> <p>Examples</p> <p>Check schema diff by node selector <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: schema_diff\n    params:\n       select: state:modified+\n       exclude: tag:dev \n</code></pre></p> <p>Check schema diff by node ides <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: row_count_diff\n    params:\n      node_id: model.jaffle_shop.customers\n</code></pre></p>"},{"location":"docs/reference/configuration/#lineage-diff","title":"Lineage Diff","text":"Field Description Type Required <code>select</code> The node selection syntax to select. See more <code>string</code> <code>exclude</code> the node selection syntax to exclude. See more <code>string</code> <code>packages</code> The package filter <code>string[]</code> <code>view_mode</code> The quick filter to select changed model and 1st degree of upstream. <code>all</code>, <code>changed_model</code> <p>Examples</p> <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: lineage_diff\n    params:\n       select: state:modified+\n       exclude: tag:dev\n</code></pre>"},{"location":"docs/reference/configuration/#query","title":"Query","text":"Field Description Type Required <code>sql_template</code> The SQL statement, templated using Jinja, to be executed <code>string</code> Yes <p>Examples</p> <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: query\n    params:\n       sql_template: select * from {{ ref(\"customers\") }}\n</code></pre>"},{"location":"docs/reference/configuration/#query-diff","title":"Query Diff","text":"Field Description Type Required <code>sql_template</code> The SQL statement, templated using Jinja, to be executed <code>string</code> Yes <code>base_sql_template</code> The SQL statement to execute in the base environment, if specified. <code>string</code> <code>primary_keys</code> The primary keys used to identify a record. <code>string[]</code> *1 <p>*1: If primary_keys is specified, the query diff is performed in the warehouse. Otherwise, the query result (up to the first 2000 records) is returned, and the diff is executed on the client side.</p> <p>Examples</p> <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: query_diff\n    params:\n      sql_template: select * from {{ ref(\"customers\") }}\n      primary_keys:\n      - customer_id\n</code></pre>"},{"location":"docs/reference/configuration/#value-diff","title":"Value Diff","text":"Field Description Type Required <code>model</code> The name of the model. <code>string</code> Yes <code>primary_key</code> The primary key(s) used to uniquely identify a record. <code>string</code> or <code>string[]</code> Yes <code>columns</code> The list of columns to include in the value diff. <code>string[]</code> <p>Examples</p> <p>Value diff summary <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: value_diff\n    params:\n      model: customers\n      primary_key: customer_id\n</code></pre></p> <p>Value diff with detailed rows <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: value_diff_detail\n    params:\n      model: customers\n      primary_key: customer_id\n</code></pre></p>"},{"location":"docs/reference/configuration/#profile-diff","title":"Profile Diff","text":"Field Description Type Required <code>model</code> The name of the model. <code>string</code> Yes <p>Examples <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: profile_diff\n    params:\n      model: customers\n</code></pre></p>"},{"location":"docs/reference/configuration/#histogram-diff","title":"Histogram Diff","text":"Field Description Type Required <code>model</code> The name of the model. <code>string</code> Yes <code>column_name</code> The name of the column. <code>string</code> Yes <code>column_type</code> The type of the column. <code>string</code> Yes <p>Examples <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: histogram_diff\n    params:\n      model: customers\n      column_name: customer_lifetime_value\n      column_type: BIGINT      \n</code></pre></p>"},{"location":"docs/reference/configuration/#top-k-diff","title":"Top-K Diff","text":"Field Description Type Required <code>model</code> The name of the model. <code>string</code> Yes <code>column_name</code> The name of the column. <code>string</code> Yes <code>k</code> Specifies the top-k items to include in the result. <code>number</code> (Default 50) <p>Examples <pre><code>checks:\n  - name: 'name'\n    description: 'description'\n    type: top_k_diff\n    params:\n      model: customers\n      column_name: customer_lifetime_value\n      k: 50\n</code></pre></p>"},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/category/guides/","title":"Guides","text":""},{"location":"blog/category/features/","title":"Features","text":""},{"location":"blog/category/announcements/","title":"Announcements","text":""},{"location":"blog/category/data-change-management/","title":"Data Change Management","text":""},{"location":"blog/category/impact-assessment/","title":"Impact assessment","text":""},{"location":"blog/category/data-validation/","title":"Data Validation","text":""},{"location":"blog/category/data-exploration/","title":"Data Exploration","text":""},{"location":"blog/category/pull-request/","title":"Pull Request","text":""},{"location":"blog/category/pr-review/","title":"PR Review","text":""},{"location":"blog/category/best-practices/","title":"Best Practices","text":""},{"location":"blog/category/concepts/","title":"Concepts","text":""},{"location":"blog/category/self-serve-review/","title":"Self-Serve Review","text":""},{"location":"blog/category/dbt/","title":"dbt","text":""},{"location":"blog/category/webinar/","title":"Webinar","text":""},{"location":"blog/category/usage/","title":"Usage","text":""},{"location":"blog/page/2/","title":"The Recce Blog","text":""}]}